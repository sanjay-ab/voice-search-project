Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training embeddings directory: data/banjara/embeddings/training_data/9/raw
Validation embeddings file: data/banjara/embeddings/validation_data/9/perturbed_0.1_phonetized_3_9/all_embeddings_phonetized.pkl
START TIME: 2024-07-19_14:27:11
Training model for banjara with inputs from mHuBERT layer 9
Number of epochs: 10, patience: 2, learning rate: 1e-06
clip norm: 20, temperature: 0.5, num pairs per batch: 700
time limit to create dataset: 240
temperature: 0.5
min phone seq length: 3, max phone seq length: 9
perturb sequences: False, max one sided perturb amount: 0.1

Loading embedded data from directory: data/banjara/embeddings/training_data/9/raw
Loaded embedded data from data/banjara/embeddings/training_data/9/raw
Time taken: 1.18 s
Created paired data
Time taken to create datasets: 1.83 s

Loading model from data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_13:47:56_checkpoint_epoch_2.pt

/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.03% done
Loss: 69.69931125640869
Time: 3.93 s
Epoch 0: 10.02% done
Loss: 136.91425281258572
Time: 11.99 s
Epoch 0: 20.02% done
Loss: 131.9645352317813
Time: 19.12 s
Epoch 0: 30.01% done
Loss: 134.84168493660746
Time: 25.35 s
Epoch 0: 40.01% done
Loss: 126.03898628660055
Time: 31.57 s
Epoch 0: 50.03% done
Loss: 129.3632731177203
Time: 37.42 s
Epoch 0: 60.02% done
Loss: 130.15833713866937
Time: 44.28 s
Epoch 0: 70.02% done
Loss: 124.69709299134387
Time: 50.23 s
Epoch 0: 80.01% done
Loss: 125.53309968476192
Time: 56.43 s
Epoch 0: 90.01% done
Loss: 120.90894455940072
Time: 62.72 s

Epoch 0 done
Epoch loss: 127.67781643088227

Time taken for epoch: 69.10 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.03% done
Loss: 127.77668237686157
Time: 0.01 s
Epoch 1: 10.02% done
Loss: 123.54757450635908
Time: 6.19 s
Epoch 1: 20.02% done
Loss: 120.17169926586251
Time: 11.78 s
Epoch 1: 30.02% done
Loss: 120.92255568497198
Time: 17.77 s
Epoch 1: 40.02% done
Loss: 117.66539819454296
Time: 24.58 s
Epoch 1: 50.01% done
Loss: 121.28316371179066
Time: 31.24 s
Epoch 1: 60.01% done
Loss: 117.6794554858591
Time: 37.06 s
Epoch 1: 70.01% done
Loss: 112.7384134791535
Time: 43.86 s
Epoch 1: 80.01% done
Loss: 117.24870299257331
Time: 51.93 s
Epoch 1: 90.00% done
Loss: 113.01039646041004
Time: 60.43 s

Epoch 1 done
Epoch loss: 117.6432651639924

Time taken for epoch: 70.03 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.03% done
Loss: 110.18033981323242
Time: 0.01 s
Epoch 2: 10.01% done
Loss: 113.153938165178
Time: 6.86 s
Epoch 2: 20.03% done
Loss: 107.10352052727458
Time: 14.65 s
Epoch 2: 30.01% done
Loss: 109.10975677029245
Time: 22.39 s
Epoch 2: 40.03% done
Loss: 110.20129464394057
Time: 29.32 s
Epoch 2: 50.01% done
Loss: 111.07541981643982
Time: 35.71 s
Epoch 2: 60.03% done
Loss: 109.9452331021117
Time: 42.66 s
Epoch 2: 70.01% done
Loss: 107.05590825294448
Time: 49.25 s
Epoch 2: 80.03% done
Loss: 108.7858256930773
Time: 54.81 s
Epoch 2: 90.01% done
Loss: 105.41903173235401
Time: 61.40 s

Epoch 2 done
Epoch loss: 109.13537729782047

Time taken for epoch: 68.92 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.03% done
Loss: 45.72972384366122
Time: 0.03 s
Epoch 3: 10.02% done
Loss: 107.01398736666168
Time: 6.99 s
Epoch 3: 20.02% done
Loss: 102.53730674749112
Time: 15.03 s
Epoch 3: 30.01% done
Loss: 105.5733156212238
Time: 21.78 s
Epoch 3: 40.01% done
Loss: 104.68565737102261
Time: 27.97 s
Epoch 3: 50.03% done
Loss: 105.01938330849019
Time: 34.48 s
Epoch 3: 60.02% done
Loss: 102.2361368780749
Time: 40.62 s
Epoch 3: 70.02% done
Loss: 102.9093420736288
Time: 47.08 s
Epoch 3: 80.01% done
Loss: 99.98525672022157
Time: 54.76 s
Epoch 3: 90.01% done
Loss: 100.1974137813914
Time: 62.30 s

Epoch 3 done
Epoch loss: 102.80470648194014

Time taken for epoch: 68.53 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.03% done
Loss: 68.63528092702231
Time: 0.02 s
Epoch 4: 10.01% done
Loss: 101.2764511505867
Time: 6.54 s
Epoch 4: 20.02% done
Loss: 95.72808433947128
Time: 14.14 s
Epoch 4: 30.01% done
Loss: 99.98946552967331
Time: 21.25 s
Epoch 4: 40.02% done
Loss: 96.33453986149173
Time: 28.03 s
Epoch 4: 50.03% done
Loss: 98.09127307988834
Time: 34.37 s
Epoch 4: 60.01% done
Loss: 98.49261780683396
Time: 40.45 s
Epoch 4: 70.02% done
Loss: 96.080867386259
Time: 48.13 s
Epoch 4: 80.01% done
Loss: 95.17270449093874
Time: 53.37 s
Epoch 4: 90.02% done
Loss: 96.27141193069372
Time: 59.39 s

Epoch 4 done
Epoch loss: 97.22517674900362

Time taken for epoch: 65.97 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.03% done
Loss: 140.23696899414062
Time: 0.01 s
Epoch 5: 10.02% done
Loss: 96.1170860810249
Time: 6.98 s
Epoch 5: 20.01% done
Loss: 95.3526455763362
Time: 12.88 s
Epoch 5: 30.00% done
Loss: 96.17759386175972
Time: 19.15 s
Epoch 5: 40.02% done
Loss: 92.97556960026681
Time: 24.50 s
Epoch 5: 50.01% done
Loss: 93.05755699660047
Time: 30.34 s
Epoch 5: 60.01% done
Loss: 93.57811877364087
Time: 35.83 s
Epoch 5: 70.02% done
Loss: 91.529711487248
Time: 41.37 s
Epoch 5: 80.02% done
Loss: 91.19360096999348
Time: 46.45 s
Epoch 5: 90.01% done
Loss: 91.42011652381028
Time: 53.91 s

Epoch 5 done
Epoch loss: 93.61221222918226

Time taken for epoch: 59.62 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.03% done
Loss: 57.391564051310226
Time: 0.02 s
Epoch 6: 10.01% done
Loss: 88.99005049124321
Time: 5.61 s
Epoch 6: 20.03% done
Loss: 90.23603182275949
Time: 12.06 s
Epoch 6: 30.01% done
Loss: 91.20554234061193
Time: 19.39 s
Epoch 6: 40.03% done
Loss: 93.93872803435866
Time: 27.72 s
Epoch 6: 50.01% done
Loss: 89.24720923205777
Time: 35.88 s
Epoch 6: 60.03% done
Loss: 90.15564792253919
Time: 44.08 s
Epoch 6: 70.01% done
Loss: 91.42712329394197
Time: 50.94 s
Epoch 6: 80.03% done
Loss: 89.0972412156757
Time: 57.83 s
Epoch 6: 90.01% done
Loss: 93.0014791284312
Time: 65.01 s

Epoch 6 done
Epoch loss: 90.75476780552215

Time taken for epoch: 72.09 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.03% done
Loss: 59.119586149851486
Time: 0.01 s
Epoch 7: 10.02% done
Loss: 91.77910700250632
Time: 5.90 s
Epoch 7: 20.01% done
Loss: 86.9357954816648
Time: 11.78 s
Epoch 7: 30.00% done
Loss: 88.81480412508567
Time: 18.07 s
Epoch 7: 40.02% done
Loss: 89.17771463518942
Time: 24.66 s
Epoch 7: 50.01% done
Loss: 88.68814280905177
Time: 31.24 s
Epoch 7: 60.01% done
Loss: 88.88943584955832
Time: 37.56 s
Epoch 7: 70.02% done
Loss: 88.47008249173614
Time: 45.57 s
Epoch 7: 80.02% done
Loss: 89.22230471742262
Time: 51.95 s
Epoch 7: 90.01% done
Loss: 88.68646210283262
Time: 58.06 s

Epoch 7 done
Epoch loss: 88.60474360440104

Time taken for epoch: 65.73 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.03% done
Loss: 138.99943351745605
Time: 0.01 s
Epoch 8: 10.02% done
Loss: 88.53188798802124
Time: 7.00 s
Epoch 8: 20.01% done
Loss: 90.30251063336415
Time: 13.76 s
Epoch 8: 30.00% done
Loss: 86.26088405556004
Time: 20.26 s
Epoch 8: 40.02% done
Loss: 86.56028409791737
Time: 29.26 s
Epoch 8: 50.01% done
Loss: 86.72587985148681
Time: 35.96 s
Epoch 8: 60.01% done
Loss: 88.25983508188833
Time: 42.47 s
Epoch 8: 70.02% done
Loss: 87.64834082486628
Time: 50.29 s
Epoch 8: 80.02% done
Loss: 86.77170558947914
Time: 57.04 s
Epoch 8: 90.01% done
Loss: 85.5065814280089
Time: 63.51 s

Epoch 8 done
Epoch loss: 87.01719253009627

Time taken for epoch: 68.82 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.03% done
Loss: 80.08275713239398
Time: 0.01 s
Epoch 9: 10.02% done
Loss: 84.12799113662861
Time: 6.11 s
Epoch 9: 20.01% done
Loss: 85.88681670201315
Time: 11.60 s
Epoch 9: 30.02% done
Loss: 88.43010437415822
Time: 18.21 s
Epoch 9: 40.01% done
Loss: 86.6065340400969
Time: 26.30 s
Epoch 9: 50.03% done
Loss: 85.33765307362796
Time: 33.87 s
Epoch 9: 60.02% done
Loss: 84.47243789354401
Time: 40.20 s
Epoch 9: 70.01% done
Loss: 87.04335030869123
Time: 46.81 s
Epoch 9: 80.02% done
Loss: 87.74474827324742
Time: 53.11 s
Epoch 9: 90.01% done
Loss: 83.27980590016118
Time: 58.69 s

Epoch 9 done
Epoch loss: 85.64708897843342

Time taken for epoch: 64.75 s
Number of gradients clipped: 0

Saving model to data/banjara/models/9/tamil_lr_1e-4_tmp_0.07_banj_lr_1e-6_tmp_0.5_3_9/2024-07-19_14:27:11_checkpoint_epoch_9.pt

Regenerated paired data
BEST VALIDATION LOSS: 0 at epoch 0

