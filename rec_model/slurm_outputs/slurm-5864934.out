Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-06_22:19:10
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 5, learning rate: 0.0001
clip norm: 10, temperature: 0.15, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr_division: 100
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 72.05 s
Number of parameters in model: 6825984

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 446.41761779785156
Time: 15.76 s
Epoch 0: 10.04% done
Loss: 482.19664440879336
Time: 21.04 s
Epoch 0: 20.08% done
Loss: 459.0003399848938
Time: 22.87 s
Epoch 0: 30.11% done
Loss: 387.6952553590139
Time: 24.26 s
Epoch 0: 40.03% done
Loss: 367.1914997583703
Time: 26.25 s
Epoch 0: 50.06% done
Loss: 345.80672258138657
Time: 28.17 s
Epoch 0: 60.10% done
Loss: 320.8055565357208
Time: 30.01 s
Epoch 0: 70.01% done
Loss: 329.62626453190416
Time: 31.92 s
Epoch 0: 80.05% done
Loss: 319.8389184474945
Time: 33.73 s
Epoch 0: 90.09% done
Loss: 314.2269341945648
Time: 35.52 s

Epoch 0 done
Epoch loss: 364.565444140193

Time taken for epoch: 37.73 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.93482872368634

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 298.5831832885742
Time: 0.01 s
Epoch 1: 10.06% done
Loss: 311.96235910246645
Time: 1.45 s
Epoch 1: 20.13% done
Loss: 299.7664201259613
Time: 3.36 s
Epoch 1: 30.06% done
Loss: 307.04942759582264
Time: 5.23 s
Epoch 1: 40.13% done
Loss: 287.99436140060425
Time: 7.04 s
Epoch 1: 50.06% done
Loss: 282.073195976547
Time: 8.86 s
Epoch 1: 60.13% done
Loss: 290.98356223106384
Time: 10.65 s
Epoch 1: 70.06% done
Loss: 283.33368627330924
Time: 12.46 s
Epoch 1: 80.13% done
Loss: 279.4311944246292
Time: 14.29 s
Epoch 1: 90.06% done
Loss: 260.9668258473843
Time: 16.14 s

Epoch 1 done
Epoch loss: 286.98148376736754

Time taken for epoch: 18.38 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.46 s

Validation loss: 346.0351621931877

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 284.91443634033203
Time: 0.01 s
Epoch 2: 10.06% done
Loss: 260.366972911207
Time: 1.35 s
Epoch 2: 20.13% done
Loss: 252.01187479496002
Time: 3.15 s
Epoch 2: 30.06% done
Loss: 255.89843182624142
Time: 5.10 s
Epoch 2: 40.13% done
Loss: 245.51887574791908
Time: 6.96 s
Epoch 2: 50.06% done
Loss: 238.69412313533735
Time: 8.92 s
Epoch 2: 60.13% done
Loss: 234.8494974374771
Time: 10.93 s
Epoch 2: 70.06% done
Loss: 242.04158288014085
Time: 12.76 s
Epoch 2: 80.13% done
Loss: 231.97382697463036
Time: 14.76 s
Epoch 2: 90.06% done
Loss: 236.04670802249183
Time: 16.69 s

Epoch 2 done
Epoch loss: 243.36618794135327

Time taken for epoch: 18.93 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.46 s

Validation loss: 345.6594233063684

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 256.4784812927246
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 219.02209583717055
Time: 1.94 s
Epoch 3: 20.13% done
Loss: 207.2631413936615
Time: 3.84 s
Epoch 3: 30.06% done
Loss: 218.50499394573743
Time: 5.21 s
Epoch 3: 40.13% done
Loss: 211.4873992204666
Time: 7.16 s
Epoch 3: 50.06% done
Loss: 194.32656891738313
Time: 9.09 s
Epoch 3: 60.13% done
Loss: 201.51617262760797
Time: 10.95 s
Epoch 3: 70.06% done
Loss: 199.6937471703638
Time: 12.86 s
Epoch 3: 80.13% done
Loss: 187.24097681045532
Time: 14.75 s
Epoch 3: 90.06% done
Loss: 207.67239673228204
Time: 16.72 s

Epoch 3 done
Epoch loss: 204.68914131948281

Time taken for epoch: 19.01 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 345.52602422410166

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 96.49508476257324
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 185.58603760562366
Time: 1.32 s
Epoch 4: 20.03% done
Loss: 183.8515135608142
Time: 3.14 s
Epoch 4: 30.10% done
Loss: 176.86420023441315
Time: 4.92 s
Epoch 4: 40.05% done
Loss: 175.3631942483443
Time: 6.86 s
Epoch 4: 50.13% done
Loss: 165.76662689447403
Time: 8.86 s
Epoch 4: 60.08% done
Loss: 168.65675690807873
Time: 10.78 s
Epoch 4: 70.03% done
Loss: 164.114426962937
Time: 12.60 s
Epoch 4: 80.10% done
Loss: 161.46352940797806
Time: 14.46 s
Epoch 4: 90.05% done
Loss: 166.0280540925038
Time: 16.35 s

Epoch 4 done
Epoch loss: 170.30873401038892

Time taken for epoch: 18.58 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.33 s
Calculating validation loss: 80.43% done
Time: 0.45 s

Validation loss: 345.7210443330847

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 95.29167175292969
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 153.00294194040418
Time: 1.83 s
Epoch 5: 20.05% done
Loss: 156.4271292505385
Time: 3.24 s
Epoch 5: 30.01% done
Loss: 141.03361347053624
Time: 5.10 s
Epoch 5: 40.10% done
Loss: 149.97595277801156
Time: 6.93 s
Epoch 5: 50.06% done
Loss: 137.19120206712168
Time: 8.80 s
Epoch 5: 60.03% done
Loss: 131.35889053344727
Time: 10.57 s
Epoch 5: 70.11% done
Loss: 138.45020711794496
Time: 12.49 s
Epoch 5: 80.08% done
Loss: 140.30940976323961
Time: 14.42 s
Epoch 5: 90.04% done
Loss: 133.29388117488426
Time: 16.29 s

Epoch 5 done
Epoch loss: 141.8343810453391

Time taken for epoch: 18.49 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.21 s
Calculating validation loss: 60.87% done
Time: 0.33 s
Calculating validation loss: 80.43% done
Time: 0.45 s

Validation loss: 346.0196066766546

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 118.9590072631836
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 120.13089505932
Time: 1.78 s
Epoch 6: 20.13% done
Loss: 126.56476566568017
Time: 3.09 s
Epoch 6: 30.06% done
Loss: 117.08793048617206
Time: 4.94 s
Epoch 6: 40.13% done
Loss: 114.43161106109619
Time: 6.90 s
Epoch 6: 50.06% done
Loss: 119.88431248483779
Time: 8.77 s
Epoch 6: 60.13% done
Loss: 121.61472868919373
Time: 10.79 s
Epoch 6: 70.06% done
Loss: 110.4854378519179
Time: 12.65 s
Epoch 6: 80.13% done
Loss: 118.83994939923286
Time: 14.42 s
Epoch 6: 90.06% done
Loss: 124.40488878684708
Time: 16.21 s

Epoch 6 done
Epoch loss: 118.62693571107192

Time taken for epoch: 18.56 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 346.30978204201955

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 130.49975395202637
Time: 0.01 s
Epoch 7: 10.04% done
Loss: 97.84297915953624
Time: 1.99 s
Epoch 7: 20.08% done
Loss: 104.04970818758011
Time: 3.30 s
Epoch 7: 30.11% done
Loss: 103.72927504777908
Time: 5.10 s
Epoch 7: 40.03% done
Loss: 109.51791503761388
Time: 6.91 s
Epoch 7: 50.06% done
Loss: 105.60978001356125
Time: 8.80 s
Epoch 7: 60.10% done
Loss: 95.0621634721756
Time: 10.65 s
Epoch 7: 70.01% done
Loss: 106.02423028100895
Time: 12.48 s
Epoch 7: 80.05% done
Loss: 100.02650152146816
Time: 14.27 s
Epoch 7: 90.09% done
Loss: 106.42969869077206
Time: 16.16 s

Epoch 7 done
Epoch loss: 102.61536531695658

Time taken for epoch: 18.39 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.6651731470357

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 56.769423484802246
Time: 0.01 s
Epoch 8: 10.08% done
Loss: 81.62201901025409
Time: 1.83 s
Epoch 8: 20.03% done
Loss: 89.61462029927894
Time: 3.13 s
Epoch 8: 30.10% done
Loss: 86.76476582884789
Time: 5.00 s
Epoch 8: 40.05% done
Loss: 87.62163551547859
Time: 6.90 s
Epoch 8: 50.13% done
Loss: 84.79841139912605
Time: 8.85 s
Epoch 8: 60.08% done
Loss: 82.09547054918507
Time: 10.71 s
Epoch 8: 70.03% done
Loss: 86.7557653143436
Time: 12.54 s
Epoch 8: 80.10% done
Loss: 82.75093026459217
Time: 14.40 s
Epoch 8: 90.05% done
Loss: 96.706549728973
Time: 16.30 s

Epoch 8 done
Epoch loss: 86.57349192525638

Time taken for epoch: 18.59 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 347.04243131305856

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_8.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 345.52602422410166 at epoch 3

