Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-07_12:43:29
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 500, patience: 5, learning rate: 0.0001
clip norm: 10, temperature: 0.15, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr: 1e-05
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 65.10 s
up_proj_dim: 512
output_dim: 8192
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 15756288
Number of parameters in AWE model: 6825984
Number of parameters in other model: 8930304
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 387.39776611328125
Time: 0.59 s
Epoch 0: 10.04% done
Loss: 470.8241243000272
Time: 2.58 s
Epoch 0: 20.08% done
Loss: 411.1711161136627
Time: 4.44 s
Epoch 0: 30.11% done
Loss: 380.2740592956543
Time: 6.34 s
Epoch 0: 40.03% done
Loss: 344.80643381046343
Time: 7.61 s
Epoch 0: 50.06% done
Loss: 333.4868652820587
Time: 9.56 s
Epoch 0: 60.10% done
Loss: 332.60649629433954
Time: 11.61 s
Epoch 0: 70.01% done
Loss: 317.4098879174341
Time: 13.50 s
Epoch 0: 80.05% done
Loss: 312.6902711391449
Time: 15.51 s
Epoch 0: 90.09% done
Loss: 313.4750728607178
Time: 17.53 s

Epoch 0 done
Epoch loss: 353.5186723145118

Time taken for epoch: 20.03 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.53 s

Validation loss: 348.89656723409456

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 254.71681594848633
Time: 0.01 s
Epoch 1: 10.06% done
Loss: 309.2652148234693
Time: 1.38 s
Epoch 1: 20.13% done
Loss: 299.6280312538147
Time: 3.28 s
Epoch 1: 30.06% done
Loss: 312.00728887244117
Time: 5.10 s
Epoch 1: 40.13% done
Loss: 286.7561115026474
Time: 7.08 s
Epoch 1: 50.06% done
Loss: 283.4287607217137
Time: 9.03 s
Epoch 1: 60.13% done
Loss: 286.11983263492584
Time: 10.92 s
Epoch 1: 70.06% done
Loss: 286.37804393526875
Time: 12.79 s
Epoch 1: 80.13% done
Loss: 278.18070944150287
Time: 14.65 s
Epoch 1: 90.06% done
Loss: 283.9906781836401
Time: 16.57 s

Epoch 1 done
Epoch loss: 290.2736069981157

Time taken for epoch: 18.83 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 348.2497881288114

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 302.42504119873047
Time: 0.01 s
Epoch 2: 10.06% done
Loss: 275.8230939696107
Time: 1.89 s
Epoch 2: 20.13% done
Loss: 283.25371873378754
Time: 3.77 s
Epoch 2: 30.06% done
Loss: 270.0398687773113
Time: 5.11 s
Epoch 2: 40.13% done
Loss: 264.21496963500977
Time: 7.04 s
Epoch 2: 50.06% done
Loss: 264.2976630186733
Time: 9.02 s
Epoch 2: 60.13% done
Loss: 269.0394261280696
Time: 10.90 s
Epoch 2: 70.06% done
Loss: 277.86829719060586
Time: 12.74 s
Epoch 2: 80.13% done
Loss: 269.35229313373566
Time: 14.69 s
Epoch 2: 90.06% done
Loss: 248.1391001351272
Time: 16.46 s

Epoch 2 done
Epoch loss: 267.6240595581646

Time taken for epoch: 18.78 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.00 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 347.87283826565397

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 218.32208633422852
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 266.745588326756
Time: 1.95 s
Epoch 3: 20.13% done
Loss: 262.5698860883713
Time: 3.99 s
Epoch 3: 30.06% done
Loss: 240.93477068068105
Time: 5.29 s
Epoch 3: 40.13% done
Loss: 256.0243146419525
Time: 7.17 s
Epoch 3: 50.06% done
Loss: 255.70024454140966
Time: 8.99 s
Epoch 3: 60.13% done
Loss: 250.06605887413025
Time: 10.79 s
Epoch 3: 70.06% done
Loss: 233.50830548926245
Time: 12.75 s
Epoch 3: 80.13% done
Loss: 238.426859498024
Time: 14.71 s
Epoch 3: 90.06% done
Loss: 240.87540288514728
Time: 16.65 s

Epoch 3 done
Epoch loss: 248.992070292027

Time taken for epoch: 19.04 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.55 s

Validation loss: 347.66963813615877

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 260.6975746154785
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 240.36985530128962
Time: 1.89 s
Epoch 4: 20.03% done
Loss: 237.34738084334361
Time: 3.32 s
Epoch 4: 30.10% done
Loss: 236.4616596698761
Time: 5.20 s
Epoch 4: 40.05% done
Loss: 221.4854230760019
Time: 7.12 s
Epoch 4: 50.13% done
Loss: 227.20117282867432
Time: 9.09 s
Epoch 4: 60.08% done
Loss: 229.67564754848237
Time: 10.96 s
Epoch 4: 70.03% done
Loss: 229.62279319763184
Time: 12.97 s
Epoch 4: 80.10% done
Loss: 232.6191120147705
Time: 14.85 s
Epoch 4: 90.05% done
Loss: 231.4992739279059
Time: 16.78 s

Epoch 4 done
Epoch loss: 232.62653725873915

Time taken for epoch: 19.14 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 347.41160662277883

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 301.5445327758789
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 227.2903281827516
Time: 1.96 s
Epoch 5: 20.05% done
Loss: 221.95039314559745
Time: 3.40 s
Epoch 5: 30.01% done
Loss: 210.10680681542505
Time: 5.34 s
Epoch 5: 40.10% done
Loss: 225.71060585975647
Time: 7.16 s
Epoch 5: 50.06% done
Loss: 208.13314765817503
Time: 8.95 s
Epoch 5: 60.03% done
Loss: 223.16902776307697
Time: 10.84 s
Epoch 5: 70.11% done
Loss: 201.80321049690247
Time: 12.78 s
Epoch 5: 80.08% done
Loss: 205.43619608577293
Time: 14.70 s
Epoch 5: 90.04% done
Loss: 217.51790131194682
Time: 16.65 s

Epoch 5 done
Epoch loss: 215.88923147216775

Time taken for epoch: 19.07 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 347.2438196168429

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 208.16755294799805
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 208.43173099469536
Time: 1.96 s
Epoch 6: 20.13% done
Loss: 211.9803615808487
Time: 3.76 s
Epoch 6: 30.06% done
Loss: 205.80636694461484
Time: 5.12 s
Epoch 6: 40.13% done
Loss: 212.64894127845764
Time: 7.05 s
Epoch 6: 50.06% done
Loss: 205.6762864968822
Time: 9.07 s
Epoch 6: 60.13% done
Loss: 206.3211054801941
Time: 11.13 s
Epoch 6: 70.06% done
Loss: 195.22059802767598
Time: 13.08 s
Epoch 6: 80.13% done
Loss: 207.18989115953445
Time: 14.96 s
Epoch 6: 90.06% done
Loss: 199.63034331044065
Time: 16.78 s

Epoch 6 done
Epoch loss: 204.7419257828462

Time taken for epoch: 19.14 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.00 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.55 s

Validation loss: 347.0553846290146

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 120.36344528198242
Time: 0.01 s
Epoch 7: 10.04% done
Loss: 201.27591628062575
Time: 2.06 s
Epoch 7: 20.08% done
Loss: 199.64955925941467
Time: 4.01 s
Epoch 7: 30.11% done
Loss: 195.21433529257774
Time: 5.47 s
Epoch 7: 40.03% done
Loss: 185.95890008950536
Time: 7.34 s
Epoch 7: 50.06% done
Loss: 185.09204924106598
Time: 9.25 s
Epoch 7: 60.10% done
Loss: 196.99924194812775
Time: 11.18 s
Epoch 7: 70.01% done
Loss: 190.60148534895498
Time: 13.18 s
Epoch 7: 80.05% done
Loss: 193.3118662238121
Time: 15.13 s
Epoch 7: 90.09% done
Loss: 188.19254583120346
Time: 17.05 s

Epoch 7 done
Epoch loss: 192.70682024088228

Time taken for epoch: 19.41 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.96048000584483

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 193.69272232055664
Time: 0.01 s
Epoch 8: 10.08% done
Loss: 176.9078983838045
Time: 1.97 s
Epoch 8: 20.03% done
Loss: 184.30178159399878
Time: 3.35 s
Epoch 8: 30.10% done
Loss: 191.88140213489532
Time: 5.35 s
Epoch 8: 40.05% done
Loss: 187.41180359562742
Time: 7.36 s
Epoch 8: 50.13% done
Loss: 182.39303785562515
Time: 9.22 s
Epoch 8: 60.08% done
Loss: 173.84292892262906
Time: 11.03 s
Epoch 8: 70.03% done
Loss: 177.61787851148517
Time: 12.89 s
Epoch 8: 80.10% done
Loss: 178.78724014759064
Time: 14.78 s
Epoch 8: 90.05% done
Loss: 183.08123914501334
Time: 16.72 s

Epoch 8 done
Epoch loss: 180.2818648781124

Time taken for epoch: 19.13 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.9023056825002

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.13% done
Loss: 225.48986434936523
Time: 0.01 s
Epoch 9: 10.06% done
Loss: 179.13785816747932
Time: 1.91 s
Epoch 9: 20.13% done
Loss: 172.8688308596611
Time: 3.80 s
Epoch 9: 30.06% done
Loss: 173.13926222958142
Time: 5.10 s
Epoch 9: 40.13% done
Loss: 175.96753585338593
Time: 7.11 s
Epoch 9: 50.06% done
Loss: 176.2037785143792
Time: 9.06 s
Epoch 9: 60.13% done
Loss: 164.24451863765717
Time: 11.08 s
Epoch 9: 70.06% done
Loss: 169.73869118509413
Time: 12.88 s
Epoch 9: 80.13% done
Loss: 172.7414694428444
Time: 14.79 s
Epoch 9: 90.06% done
Loss: 170.71764670343842
Time: 16.69 s

Epoch 9 done
Epoch loss: 172.77333033959573

Time taken for epoch: 19.04 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.8252148490021

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_9.pt

Regenerated paired data
Epoch 10: 0.13% done
Loss: 116.83516502380371
Time: 0.01 s
Epoch 10: 10.06% done
Loss: 163.92899679232247
Time: 1.29 s
Epoch 10: 20.13% done
Loss: 168.1561890244484
Time: 3.15 s
Epoch 10: 30.06% done
Loss: 167.26765424390382
Time: 4.99 s
Epoch 10: 40.13% done
Loss: 150.28089278936386
Time: 6.89 s
Epoch 10: 50.06% done
Loss: 156.7792912374569
Time: 8.80 s
Epoch 10: 60.13% done
Loss: 156.3834167122841
Time: 10.75 s
Epoch 10: 70.06% done
Loss: 166.0702323309983
Time: 12.78 s
Epoch 10: 80.13% done
Loss: 151.85171806812286
Time: 14.81 s
Epoch 10: 90.06% done
Loss: 167.0762531666816
Time: 16.70 s

Epoch 10 done
Epoch loss: 159.47775381915974

Time taken for epoch: 19.20 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.8025494312895

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_10.pt

Regenerated paired data
Epoch 11: 0.13% done
Loss: 97.83322334289551
Time: 0.01 s
Epoch 11: 10.06% done
Loss: 157.6657793189906
Time: 1.38 s
Epoch 11: 20.13% done
Loss: 146.61588902771473
Time: 3.23 s
Epoch 11: 30.06% done
Loss: 145.87570329255695
Time: 5.24 s
Epoch 11: 40.13% done
Loss: 158.81136628985405
Time: 7.21 s
Epoch 11: 50.06% done
Loss: 155.8638853362844
Time: 9.16 s
Epoch 11: 60.13% done
Loss: 155.9839128255844
Time: 11.15 s
Epoch 11: 70.06% done
Loss: 149.56290070014663
Time: 13.06 s
Epoch 11: 80.13% done
Loss: 154.18134105205536
Time: 14.94 s
Epoch 11: 90.06% done
Loss: 152.73708995384507
Time: 17.01 s

Epoch 11 done
Epoch loss: 153.18492152106086

Time taken for epoch: 19.45 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.7502855045208

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_11.pt

Regenerated paired data
Epoch 12: 0.13% done
Loss: 118.10033798217773
Time: 0.01 s
Epoch 12: 10.05% done
Loss: 148.55994064596635
Time: 1.43 s
Epoch 12: 20.10% done
Loss: 142.18158984184265
Time: 3.39 s
Epoch 12: 30.03% done
Loss: 150.50771387317513
Time: 5.32 s
Epoch 12: 40.08% done
Loss: 138.76791033148766
Time: 7.29 s
Epoch 12: 50.13% done
Loss: 146.1078342795372
Time: 9.12 s
Epoch 12: 60.05% done
Loss: 146.980088843575
Time: 10.99 s
Epoch 12: 70.10% done
Loss: 142.0881205201149
Time: 12.92 s
Epoch 12: 80.03% done
Loss: 146.77210741405244
Time: 14.99 s
Epoch 12: 90.08% done
Loss: 141.73502093553543
Time: 16.86 s

Epoch 12 done
Epoch loss: 144.4894279457217

Time taken for epoch: 19.25 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.6986017987348

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_12.pt

Regenerated paired data
Epoch 13: 0.13% done
Loss: 178.4952735900879
Time: 0.01 s
Epoch 13: 10.05% done
Loss: 138.2854115208493
Time: 1.93 s
Epoch 13: 20.10% done
Loss: 136.41191188494366
Time: 3.33 s
Epoch 13: 30.03% done
Loss: 144.62143080143989
Time: 5.26 s
Epoch 13: 40.08% done
Loss: 139.44963738694787
Time: 7.26 s
Epoch 13: 50.13% done
Loss: 141.6990150809288
Time: 9.22 s
Epoch 13: 60.05% done
Loss: 133.5439382587807
Time: 11.15 s
Epoch 13: 70.10% done
Loss: 128.8291538953781
Time: 13.06 s
Epoch 13: 80.03% done
Loss: 127.96056831184822
Time: 14.96 s
Epoch 13: 90.08% done
Loss: 137.7901307940483
Time: 16.86 s

Epoch 13 done
Epoch loss: 137.74586186062552

Time taken for epoch: 19.37 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.53 s

Validation loss: 346.60198239312655

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_13.pt

Regenerated paired data
Epoch 14: 0.13% done
Loss: 145.77566146850586
Time: 0.01 s
Epoch 14: 10.09% done
Loss: 138.5556674908988
Time: 1.99 s
Epoch 14: 20.05% done
Loss: 135.36844663982149
Time: 3.51 s
Epoch 14: 30.01% done
Loss: 133.91661330114437
Time: 5.44 s
Epoch 14: 40.10% done
Loss: 129.52629607915878
Time: 7.32 s
Epoch 14: 50.06% done
Loss: 128.4791488436204
Time: 9.20 s
Epoch 14: 60.03% done
Loss: 126.5945844408832
Time: 11.16 s
Epoch 14: 70.11% done
Loss: 126.09849399328232
Time: 13.20 s
Epoch 14: 80.08% done
Loss: 117.29617360271985
Time: 15.08 s
Epoch 14: 90.04% done
Loss: 135.78146490869642
Time: 16.96 s

Epoch 14 done
Epoch loss: 129.68287314472704

Time taken for epoch: 19.29 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.5671772196673

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_14.pt

Regenerated paired data
Epoch 15: 0.13% done
Loss: 127.15250968933105
Time: 0.01 s
Epoch 15: 10.08% done
Loss: 125.0426135787481
Time: 2.00 s
Epoch 15: 20.03% done
Loss: 126.05751979200146
Time: 3.40 s
Epoch 15: 30.10% done
Loss: 126.21283656358719
Time: 5.40 s
Epoch 15: 40.05% done
Loss: 126.52267244797719
Time: 7.41 s
Epoch 15: 50.13% done
Loss: 123.84242886304855
Time: 9.42 s
Epoch 15: 60.08% done
Loss: 129.74211904067027
Time: 11.27 s
Epoch 15: 70.03% done
Loss: 132.63433076158356
Time: 13.04 s
Epoch 15: 80.10% done
Loss: 126.69294568896294
Time: 14.97 s
Epoch 15: 90.05% done
Loss: 111.5271804906145
Time: 16.83 s

Epoch 15 done
Epoch loss: 124.86021309115124

Time taken for epoch: 19.18 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 346.5537589529286

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_15.pt

Regenerated paired data
Epoch 16: 0.13% done
Loss: 36.26606464385986
Time: 0.01 s
Epoch 16: 10.06% done
Loss: 116.87779028204423
Time: 1.83 s
Epoch 16: 20.13% done
Loss: 121.37694063782692
Time: 3.85 s
Epoch 16: 30.06% done
Loss: 116.0423135153855
Time: 5.12 s
Epoch 16: 40.13% done
Loss: 119.13199055194855
Time: 6.96 s
Epoch 16: 50.06% done
Loss: 118.38735121714917
Time: 8.81 s
Epoch 16: 60.13% done
Loss: 115.66018785784641
Time: 10.68 s
Epoch 16: 70.06% done
Loss: 111.8818047982228
Time: 12.64 s
Epoch 16: 80.13% done
Loss: 111.79533429443836
Time: 14.62 s
Epoch 16: 90.06% done
Loss: 116.39324496064005
Time: 16.63 s

Epoch 16 done
Epoch loss: 115.88188207764922

Time taken for epoch: 18.99 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 346.51327976282096

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_16.pt

Regenerated paired data
Epoch 17: 0.13% done
Loss: 87.10432052612305
Time: 0.01 s
Epoch 17: 10.08% done
Loss: 112.65128093429759
Time: 1.84 s
Epoch 17: 20.03% done
Loss: 116.56022120125687
Time: 3.12 s
Epoch 17: 30.10% done
Loss: 113.64211544394493
Time: 5.12 s
Epoch 17: 40.05% done
Loss: 106.50604284262356
Time: 7.05 s
Epoch 17: 50.13% done
Loss: 109.16814017295837
Time: 9.02 s
Epoch 17: 60.08% done
Loss: 115.72621716728693
Time: 10.94 s
Epoch 17: 70.03% done
Loss: 106.32092444768435
Time: 12.96 s
Epoch 17: 80.10% done
Loss: 113.7580361366272
Time: 14.89 s
Epoch 17: 90.05% done
Loss: 102.26945575279525
Time: 16.82 s

Epoch 17 done
Epoch loss: 111.3953107090606

Time taken for epoch: 19.09 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.00 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 346.5173038883486

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_17.pt

Regenerated paired data
Epoch 18: 0.13% done
Loss: 138.30202102661133
Time: 0.01 s
Epoch 18: 10.05% done
Loss: 111.52066623108297
Time: 1.85 s
Epoch 18: 20.10% done
Loss: 94.72717103362083
Time: 3.15 s
Epoch 18: 30.03% done
Loss: 108.74277976494801
Time: 5.03 s
Epoch 18: 40.08% done
Loss: 110.09063366055489
Time: 6.86 s
Epoch 18: 50.13% done
Loss: 106.62955981492996
Time: 8.79 s
Epoch 18: 60.05% done
Loss: 110.72067402586153
Time: 10.75 s
Epoch 18: 70.10% done
Loss: 105.84987366199493
Time: 12.77 s
Epoch 18: 80.03% done
Loss: 95.27980273282981
Time: 14.79 s
Epoch 18: 90.08% done
Loss: 105.07140383124352
Time: 16.69 s

Epoch 18 done
Epoch loss: 105.56481738036601

Time taken for epoch: 19.05 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.5704773819965

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_18.pt

Regenerated paired data
Epoch 19: 0.13% done
Loss: 109.81704711914062
Time: 0.01 s
Epoch 19: 10.08% done
Loss: 105.3690242767334
Time: 1.99 s
Epoch 19: 20.03% done
Loss: 105.34426375280452
Time: 3.29 s
Epoch 19: 30.10% done
Loss: 96.76751798391342
Time: 5.15 s
Epoch 19: 40.05% done
Loss: 99.26371559312072
Time: 7.06 s
Epoch 19: 50.13% done
Loss: 100.63063511252403
Time: 9.04 s
Epoch 19: 60.08% done
Loss: 93.38787023025223
Time: 10.90 s
Epoch 19: 70.03% done
Loss: 100.61357984059974
Time: 12.89 s
Epoch 19: 80.10% done
Loss: 90.78288635611534
Time: 14.70 s
Epoch 19: 90.05% done
Loss: 106.1714474158951
Time: 16.46 s

Epoch 19 done
Epoch loss: 100.16232247917118

Time taken for epoch: 18.72 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.10 s
Calculating validation loss: 40.22% done
Time: 0.22 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 346.5546124568884

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_19.pt

Regenerated paired data
Epoch 20: 0.13% done
Loss: 176.07568740844727
Time: 0.01 s
Epoch 20: 10.09% done
Loss: 91.49841492689109
Time: 1.30 s
Epoch 20: 20.05% done
Loss: 93.9484454106681
Time: 3.16 s
Epoch 20: 30.01% done
Loss: 88.22654289535329
Time: 4.99 s
Epoch 20: 40.10% done
Loss: 97.79280871152878
Time: 6.95 s
Epoch 20: 50.06% done
Loss: 99.215389459948
Time: 8.88 s
Epoch 20: 60.03% done
Loss: 96.35682766950583
Time: 10.78 s
Epoch 20: 70.11% done
Loss: 100.97267100214958
Time: 12.66 s
Epoch 20: 80.08% done
Loss: 95.28085418894321
Time: 14.49 s
Epoch 20: 90.04% done
Loss: 88.33258852677004
Time: 16.38 s

Epoch 20 done
Epoch loss: 95.97555761730135

Time taken for epoch: 18.85 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.30 s
Calculating validation loss: 60.87% done
Time: 0.43 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.47043031194937

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_20.pt

Regenerated paired data
Epoch 21: 0.13% done
Loss: 38.307297229766846
Time: 0.01 s
Epoch 21: 10.06% done
Loss: 88.31715034533151
Time: 1.90 s
Epoch 21: 20.13% done
Loss: 86.9405377805233
Time: 3.94 s
Epoch 21: 30.06% done
Loss: 87.55970746656007
Time: 5.31 s
Epoch 21: 40.13% done
Loss: 91.45880335569382
Time: 7.27 s
Epoch 21: 50.06% done
Loss: 86.73209549505499
Time: 9.21 s
Epoch 21: 60.13% done
Loss: 87.89434977372487
Time: 11.08 s
Epoch 21: 70.06% done
Loss: 96.02859998051125
Time: 12.97 s
Epoch 21: 80.13% done
Loss: 94.95070052146912
Time: 14.85 s
Epoch 21: 90.06% done
Loss: 88.04615844654131
Time: 16.65 s

Epoch 21 done
Epoch loss: 89.58248083446509

Time taken for epoch: 18.93 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 346.46534076635385

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_21.pt

Regenerated paired data
Epoch 22: 0.13% done
Loss: 60.27742385864258
Time: 0.01 s
Epoch 22: 10.08% done
Loss: 89.2822173275525
Time: 1.90 s
Epoch 22: 20.03% done
Loss: 87.93866912020913
Time: 3.27 s
Epoch 22: 30.10% done
Loss: 93.8446701169014
Time: 5.23 s
Epoch 22: 40.05% done
Loss: 85.15534583526322
Time: 7.08 s
Epoch 22: 50.13% done
Loss: 86.79163590073586
Time: 9.03 s
Epoch 22: 60.08% done
Loss: 84.5590977667819
Time: 10.95 s
Epoch 22: 70.03% done
Loss: 88.66910421395603
Time: 13.00 s
Epoch 22: 80.10% done
Loss: 83.72048246860504
Time: 14.94 s
Epoch 22: 90.05% done
Loss: 91.67189833484119
Time: 16.95 s

Epoch 22 done
Epoch loss: 87.84135930272997

Time taken for epoch: 19.26 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 346.55989976896757

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_22.pt

Regenerated paired data
Epoch 23: 0.13% done
Loss: 64.9225902557373
Time: 0.01 s
Epoch 23: 10.06% done
Loss: 87.17161489438406
Time: 1.85 s
Epoch 23: 20.13% done
Loss: 87.89239451289177
Time: 3.81 s
Epoch 23: 30.06% done
Loss: 83.56125019773653
Time: 5.26 s
Epoch 23: 40.13% done
Loss: 81.42318052053452
Time: 7.12 s
Epoch 23: 50.06% done
Loss: 83.33478788786297
Time: 8.94 s
Epoch 23: 60.13% done
Loss: 78.91413037478924
Time: 10.94 s
Epoch 23: 70.06% done
Loss: 74.64740194600343
Time: 12.98 s
Epoch 23: 80.13% done
Loss: 82.97167581319809
Time: 14.84 s
Epoch 23: 90.06% done
Loss: 83.22169976898387
Time: 16.65 s

Epoch 23 done
Epoch loss: 81.81746899681771

Time taken for epoch: 19.09 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.6073118258214

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_23.pt

Regenerated paired data
Epoch 24: 0.13% done
Loss: 75.65267086029053
Time: 0.01 s
Epoch 24: 10.06% done
Loss: 78.79441353276309
Time: 1.33 s
Epoch 24: 20.13% done
Loss: 77.5370372235775
Time: 3.29 s
Epoch 24: 30.06% done
Loss: 81.07704892943177
Time: 5.21 s
Epoch 24: 40.13% done
Loss: 89.99513432383537
Time: 7.20 s
Epoch 24: 50.06% done
Loss: 77.51103778428669
Time: 9.11 s
Epoch 24: 60.13% done
Loss: 75.75849717855453
Time: 11.04 s
Epoch 24: 70.06% done
Loss: 77.55082601233374
Time: 12.93 s
Epoch 24: 80.13% done
Loss: 90.0642626285553
Time: 14.83 s
Epoch 24: 90.06% done
Loss: 76.13705322712282
Time: 16.91 s

Epoch 24 done
Epoch loss: 80.39565687053359

Time taken for epoch: 19.27 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 346.6469022156536

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_24.pt

Regenerated paired data
Epoch 25: 0.13% done
Loss: 68.53644371032715
Time: 0.01 s
Epoch 25: 10.06% done
Loss: 82.35345952118499
Time: 1.81 s
Epoch 25: 20.13% done
Loss: 78.50767476856709
Time: 3.73 s
Epoch 25: 30.06% done
Loss: 76.84211753591707
Time: 5.01 s
Epoch 25: 40.13% done
Loss: 76.24331858754158
Time: 6.86 s
Epoch 25: 50.06% done
Loss: 74.85630500165722
Time: 8.77 s
Epoch 25: 60.13% done
Loss: 83.070406883955
Time: 10.68 s
Epoch 25: 70.06% done
Loss: 75.87310615974137
Time: 12.53 s
Epoch 25: 80.13% done
Loss: 71.24626728892326
Time: 14.48 s
Epoch 25: 90.06% done
Loss: 74.84550646588772
Time: 16.31 s

Epoch 25 done
Epoch loss: 77.02821463908789

Time taken for epoch: 18.66 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 346.70381713604587

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_25.pt

Regenerated paired data
Epoch 26: 0.13% done
Loss: 134.9231719970703
Time: 0.02 s
Epoch 26: 10.08% done
Loss: 71.40098780016356
Time: 1.79 s
Epoch 26: 20.03% done
Loss: 73.16943708854386
Time: 3.13 s
Epoch 26: 30.10% done
Loss: 71.76008248329163
Time: 5.06 s
Epoch 26: 40.05% done
Loss: 64.76279696331748
Time: 7.02 s
Epoch 26: 50.13% done
Loss: 69.43107550839584
Time: 8.97 s
Epoch 26: 60.08% done
Loss: 72.04881665072864
Time: 10.88 s
Epoch 26: 70.03% done
Loss: 76.5966063964216
Time: 12.77 s
Epoch 26: 80.10% done
Loss: 75.43358066678047
Time: 14.65 s
Epoch 26: 90.05% done
Loss: 74.77219421652299
Time: 16.50 s

Epoch 26 done
Epoch loss: 72.42226650592762

Time taken for epoch: 18.84 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 346.6517905048702

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_8192_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:29_checkpoint_epoch_26.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 346.46534076635385 at epoch 21

