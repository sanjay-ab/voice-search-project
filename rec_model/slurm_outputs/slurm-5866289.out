Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-07_11:11:20
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 500, patience: 5, learning rate: 1e-05
clip norm: 10, temperature: 0.15, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr: 1e-05
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 55.59 s
up_proj_dim: 512
output_dim: 512
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 7876608
Number of parameters in AWE model: 6825984
Number of parameters in other model: 1050624
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 546.0969924926758
Time: 1.35 s
Epoch 0: 10.04% done
Loss: 505.2848656569855
Time: 4.26 s
Epoch 0: 20.08% done
Loss: 493.8405120372772
Time: 6.16 s
Epoch 0: 30.11% done
Loss: 429.2158715724945
Time: 8.13 s
Epoch 0: 40.03% done
Loss: 411.3401738903191
Time: 9.51 s
Epoch 0: 50.06% done
Loss: 397.4391944408417
Time: 11.46 s
Epoch 0: 60.10% done
Loss: 383.37273359298706
Time: 13.43 s
Epoch 0: 70.01% done
Loss: 371.3912480994116
Time: 15.38 s
Epoch 0: 80.05% done
Loss: 358.61968129873276
Time: 17.31 s
Epoch 0: 90.09% done
Loss: 350.6667648553848
Time: 19.19 s

Epoch 0 done
Epoch loss: 405.4018799065048

Time taken for epoch: 21.45 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 351.49415290873986

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 331.61415100097656
Time: 0.02 s
Epoch 1: 10.06% done
Loss: 339.32591027851345
Time: 1.82 s
Epoch 1: 20.13% done
Loss: 324.3845148086548
Time: 3.64 s
Epoch 1: 30.06% done
Loss: 318.66740920875645
Time: 4.99 s
Epoch 1: 40.13% done
Loss: 325.718554576238
Time: 6.89 s
Epoch 1: 50.06% done
Loss: 318.0507671983936
Time: 8.74 s
Epoch 1: 60.13% done
Loss: 319.24223279953003
Time: 10.60 s
Epoch 1: 70.06% done
Loss: 307.6122114929972
Time: 12.43 s
Epoch 1: 80.13% done
Loss: 318.9592983722687
Time: 14.44 s
Epoch 1: 90.06% done
Loss: 316.91959646683705
Time: 16.40 s

Epoch 1 done
Epoch loss: 320.47579669552533

Time taken for epoch: 18.73 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 350.27361847352296

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 324.61597442626953
Time: 0.06 s
Epoch 2: 10.06% done
Loss: 299.88934649696836
Time: 1.95 s
Epoch 2: 20.13% done
Loss: 307.1747171680133
Time: 3.34 s
Epoch 2: 30.06% done
Loss: 307.6488567304008
Time: 5.20 s
Epoch 2: 40.13% done
Loss: 308.50859463214874
Time: 7.03 s
Epoch 2: 50.06% done
Loss: 300.4886648926554
Time: 8.85 s
Epoch 2: 60.13% done
Loss: 313.27953338623047
Time: 10.71 s
Epoch 2: 70.06% done
Loss: 297.82916612262966
Time: 12.62 s
Epoch 2: 80.13% done
Loss: 293.0699951648712
Time: 14.53 s
Epoch 2: 90.06% done
Loss: 298.9091216461568
Time: 16.54 s

Epoch 2 done
Epoch loss: 302.21259662740135

Time taken for epoch: 18.90 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.46 s

Validation loss: 349.7583603513413

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 293.2731246948242
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 297.7833489526676
Time: 1.83 s
Epoch 3: 20.13% done
Loss: 286.31785345077515
Time: 3.80 s
Epoch 3: 30.06% done
Loss: 293.92088443418095
Time: 5.21 s
Epoch 3: 40.13% done
Loss: 287.18509415785473
Time: 7.09 s
Epoch 3: 50.06% done
Loss: 286.44677632971656
Time: 8.96 s
Epoch 3: 60.13% done
Loss: 286.16708076000214
Time: 10.87 s
Epoch 3: 70.06% done
Loss: 283.8782595380952
Time: 12.81 s
Epoch 3: 80.13% done
Loss: 276.96440184116364
Time: 14.75 s
Epoch 3: 90.06% done
Loss: 282.3297836207136
Time: 16.63 s

Epoch 3 done
Epoch loss: 286.1749522351119

Time taken for epoch: 18.91 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.34 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 349.45122635882836

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 346.81385040283203
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 275.09547004216836
Time: 1.28 s
Epoch 4: 20.03% done
Loss: 274.48538574991346
Time: 3.34 s
Epoch 4: 30.10% done
Loss: 284.8034291267395
Time: 5.26 s
Epoch 4: 40.05% done
Loss: 276.1306437963172
Time: 7.27 s
Epoch 4: 50.13% done
Loss: 278.65890073776245
Time: 9.11 s
Epoch 4: 60.08% done
Loss: 263.963715879223
Time: 11.09 s
Epoch 4: 70.03% done
Loss: 273.9582009858723
Time: 13.01 s
Epoch 4: 80.10% done
Loss: 265.7159368991852
Time: 15.06 s
Epoch 4: 90.05% done
Loss: 263.9343775978571
Time: 16.96 s

Epoch 4 done
Epoch loss: 272.9152581043147

Time taken for epoch: 19.42 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 349.2149783908458

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 269.44080352783203
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 253.14644366880006
Time: 1.38 s
Epoch 5: 20.05% done
Loss: 265.7372675062735
Time: 3.36 s
Epoch 5: 30.01% done
Loss: 252.19965053510063
Time: 5.21 s
Epoch 5: 40.10% done
Loss: 263.1763334274292
Time: 7.06 s
Epoch 5: 50.06% done
Loss: 262.6080900506128
Time: 8.86 s
Epoch 5: 60.03% done
Loss: 268.2574677769142
Time: 10.83 s
Epoch 5: 70.11% done
Loss: 263.36106157302856
Time: 12.97 s
Epoch 5: 80.08% done
Loss: 266.90428238880787
Time: 14.87 s
Epoch 5: 90.04% done
Loss: 252.9225238365463
Time: 16.83 s

Epoch 5 done
Epoch loss: 261.02036532658195

Time taken for epoch: 19.35 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 348.9980574276137

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 197.77706146240234
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 256.22704252412046
Time: 1.80 s
Epoch 6: 20.13% done
Loss: 254.1217702627182
Time: 3.76 s
Epoch 6: 30.06% done
Loss: 242.88811273212676
Time: 5.15 s
Epoch 6: 40.13% done
Loss: 245.54572176933289
Time: 7.12 s
Epoch 6: 50.06% done
Loss: 245.86399633673173
Time: 9.08 s
Epoch 6: 60.13% done
Loss: 257.9112401008606
Time: 10.97 s
Epoch 6: 70.06% done
Loss: 251.68358247491378
Time: 12.79 s
Epoch 6: 80.13% done
Loss: 241.25964999198914
Time: 14.64 s
Epoch 6: 90.06% done
Loss: 250.51345221603972
Time: 16.47 s

Epoch 6 done
Epoch loss: 248.78963847880092

Time taken for epoch: 18.82 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 348.8647360041521

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 159.08281326293945
Time: 0.02 s
Epoch 7: 10.04% done
Loss: 257.1722147736368
Time: 1.33 s
Epoch 7: 20.08% done
Loss: 241.62842309474945
Time: 3.27 s
Epoch 7: 30.11% done
Loss: 245.7712643146515
Time: 5.27 s
Epoch 7: 40.03% done
Loss: 243.98470009429545
Time: 7.17 s
Epoch 7: 50.06% done
Loss: 236.3377525806427
Time: 9.17 s
Epoch 7: 60.10% done
Loss: 235.3425931930542
Time: 11.08 s
Epoch 7: 70.01% done
Loss: 242.82223182388498
Time: 13.05 s
Epoch 7: 80.05% done
Loss: 243.53866851329803
Time: 14.90 s
Epoch 7: 90.09% done
Loss: 247.72943484783173
Time: 16.77 s

Epoch 7 done
Epoch loss: 243.19872588106202

Time taken for epoch: 19.16 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.22 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.46 s

Validation loss: 348.7164422740107

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 267.81164169311523
Time: 0.01 s
Epoch 8: 10.08% done
Loss: 228.13375835177266
Time: 1.86 s
Epoch 8: 20.03% done
Loss: 232.4861976768397
Time: 3.21 s
Epoch 8: 30.10% done
Loss: 232.87882483005524
Time: 5.03 s
Epoch 8: 40.05% done
Loss: 236.35121731818478
Time: 7.02 s
Epoch 8: 50.13% done
Loss: 234.6794319152832
Time: 9.01 s
Epoch 8: 60.08% done
Loss: 238.07992669600475
Time: 10.99 s
Epoch 8: 70.03% done
Loss: 231.0437964588278
Time: 12.85 s
Epoch 8: 80.10% done
Loss: 226.07436060905457
Time: 14.79 s
Epoch 8: 90.05% done
Loss: 229.73700463017332
Time: 16.62 s

Epoch 8 done
Epoch loss: 231.67934520218571

Time taken for epoch: 19.17 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 348.6109800960707

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.13% done
Loss: 188.54450225830078
Time: 0.01 s
Epoch 9: 10.06% done
Loss: 224.0111940923119
Time: 1.35 s
Epoch 9: 20.13% done
Loss: 229.01062381267548
Time: 3.34 s
Epoch 9: 30.06% done
Loss: 217.8793682629549
Time: 5.25 s
Epoch 9: 40.13% done
Loss: 217.4109974205494
Time: 7.27 s
Epoch 9: 50.06% done
Loss: 224.72331397141082
Time: 9.06 s
Epoch 9: 60.13% done
Loss: 223.06761288642883
Time: 11.07 s
Epoch 9: 70.06% done
Loss: 224.6589361866818
Time: 13.04 s
Epoch 9: 80.13% done
Loss: 224.444633603096
Time: 15.09 s
Epoch 9: 90.06% done
Loss: 222.3036924193177
Time: 17.04 s

Epoch 9 done
Epoch loss: 222.38073780151782

Time taken for epoch: 19.37 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 348.6049943903218

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_9.pt

Regenerated paired data
Epoch 10: 0.13% done
Loss: 124.33771133422852
Time: 0.01 s
Epoch 10: 10.06% done
Loss: 217.6729181144811
Time: 1.94 s
Epoch 10: 20.13% done
Loss: 209.62703800201416
Time: 3.87 s
Epoch 10: 30.06% done
Loss: 206.0459451886672
Time: 5.25 s
Epoch 10: 40.13% done
Loss: 216.86890387535095
Time: 7.19 s
Epoch 10: 50.06% done
Loss: 217.1586583083189
Time: 9.12 s
Epoch 10: 60.13% done
Loss: 212.136727809906
Time: 11.01 s
Epoch 10: 70.06% done
Loss: 218.64360338524926
Time: 12.82 s
Epoch 10: 80.13% done
Loss: 212.02544224262238
Time: 14.68 s
Epoch 10: 90.06% done
Loss: 204.74597580825227
Time: 16.64 s

Epoch 10 done
Epoch loss: 212.54107592840614

Time taken for epoch: 19.01 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 348.52223102597225

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_10.pt

Regenerated paired data
Epoch 11: 0.13% done
Loss: 245.24185180664062
Time: 0.01 s
Epoch 11: 10.06% done
Loss: 200.76313767252088
Time: 2.01 s
Epoch 11: 20.13% done
Loss: 213.16685390472412
Time: 3.94 s
Epoch 11: 30.06% done
Loss: 204.29693246189552
Time: 5.36 s
Epoch 11: 40.13% done
Loss: 208.88217747211456
Time: 7.35 s
Epoch 11: 50.06% done
Loss: 205.40824914280373
Time: 9.26 s
Epoch 11: 60.13% done
Loss: 207.5512471795082
Time: 11.23 s
Epoch 11: 70.06% done
Loss: 201.8087454687191
Time: 13.16 s
Epoch 11: 80.13% done
Loss: 201.96676310896873
Time: 15.08 s
Epoch 11: 90.06% done
Loss: 203.3641468724118
Time: 16.97 s

Epoch 11 done
Epoch loss: 205.16108990615268

Time taken for epoch: 19.36 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 348.4521847531415

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_11.pt

Regenerated paired data
Epoch 12: 0.13% done
Loss: 118.48884582519531
Time: 0.01 s
Epoch 12: 10.05% done
Loss: 197.0318347592897
Time: 1.99 s
Epoch 12: 20.10% done
Loss: 199.28254956007004
Time: 3.92 s
Epoch 12: 30.03% done
Loss: 200.7300233237351
Time: 5.28 s
Epoch 12: 40.08% done
Loss: 189.0740933418274
Time: 7.13 s
Epoch 12: 50.13% done
Loss: 189.21282202005386
Time: 9.05 s
Epoch 12: 60.05% done
Loss: 198.49779043016554
Time: 10.88 s
Epoch 12: 70.10% done
Loss: 198.91131955385208
Time: 12.84 s
Epoch 12: 80.03% done
Loss: 198.80866162384612
Time: 14.73 s
Epoch 12: 90.08% done
Loss: 187.9582255333662
Time: 16.66 s

Epoch 12 done
Epoch loss: 195.7177858137006

Time taken for epoch: 19.15 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.57 s

Validation loss: 348.3705914884374

Time taken: 0.69 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_12.pt

Regenerated paired data
Epoch 13: 0.13% done
Loss: 143.65068435668945
Time: 0.01 s
Epoch 13: 10.05% done
Loss: 174.53001251703577
Time: 1.91 s
Epoch 13: 20.10% done
Loss: 192.64745146036148
Time: 3.82 s
Epoch 13: 30.03% done
Loss: 187.5516165962702
Time: 5.06 s
Epoch 13: 40.08% done
Loss: 203.9122470418612
Time: 6.97 s
Epoch 13: 50.13% done
Loss: 187.73261082172394
Time: 8.92 s
Epoch 13: 60.05% done
Loss: 181.8048570880407
Time: 10.88 s
Epoch 13: 70.10% done
Loss: 195.40211087465286
Time: 12.89 s
Epoch 13: 80.03% done
Loss: 179.47535351861882
Time: 14.91 s
Epoch 13: 90.08% done
Loss: 186.64673978090286
Time: 16.89 s

Epoch 13 done
Epoch loss: 187.8001764187062

Time taken for epoch: 19.28 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 348.3257839817931

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_13.pt

Regenerated paired data
Epoch 14: 0.13% done
Loss: 223.4735870361328
Time: 0.01 s
Epoch 14: 10.09% done
Loss: 179.2335790923879
Time: 1.82 s
Epoch 14: 20.05% done
Loss: 184.10440203509754
Time: 3.12 s
Epoch 14: 30.01% done
Loss: 179.17977749546873
Time: 4.95 s
Epoch 14: 40.10% done
Loss: 179.79992628097534
Time: 6.85 s
Epoch 14: 50.06% done
Loss: 178.02529129800917
Time: 8.79 s
Epoch 14: 60.03% done
Loss: 185.70636477651476
Time: 10.77 s
Epoch 14: 70.11% done
Loss: 179.78001183271408
Time: 12.70 s
Epoch 14: 80.08% done
Loss: 183.47207141827934
Time: 14.75 s
Epoch 14: 90.04% done
Loss: 185.9950065612793
Time: 16.81 s

Epoch 14 done
Epoch loss: 180.7221666651145

Time taken for epoch: 19.28 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 348.3918992678324

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_14.pt

Regenerated paired data
Epoch 15: 0.13% done
Loss: 139.51977729797363
Time: 0.01 s
Epoch 15: 10.08% done
Loss: 179.40339106547682
Time: 1.91 s
Epoch 15: 20.03% done
Loss: 165.60849219937867
Time: 3.29 s
Epoch 15: 30.10% done
Loss: 171.59552764892578
Time: 5.26 s
Epoch 15: 40.05% done
Loss: 183.15801185897635
Time: 7.30 s
Epoch 15: 50.13% done
Loss: 173.23411213234067
Time: 9.30 s
Epoch 15: 60.08% done
Loss: 167.8485765336435
Time: 11.18 s
Epoch 15: 70.03% done
Loss: 167.01505661010742
Time: 13.12 s
Epoch 15: 80.10% done
Loss: 159.23838132619858
Time: 15.00 s
Epoch 15: 90.05% done
Loss: 186.74392941631848
Time: 16.97 s

Epoch 15 done
Epoch loss: 173.13264353637132

Time taken for epoch: 19.36 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 348.4057713937068

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_15.pt

Regenerated paired data
Epoch 16: 0.13% done
Loss: 101.34640693664551
Time: 0.01 s
Epoch 16: 10.06% done
Loss: 172.59413200088693
Time: 1.95 s
Epoch 16: 20.13% done
Loss: 174.8721240758896
Time: 3.83 s
Epoch 16: 30.06% done
Loss: 168.37062612364565
Time: 5.24 s
Epoch 16: 40.13% done
Loss: 170.7603526711464
Time: 7.10 s
Epoch 16: 50.06% done
Loss: 154.77909776228893
Time: 8.90 s
Epoch 16: 60.13% done
Loss: 167.46622198820114
Time: 10.83 s
Epoch 16: 70.06% done
Loss: 170.06888153809535
Time: 12.74 s
Epoch 16: 80.13% done
Loss: 161.92587649822235
Time: 14.64 s
Epoch 16: 90.06% done
Loss: 151.91175328025335
Time: 16.50 s

Epoch 16 done
Epoch loss: 165.3292809908495

Time taken for epoch: 18.77 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 348.4084214680437

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_16.pt

Regenerated paired data
Epoch 17: 0.13% done
Loss: 168.62489700317383
Time: 0.01 s
Epoch 17: 10.08% done
Loss: 153.0626134631
Time: 1.87 s
Epoch 17: 20.03% done
Loss: 166.99288911457305
Time: 3.16 s
Epoch 17: 30.10% done
Loss: 160.92493652924895
Time: 5.04 s
Epoch 17: 40.05% done
Loss: 157.6524566698678
Time: 7.01 s
Epoch 17: 50.13% done
Loss: 165.32528001070023
Time: 8.98 s
Epoch 17: 60.08% done
Loss: 153.85745024379295
Time: 10.79 s
Epoch 17: 70.03% done
Loss: 160.43505499634563
Time: 12.64 s
Epoch 17: 80.10% done
Loss: 160.28123182058334
Time: 14.44 s
Epoch 17: 90.05% done
Loss: 152.35909872417207
Time: 16.32 s

Epoch 17 done
Epoch loss: 158.02190060541972

Time taken for epoch: 18.67 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 348.38351548581886

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_17.pt

Regenerated paired data
Epoch 18: 0.13% done
Loss: 62.69991874694824
Time: 0.01 s
Epoch 18: 10.05% done
Loss: 156.42805805689173
Time: 1.42 s
Epoch 18: 20.10% done
Loss: 148.39361423254013
Time: 3.30 s
Epoch 18: 30.03% done
Loss: 144.3579868123501
Time: 5.20 s
Epoch 18: 40.08% done
Loss: 149.3578326702118
Time: 7.09 s
Epoch 18: 50.13% done
Loss: 158.5088875889778
Time: 9.06 s
Epoch 18: 60.05% done
Loss: 147.52017528195924
Time: 11.02 s
Epoch 18: 70.10% done
Loss: 147.83556532859802
Time: 12.82 s
Epoch 18: 80.03% done
Loss: 160.85031947003137
Time: 14.70 s
Epoch 18: 90.08% done
Loss: 142.71382495760918
Time: 16.54 s

Epoch 18 done
Epoch loss: 150.11238458767608

Time taken for epoch: 18.85 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 348.4491517578346

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_1e-05_tmp_0.15_weight_decay_0.0/2024-08-07_11:11:20_checkpoint_epoch_18.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 348.3257839817931 at epoch 13

