Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-05_20:21:45
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 15, learning rate: 0.0005
clip norm: 10, temperature: 0.15, num pairs per batch: 2
time limit to create dataset: 600
weight decay: 0.01
awe_lr_division: 10
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 56.08 s
up_proj_dim: 512
output_dim: 128
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 7482624
Number of parameters in AWE model: 6825984
Number of parameters in other model: 656640
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.05% done
Loss: 425.00762939453125
Time: 0.66 s
Epoch 0: 10.03% done
Loss: 675.1441779461774
Time: 2.55 s
Epoch 0: 20.05% done
Loss: 431.56059020428205
Time: 4.29 s
Epoch 0: 30.03% done
Loss: 373.02544035694814
Time: 5.97 s
Epoch 0: 40.05% done
Loss: 380.731841427597
Time: 7.69 s
Epoch 0: 50.03% done
Loss: 353.14761256479255
Time: 10.04 s
Epoch 0: 60.05% done
Loss: 348.76780005256137
Time: 11.90 s
Epoch 0: 70.03% done
Loss: 354.33850061110775
Time: 13.74 s
Epoch 0: 80.05% done
Loss: 353.28335941736424
Time: 15.51 s
Epoch 0: 90.03% done
Loss: 354.0605860828149
Time: 17.25 s

Epoch 0 done
Epoch loss: 396.8359731509343

Time taken for epoch: 19.34 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.49 s

Validation loss: 410.9153510233678

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.05% done
Loss: 70.22700905799866
Time: 0.00 s
Epoch 1: 10.04% done
Loss: 308.43134434385735
Time: 1.26 s
Epoch 1: 20.03% done
Loss: 291.3378431339457
Time: 2.97 s
Epoch 1: 30.02% done
Loss: 290.76510533088384
Time: 4.66 s
Epoch 1: 40.01% done
Loss: 299.0688885251681
Time: 6.36 s
Epoch 1: 50.05% done
Loss: 266.5196338790146
Time: 8.10 s
Epoch 1: 60.04% done
Loss: 258.14028774773834
Time: 9.82 s
Epoch 1: 70.03% done
Loss: 257.0698342596491
Time: 11.57 s
Epoch 1: 80.02% done
Loss: 237.19531303139948
Time: 13.52 s
Epoch 1: 90.01% done
Loss: 279.1545009579171
Time: 15.26 s

Epoch 1 done
Epoch loss: 271.75372576305324

Time taken for epoch: 17.40 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 410.2234122950003

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.05% done
Loss: 43.62126290798187
Time: 0.01 s
Epoch 2: 10.04% done
Loss: 218.0595694090983
Time: 1.31 s
Epoch 2: 20.02% done
Loss: 192.0694889369035
Time: 3.07 s
Epoch 2: 30.01% done
Loss: 220.52621644065536
Time: 4.79 s
Epoch 2: 40.04% done
Loss: 227.57380491674846
Time: 6.49 s
Epoch 2: 50.03% done
Loss: 184.849456518023
Time: 8.15 s
Epoch 2: 60.01% done
Loss: 215.12192959398632
Time: 9.80 s
Epoch 2: 70.05% done
Loss: 182.70561683024442
Time: 11.53 s
Epoch 2: 80.03% done
Loss: 204.45398569784382
Time: 13.27 s
Epoch 2: 90.02% done
Loss: 176.54487573908585
Time: 15.02 s

Epoch 2 done
Epoch loss: 201.39140287058197

Time taken for epoch: 17.10 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.40 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 410.3408098220825

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.05% done
Loss: 5.712450295686722
Time: 0.01 s
Epoch 3: 10.03% done
Loss: 160.65189524393793
Time: 1.26 s
Epoch 3: 20.01% done
Loss: 172.41544833576137
Time: 2.94 s
Epoch 3: 30.04% done
Loss: 157.85877208070988
Time: 4.69 s
Epoch 3: 40.02% done
Loss: 163.35786445198036
Time: 6.36 s
Epoch 3: 50.05% done
Loss: 152.67057574293273
Time: 8.07 s
Epoch 3: 60.03% done
Loss: 153.22559843159686
Time: 9.83 s
Epoch 3: 70.01% done
Loss: 153.22326371502695
Time: 11.60 s
Epoch 3: 80.04% done
Loss: 153.01017574404352
Time: 13.35 s
Epoch 3: 90.02% done
Loss: 139.7788356063003
Time: 15.14 s

Epoch 3 done
Epoch loss: 155.43814971247377

Time taken for epoch: 17.25 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.11 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.35 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 410.6398792441832

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.05% done
Loss: 277.69672870635986
Time: 0.00 s
Epoch 4: 10.03% done
Loss: 155.6998948764169
Time: 1.21 s
Epoch 4: 20.05% done
Loss: 131.65027372557765
Time: 2.95 s
Epoch 4: 30.03% done
Loss: 119.40969197888568
Time: 4.73 s
Epoch 4: 40.05% done
Loss: 133.56099986382316
Time: 6.47 s
Epoch 4: 50.03% done
Loss: 125.05889971825209
Time: 8.18 s
Epoch 4: 60.05% done
Loss: 128.1527450766845
Time: 9.87 s
Epoch 4: 70.03% done
Loss: 118.43082078290406
Time: 11.57 s
Epoch 4: 80.05% done
Loss: 122.17479784359884
Time: 13.32 s
Epoch 4: 90.03% done
Loss: 141.47646424191242
Time: 15.07 s

Epoch 4 done
Epoch loss: 128.7436270650467

Time taken for epoch: 17.25 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.35 s
Calculating validation loss: 80.28% done
Time: 0.47 s

Validation loss: 411.1468576510018

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.05% done
Loss: 180.50609827041626
Time: 0.01 s
Epoch 5: 10.04% done
Loss: 110.44703575058116
Time: 1.20 s
Epoch 5: 20.03% done
Loss: 100.4290784125903
Time: 2.95 s
Epoch 5: 30.02% done
Loss: 104.92049378502851
Time: 4.69 s
Epoch 5: 40.01% done
Loss: 96.56427001034973
Time: 6.43 s
Epoch 5: 50.05% done
Loss: 102.09490242109976
Time: 8.16 s
Epoch 5: 60.04% done
Loss: 100.07526333246267
Time: 9.86 s
Epoch 5: 70.03% done
Loss: 104.81896361129151
Time: 11.57 s
Epoch 5: 80.02% done
Loss: 93.95928287436489
Time: 13.34 s
Epoch 5: 90.01% done
Loss: 101.85711233914274
Time: 15.02 s

Epoch 5 done
Epoch loss: 102.11266512309665

Time taken for epoch: 17.12 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.23 s
Calculating validation loss: 60.09% done
Time: 0.35 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 411.12528219135527

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.05% done
Loss: 9.651218354701996
Time: 0.00 s
Epoch 6: 10.03% done
Loss: 74.52268593008583
Time: 1.21 s
Epoch 6: 20.01% done
Loss: 80.30123714148301
Time: 2.93 s
Epoch 6: 30.04% done
Loss: 97.10364127438048
Time: 4.64 s
Epoch 6: 40.02% done
Loss: 89.58573060208988
Time: 6.38 s
Epoch 6: 50.05% done
Loss: 75.22229452771907
Time: 8.12 s
Epoch 6: 60.03% done
Loss: 78.51860115558587
Time: 9.86 s
Epoch 6: 70.01% done
Loss: 73.75078324553077
Time: 11.60 s
Epoch 6: 80.04% done
Loss: 74.43102002331061
Time: 13.38 s
Epoch 6: 90.02% done
Loss: 72.01436339463625
Time: 15.09 s

Epoch 6 done
Epoch loss: 79.16148306379206

Time taken for epoch: 17.20 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.02 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 411.2712790112977

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.05% done
Loss: 197.1156358718872
Time: 0.00 s
Epoch 7: 10.03% done
Loss: 90.66286749874402
Time: 1.30 s
Epoch 7: 20.01% done
Loss: 67.07725343952953
Time: 3.06 s
Epoch 7: 30.04% done
Loss: 75.87637425068036
Time: 4.75 s
Epoch 7: 40.02% done
Loss: 78.3152071808493
Time: 6.44 s
Epoch 7: 50.05% done
Loss: 71.11810228118121
Time: 8.20 s
Epoch 7: 60.03% done
Loss: 81.73413614018096
Time: 9.96 s
Epoch 7: 70.01% done
Loss: 84.4337190270913
Time: 11.73 s
Epoch 7: 80.04% done
Loss: 75.28960555557855
Time: 13.47 s
Epoch 7: 90.02% done
Loss: 68.23326607441737
Time: 15.25 s

Epoch 7 done
Epoch loss: 76.8227066077323

Time taken for epoch: 17.38 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.35 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 411.58222736568626

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.05% done
Loss: 43.777626752853394
Time: 0.01 s
Epoch 8: 10.04% done
Loss: 57.38540325934688
Time: 1.19 s
Epoch 8: 20.03% done
Loss: 61.817535761193454
Time: 2.86 s
Epoch 8: 30.02% done
Loss: 67.75861137706524
Time: 4.63 s
Epoch 8: 40.01% done
Loss: 74.42994192571878
Time: 6.38 s
Epoch 8: 50.05% done
Loss: 63.35385639264936
Time: 8.11 s
Epoch 8: 60.04% done
Loss: 59.939980607202294
Time: 9.91 s
Epoch 8: 70.03% done
Loss: 57.24080341157852
Time: 11.63 s
Epoch 8: 80.02% done
Loss: 60.04179771354591
Time: 13.35 s
Epoch 8: 90.01% done
Loss: 68.18308555450517
Time: 15.07 s

Epoch 8 done
Epoch loss: 63.25115014957057

Time taken for epoch: 17.20 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.50 s

Validation loss: 412.4716009568731

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.05% done
Loss: 27.842438220977783
Time: 0.01 s
Epoch 9: 10.03% done
Loss: 54.571029070219154
Time: 1.26 s
Epoch 9: 20.05% done
Loss: 61.24592199956708
Time: 3.00 s
Epoch 9: 30.03% done
Loss: 58.318437443048964
Time: 4.73 s
Epoch 9: 40.05% done
Loss: 58.49724405622999
Time: 6.48 s
Epoch 9: 50.03% done
Loss: 57.21106789535796
Time: 8.18 s
Epoch 9: 60.05% done
Loss: 51.2834178440591
Time: 9.88 s
Epoch 9: 70.03% done
Loss: 52.40386566605814
Time: 11.60 s
Epoch 9: 80.05% done
Loss: 62.1561224757221
Time: 13.34 s
Epoch 9: 90.03% done
Loss: 49.05556343905063
Time: 15.07 s

Epoch 9 done
Epoch loss: 56.34189722304155

Time taken for epoch: 17.16 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 412.6454016484252

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_9.pt

Regenerated paired data
Epoch 10: 0.05% done
Loss: 5.892004817724228
Time: 0.01 s
Epoch 10: 10.03% done
Loss: 52.63600615123663
Time: 1.35 s
Epoch 10: 20.01% done
Loss: 49.795449694096476
Time: 3.17 s
Epoch 10: 30.04% done
Loss: 58.42851188955782
Time: 4.90 s
Epoch 10: 40.02% done
Loss: 47.37615798567092
Time: 6.61 s
Epoch 10: 50.05% done
Loss: 44.45950079594631
Time: 8.33 s
Epoch 10: 60.03% done
Loss: 49.81628440983707
Time: 10.07 s
Epoch 10: 70.01% done
Loss: 43.631183264325514
Time: 11.82 s
Epoch 10: 80.04% done
Loss: 45.048735142877355
Time: 13.58 s
Epoch 10: 90.02% done
Loss: 51.71410927932352
Time: 15.33 s

Epoch 10 done
Epoch loss: 49.89213897349527

Time taken for epoch: 17.48 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.23 s
Calculating validation loss: 60.09% done
Time: 0.34 s
Calculating validation loss: 80.28% done
Time: 0.45 s

Validation loss: 412.3979424117902

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_10.pt

Regenerated paired data
Epoch 11: 0.05% done
Loss: 9.04163122177124
Time: 0.01 s
Epoch 11: 10.03% done
Loss: 51.20208528189158
Time: 1.24 s
Epoch 11: 20.05% done
Loss: 42.70118671064569
Time: 2.95 s
Epoch 11: 30.03% done
Loss: 42.298183091880865
Time: 4.75 s
Epoch 11: 40.05% done
Loss: 41.71581178635649
Time: 6.52 s
Epoch 11: 50.03% done
Loss: 54.0654908666961
Time: 8.32 s
Epoch 11: 60.05% done
Loss: 42.384178486615234
Time: 10.09 s
Epoch 11: 70.03% done
Loss: 42.40350440212742
Time: 11.80 s
Epoch 11: 80.05% done
Loss: 48.1600651016749
Time: 13.50 s
Epoch 11: 90.03% done
Loss: 40.04017710332956
Time: 15.23 s

Epoch 11 done
Epoch loss: 43.9994995070795

Time taken for epoch: 17.35 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.50 s

Validation loss: 412.72978695160754

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_11.pt

Regenerated paired data
Epoch 12: 0.05% done
Loss: 90.09091854095459
Time: 0.01 s
Epoch 12: 10.03% done
Loss: 40.70221832424233
Time: 1.26 s
Epoch 12: 20.05% done
Loss: 38.430955874600244
Time: 3.09 s
Epoch 12: 30.03% done
Loss: 39.136704671986855
Time: 4.84 s
Epoch 12: 40.05% done
Loss: 44.173430527873975
Time: 6.63 s
Epoch 12: 50.03% done
Loss: 35.73615281734465
Time: 8.36 s
Epoch 12: 60.05% done
Loss: 42.65554315216334
Time: 10.10 s
Epoch 12: 70.03% done
Loss: 44.501344670043
Time: 11.84 s
Epoch 12: 80.05% done
Loss: 40.82788053183962
Time: 13.60 s
Epoch 12: 90.03% done
Loss: 41.96482720435597
Time: 15.29 s

Epoch 12 done
Epoch loss: 40.86899925323593

Time taken for epoch: 17.48 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.50 s

Validation loss: 413.5011067084216

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_12.pt

Regenerated paired data
Epoch 13: 0.05% done
Loss: 397.5762367248535
Time: 0.01 s
Epoch 13: 10.04% done
Loss: 32.87638890500547
Time: 1.29 s
Epoch 13: 20.02% done
Loss: 29.91567303595895
Time: 3.03 s
Epoch 13: 30.01% done
Loss: 40.58180756915732
Time: 4.89 s
Epoch 13: 40.04% done
Loss: 31.71305452565892
Time: 6.69 s
Epoch 13: 50.03% done
Loss: 37.950016467154704
Time: 8.41 s
Epoch 13: 60.01% done
Loss: 31.401502603025296
Time: 10.17 s
Epoch 13: 70.05% done
Loss: 43.33169542430858
Time: 11.96 s
Epoch 13: 80.03% done
Loss: 34.84025149126396
Time: 13.71 s
Epoch 13: 90.02% done
Loss: 36.557270502765675
Time: 15.51 s

Epoch 13 done
Epoch loss: 35.28353909047993

Time taken for epoch: 17.70 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.40 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 413.60971457367646

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_13.pt

Regenerated paired data
Epoch 14: 0.05% done
Loss: 0.8141696453094482
Time: 0.00 s
Epoch 14: 10.03% done
Loss: 35.92948456214872
Time: 1.26 s
Epoch 14: 20.01% done
Loss: 38.093340916928334
Time: 3.01 s
Epoch 14: 30.04% done
Loss: 39.25133711546098
Time: 4.84 s
Epoch 14: 40.02% done
Loss: 32.72845888054093
Time: 6.69 s
Epoch 14: 50.05% done
Loss: 31.63625141915786
Time: 8.50 s
Epoch 14: 60.03% done
Loss: 30.54365387081752
Time: 10.25 s
Epoch 14: 70.01% done
Loss: 36.03438641599643
Time: 11.98 s
Epoch 14: 80.04% done
Loss: 29.295269689603415
Time: 13.79 s
Epoch 14: 90.02% done
Loss: 38.42111195383518
Time: 15.47 s

Epoch 14 done
Epoch loss: 34.34288736958596

Time taken for epoch: 17.60 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 413.781429977592

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_14.pt

Regenerated paired data
Epoch 15: 0.05% done
Loss: 227.96835899353027
Time: 0.01 s
Epoch 15: 10.03% done
Loss: 31.518117841768237
Time: 1.21 s
Epoch 15: 20.05% done
Loss: 24.643439060222153
Time: 3.01 s
Epoch 15: 30.03% done
Loss: 31.55001049856608
Time: 4.80 s
Epoch 15: 40.05% done
Loss: 22.37488443737419
Time: 6.53 s
Epoch 15: 50.03% done
Loss: 26.65132725278546
Time: 8.30 s
Epoch 15: 60.05% done
Loss: 25.363931258913393
Time: 10.10 s
Epoch 15: 70.03% done
Loss: 25.342585217805034
Time: 11.85 s
Epoch 15: 80.05% done
Loss: 29.52334168805839
Time: 13.60 s
Epoch 15: 90.03% done
Loss: 29.915277032602596
Time: 15.35 s

Epoch 15 done
Epoch loss: 27.544613677632388

Time taken for epoch: 17.46 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 413.02621900488475

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_15.pt

Regenerated paired data
Epoch 16: 0.05% done
Loss: 5.04472441971302
Time: 0.00 s
Epoch 16: 10.03% done
Loss: 30.71519357263701
Time: 1.25 s
Epoch 16: 20.01% done
Loss: 26.618371963544487
Time: 3.09 s
Epoch 16: 30.04% done
Loss: 35.50586147759175
Time: 4.84 s
Epoch 16: 40.02% done
Loss: 26.91227263298281
Time: 6.67 s
Epoch 16: 50.05% done
Loss: 25.588288858600766
Time: 8.45 s
Epoch 16: 60.03% done
Loss: 27.191964207649832
Time: 10.28 s
Epoch 16: 70.01% done
Loss: 30.617172712979915
Time: 12.04 s
Epoch 16: 80.04% done
Loss: 33.88389723937626
Time: 13.76 s
Epoch 16: 90.02% done
Loss: 28.13463295936923
Time: 15.45 s

Epoch 16 done
Epoch loss: 29.4406888552907

Time taken for epoch: 17.61 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.6290563355892

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:21:45_checkpoint_epoch_16.pt

Regenerated paired data
Validation loss has not improved for 15 epochs. Stopping training.
BEST VALIDATION LOSS: 410.2234122950003 at epoch 1

