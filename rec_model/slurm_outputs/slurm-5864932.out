Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-06_22:19:10
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 5, learning rate: 1e-06
clip norm: 10, temperature: 0.15, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr_division: 100
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 72.05 s
Number of parameters in model: 6825984

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 446.41761779785156
Time: 15.76 s
Epoch 0: 10.04% done
Loss: 482.19664440879336
Time: 21.11 s
Epoch 0: 20.08% done
Loss: 459.00033378601074
Time: 22.89 s
Epoch 0: 30.11% done
Loss: 387.69525694847107
Time: 24.29 s
Epoch 0: 40.03% done
Loss: 367.19148653971996
Time: 26.26 s
Epoch 0: 50.06% done
Loss: 345.8067356944084
Time: 28.22 s
Epoch 0: 60.10% done
Loss: 320.80561113357544
Time: 30.13 s
Epoch 0: 70.01% done
Loss: 329.62618381162235
Time: 32.15 s
Epoch 0: 80.05% done
Loss: 319.83992850780487
Time: 33.97 s
Epoch 0: 90.09% done
Loss: 314.22907614707947
Time: 35.85 s

Epoch 0 done
Epoch loss: 364.5657737042703

Time taken for epoch: 38.11 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 346.93427709565646

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 298.57812881469727
Time: 0.01 s
Epoch 1: 10.06% done
Loss: 311.9645450688616
Time: 1.52 s
Epoch 1: 20.13% done
Loss: 299.76520305871964
Time: 3.46 s
Epoch 1: 30.06% done
Loss: 307.0410792837666
Time: 5.40 s
Epoch 1: 40.13% done
Loss: 287.99993085861206
Time: 7.25 s
Epoch 1: 50.06% done
Loss: 282.07182365127755
Time: 9.12 s
Epoch 1: 60.13% done
Loss: 290.9941267967224
Time: 10.97 s
Epoch 1: 70.06% done
Loss: 283.3122963241384
Time: 12.89 s
Epoch 1: 80.13% done
Loss: 279.4240299463272
Time: 14.79 s
Epoch 1: 90.06% done
Loss: 261.0122487514834
Time: 16.59 s

Epoch 1 done
Epoch loss: 286.9815835752827

Time taken for epoch: 18.85 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 346.03836933771765

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 285.2341651916504
Time: 0.01 s
Epoch 2: 10.06% done
Loss: 260.434309239126
Time: 1.30 s
Epoch 2: 20.13% done
Loss: 251.9961963891983
Time: 3.12 s
Epoch 2: 30.06% done
Loss: 255.85945413082462
Time: 5.04 s
Epoch 2: 40.13% done
Loss: 245.46480759978294
Time: 6.99 s
Epoch 2: 50.06% done
Loss: 238.74138868307764
Time: 8.92 s
Epoch 2: 60.13% done
Loss: 234.92094600200653
Time: 10.90 s
Epoch 2: 70.06% done
Loss: 242.03319923787177
Time: 12.67 s
Epoch 2: 80.13% done
Loss: 232.2255783379078
Time: 14.62 s
Epoch 2: 90.06% done
Loss: 236.07514997071857
Time: 16.51 s

Epoch 2 done
Epoch loss: 243.38939700136646

Time taken for epoch: 18.79 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.53 s

Validation loss: 345.66608864328134

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 256.1014175415039
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 218.9869067035144
Time: 1.99 s
Epoch 3: 20.13% done
Loss: 207.1623387336731
Time: 3.93 s
Epoch 3: 30.06% done
Loss: 218.53217583668382
Time: 5.27 s
Epoch 3: 40.13% done
Loss: 211.60529112815857
Time: 7.14 s
Epoch 3: 50.06% done
Loss: 194.27427979964244
Time: 9.03 s
Epoch 3: 60.13% done
Loss: 201.51114972432455
Time: 10.82 s
Epoch 3: 70.06% done
Loss: 199.9417150473293
Time: 12.66 s
Epoch 3: 80.13% done
Loss: 187.682488322258
Time: 14.59 s
Epoch 3: 90.06% done
Loss: 208.1138326548323
Time: 16.48 s

Epoch 3 done
Epoch loss: 204.8454488858237

Time taken for epoch: 18.77 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 345.5425030597742

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 96.92546844482422
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 185.98688837848132
Time: 1.38 s
Epoch 4: 20.03% done
Loss: 184.03356666806377
Time: 3.27 s
Epoch 4: 30.10% done
Loss: 176.77872562408447
Time: 5.13 s
Epoch 4: 40.05% done
Loss: 175.51591112643857
Time: 7.01 s
Epoch 4: 50.13% done
Loss: 166.04134088754654
Time: 8.91 s
Epoch 4: 60.08% done
Loss: 168.87335125404067
Time: 10.82 s
Epoch 4: 70.03% done
Loss: 164.6139831180814
Time: 12.66 s
Epoch 4: 80.10% done
Loss: 161.29959654808044
Time: 14.52 s
Epoch 4: 90.05% done
Loss: 166.15606054475035
Time: 16.36 s

Epoch 4 done
Epoch loss: 170.4707816986353

Time taken for epoch: 18.65 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.34 s
Calculating validation loss: 80.43% done
Time: 0.46 s

Validation loss: 345.7118291094683

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 96.61099433898926
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 153.4865637066998
Time: 1.84 s
Epoch 5: 20.05% done
Loss: 156.47174334224266
Time: 3.31 s
Epoch 5: 30.01% done
Loss: 140.92824664296984
Time: 5.22 s
Epoch 5: 40.10% done
Loss: 150.25235937163234
Time: 7.11 s
Epoch 5: 50.06% done
Loss: 137.08409496500522
Time: 8.96 s
Epoch 5: 60.03% done
Loss: 131.73582909982414
Time: 10.76 s
Epoch 5: 70.11% done
Loss: 138.60143200680614
Time: 12.73 s
Epoch 5: 80.08% done
Loss: 140.4697112795673
Time: 14.68 s
Epoch 5: 90.04% done
Loss: 133.63751897329016
Time: 16.63 s

Epoch 5 done
Epoch loss: 141.9913170699965

Time taken for epoch: 18.91 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 346.0066660418027

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 121.24730110168457
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 120.0178930125659
Time: 1.82 s
Epoch 6: 20.13% done
Loss: 126.53746819589287
Time: 3.23 s
Epoch 6: 30.06% done
Loss: 116.93748594839361
Time: 5.12 s
Epoch 6: 40.13% done
Loss: 114.23977982997894
Time: 7.03 s
Epoch 6: 50.06% done
Loss: 120.01553007319004
Time: 8.85 s
Epoch 6: 60.13% done
Loss: 121.68763327598572
Time: 10.79 s
Epoch 6: 70.06% done
Loss: 110.58414254007461
Time: 12.65 s
Epoch 6: 80.13% done
Loss: 119.20428347587585
Time: 14.46 s
Epoch 6: 90.06% done
Loss: 124.00958598414555
Time: 16.26 s

Epoch 6 done
Epoch loss: 118.6384561160263

Time taken for epoch: 18.59 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.18 s
Calculating validation loss: 40.22% done
Time: 0.30 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 346.3233720046886

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 131.44709587097168
Time: 0.01 s
Epoch 7: 10.04% done
Loss: 97.51758759534812
Time: 1.93 s
Epoch 7: 20.08% done
Loss: 103.87022891640663
Time: 3.26 s
Epoch 7: 30.11% done
Loss: 103.72989764809608
Time: 5.13 s
Epoch 7: 40.03% done
Loss: 109.43474461760702
Time: 7.03 s
Epoch 7: 50.06% done
Loss: 105.90850877761841
Time: 9.01 s
Epoch 7: 60.10% done
Loss: 95.09278872609138
Time: 10.87 s
Epoch 7: 70.01% done
Loss: 106.38339296171937
Time: 12.71 s
Epoch 7: 80.05% done
Loss: 99.91181281208992
Time: 14.54 s
Epoch 7: 90.09% done
Loss: 106.65939281880856
Time: 16.50 s

Epoch 7 done
Epoch loss: 102.61873695103506

Time taken for epoch: 18.79 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.6612119605576

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 56.06142044067383
Time: 0.01 s
Epoch 8: 10.08% done
Loss: 81.49900428856475
Time: 1.87 s
Epoch 8: 20.03% done
Loss: 89.68441404873812
Time: 3.19 s
Epoch 8: 30.10% done
Loss: 86.79375496506691
Time: 4.98 s
Epoch 8: 40.05% done
Loss: 87.74277643815375
Time: 6.81 s
Epoch 8: 50.13% done
Loss: 84.73375815153122
Time: 8.73 s
Epoch 8: 60.08% done
Loss: 82.10261387161061
Time: 10.62 s
Epoch 8: 70.03% done
Loss: 86.9180843799929
Time: 12.50 s
Epoch 8: 80.10% done
Loss: 82.76640306413174
Time: 14.41 s
Epoch 8: 90.05% done
Loss: 96.95292279690126
Time: 16.39 s

Epoch 8 done
Epoch loss: 86.6209645675672

Time taken for epoch: 18.72 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.53 s

Validation loss: 346.9814039140508

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.15/2024-08-06_22:19:10_checkpoint_epoch_8.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 345.5425030597742 at epoch 3

