Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-06_22:19:10
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 5, learning rate: 0.0001
clip norm: 10, temperature: 0.07, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr_division: 100
temperature: 0.07

Created paired data
Created paired data
Time taken to create datasets: 72.48 s
Number of parameters in model: 6825984

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 647.8575134277344
Time: 15.76 s
Epoch 0: 10.04% done
Loss: 751.585137331033
Time: 21.12 s
Epoch 0: 20.08% done
Loss: 710.5417981147766
Time: 22.97 s
Epoch 0: 30.11% done
Loss: 537.9586259921392
Time: 24.52 s
Epoch 0: 40.03% done
Loss: 462.4264966988865
Time: 26.68 s
Epoch 0: 50.06% done
Loss: 405.8859401345253
Time: 28.81 s
Epoch 0: 60.10% done
Loss: 346.7899844646454
Time: 30.80 s
Epoch 0: 70.01% done
Loss: 343.07945315847917
Time: 32.74 s
Epoch 0: 80.05% done
Loss: 329.4860512018204
Time: 34.63 s
Epoch 0: 90.09% done
Loss: 324.3683433532715
Time: 36.56 s

Epoch 0 done
Epoch loss: 453.9796630292195

Time taken for epoch: 38.88 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.05 s
Calculating validation loss: 20.65% done
Time: 0.24 s
Calculating validation loss: 40.22% done
Time: 0.36 s
Calculating validation loss: 60.87% done
Time: 0.49 s
Calculating validation loss: 80.43% done
Time: 0.61 s

Validation loss: 340.06314189537716

Time taken: 0.72 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 311.3277053833008
Time: 0.01 s
Epoch 1: 10.06% done
Loss: 323.10762399359595
Time: 1.49 s
Epoch 1: 20.13% done
Loss: 316.1891623735428
Time: 3.53 s
Epoch 1: 30.06% done
Loss: 320.42539499983
Time: 5.57 s
Epoch 1: 40.13% done
Loss: 303.87631344795227
Time: 7.44 s
Epoch 1: 50.06% done
Loss: 298.7824519676498
Time: 9.40 s
Epoch 1: 60.13% done
Loss: 302.326318025589
Time: 11.28 s
Epoch 1: 70.06% done
Loss: 294.3150824534742
Time: 13.24 s
Epoch 1: 80.13% done
Loss: 288.64787566661835
Time: 15.20 s
Epoch 1: 90.06% done
Loss: 268.55952733679663
Time: 17.31 s

Epoch 1 done
Epoch loss: 299.49052788476524

Time taken for epoch: 19.55 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.47 s
Calculating validation loss: 80.43% done
Time: 0.59 s

Validation loss: 338.0689322775689

Time taken: 0.72 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 316.0880470275879
Time: 0.02 s
Epoch 2: 10.06% done
Loss: 265.63173555623626
Time: 1.37 s
Epoch 2: 20.13% done
Loss: 254.81080877780914
Time: 3.24 s
Epoch 2: 30.06% done
Loss: 258.4883140612252
Time: 5.20 s
Epoch 2: 40.13% done
Loss: 249.3149033486843
Time: 7.08 s
Epoch 2: 50.06% done
Loss: 244.3966782243946
Time: 9.08 s
Epoch 2: 60.13% done
Loss: 231.4442102909088
Time: 11.09 s
Epoch 2: 70.06% done
Loss: 243.43624048595186
Time: 12.90 s
Epoch 2: 80.13% done
Loss: 231.45336762070656
Time: 14.89 s
Epoch 2: 90.06% done
Loss: 235.62280353111558
Time: 16.83 s

Epoch 2 done
Epoch loss: 245.14987287181478

Time taken for epoch: 19.19 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 337.1557411421901

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 301.0792922973633
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 214.06789930560922
Time: 2.01 s
Epoch 3: 20.13% done
Loss: 199.47021090984344
Time: 4.04 s
Epoch 3: 30.06% done
Loss: 214.11290838748593
Time: 5.52 s
Epoch 3: 40.13% done
Loss: 200.61317098140717
Time: 7.58 s
Epoch 3: 50.06% done
Loss: 181.78741584850263
Time: 9.52 s
Epoch 3: 60.13% done
Loss: 187.94927952686945
Time: 11.37 s
Epoch 3: 70.06% done
Loss: 193.10379281828676
Time: 13.31 s
Epoch 3: 80.13% done
Loss: 175.66715919971466
Time: 15.20 s
Epoch 3: 90.06% done
Loss: 198.32785057116158
Time: 17.16 s

Epoch 3 done
Epoch loss: 195.2849835819668

Time taken for epoch: 19.58 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.17 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 336.62043402160424

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 51.062612533569336
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 169.91745013979417
Time: 1.40 s
Epoch 4: 20.03% done
Loss: 168.35394346261327
Time: 3.32 s
Epoch 4: 30.10% done
Loss: 155.0505066215992
Time: 5.19 s
Epoch 4: 40.05% done
Loss: 161.39589068255847
Time: 7.14 s
Epoch 4: 50.13% done
Loss: 144.56307721138
Time: 9.08 s
Epoch 4: 60.08% done
Loss: 143.37587570842308
Time: 11.04 s
Epoch 4: 70.03% done
Loss: 140.64787608158738
Time: 12.88 s
Epoch 4: 80.10% done
Loss: 143.2681558728218
Time: 14.74 s
Epoch 4: 90.05% done
Loss: 148.61054245429702
Time: 16.72 s

Epoch 4 done
Epoch loss: 150.03802128717942

Time taken for epoch: 19.03 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 336.9031196573506

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 39.73525285720825
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 126.32001138940642
Time: 2.02 s
Epoch 5: 20.05% done
Loss: 130.1745412772215
Time: 3.57 s
Epoch 5: 30.01% done
Loss: 110.2112595944465
Time: 5.55 s
Epoch 5: 40.10% done
Loss: 125.85540402296465
Time: 7.45 s
Epoch 5: 50.06% done
Loss: 110.36787125883224
Time: 9.32 s
Epoch 5: 60.03% done
Loss: 97.39479249036765
Time: 11.27 s
Epoch 5: 70.11% done
Loss: 115.3432292107027
Time: 13.21 s
Epoch 5: 80.08% done
Loss: 107.23282978504518
Time: 15.33 s
Epoch 5: 90.04% done
Loss: 100.31753711308104
Time: 17.27 s

Epoch 5 done
Epoch loss: 113.2851107584484

Time taken for epoch: 19.62 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.22 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 337.30230675227403

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 86.28389358520508
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 86.23934095418906
Time: 1.86 s
Epoch 6: 20.13% done
Loss: 92.66516436755774
Time: 3.23 s
Epoch 6: 30.06% done
Loss: 84.32458859455737
Time: 5.21 s
Epoch 6: 40.13% done
Loss: 80.24717047065496
Time: 7.20 s
Epoch 6: 50.06% done
Loss: 84.88982848728759
Time: 9.16 s
Epoch 6: 60.13% done
Loss: 83.74588327109814
Time: 11.19 s
Epoch 6: 70.06% done
Loss: 71.56674889069569
Time: 13.15 s
Epoch 6: 80.13% done
Loss: 89.49047265574336
Time: 15.03 s
Epoch 6: 90.06% done
Loss: 93.97849044845074
Time: 16.95 s

Epoch 6 done
Epoch loss: 84.88956453114947

Time taken for epoch: 19.45 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.53 s

Validation loss: 337.65399658161664

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 133.31212043762207
Time: 0.01 s
Epoch 7: 10.04% done
Loss: 60.44690737241431
Time: 1.95 s
Epoch 7: 20.08% done
Loss: 69.09972090274096
Time: 3.33 s
Epoch 7: 30.11% done
Loss: 65.73188967257738
Time: 5.22 s
Epoch 7: 40.03% done
Loss: 75.65638401840306
Time: 7.15 s
Epoch 7: 50.06% done
Loss: 69.50101752579212
Time: 9.10 s
Epoch 7: 60.10% done
Loss: 58.11905039101839
Time: 11.07 s
Epoch 7: 70.01% done
Loss: 71.25382925890669
Time: 13.01 s
Epoch 7: 80.05% done
Loss: 62.90591094084084
Time: 14.84 s
Epoch 7: 90.09% done
Loss: 66.99841966852546
Time: 16.82 s

Epoch 7 done
Epoch loss: 66.14276730624117

Time taken for epoch: 19.08 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.29 s
Calculating validation loss: 60.87% done
Time: 0.43 s
Calculating validation loss: 80.43% done
Time: 0.55 s

Validation loss: 338.2560598677483

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 13.277441263198853
Time: 0.01 s
Epoch 8: 10.08% done
Loss: 46.20068346869342
Time: 1.93 s
Epoch 8: 20.03% done
Loss: 47.943026666777044
Time: 3.37 s
Epoch 8: 30.10% done
Loss: 47.55496411770582
Time: 5.26 s
Epoch 8: 40.05% done
Loss: 51.60240729146869
Time: 7.19 s
Epoch 8: 50.13% done
Loss: 48.105682564899325
Time: 9.17 s
Epoch 8: 60.08% done
Loss: 50.46926002336454
Time: 11.19 s
Epoch 8: 70.03% done
Loss: 46.38451045449776
Time: 13.20 s
Epoch 8: 80.10% done
Loss: 43.75140989944339
Time: 15.21 s
Epoch 8: 90.05% done
Loss: 58.260421602031855
Time: 17.21 s

Epoch 8 done
Epoch loss: 49.482084298983374

Time taken for epoch: 19.61 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.29 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 338.62520335377127

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.0001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_8.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 336.62043402160424 at epoch 3

