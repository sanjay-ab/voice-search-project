Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-07_11:10:58
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 500, patience: 5, learning rate: 0.001
clip norm: 10, temperature: 0.07, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr: 1e-05
temperature: 0.07

Created paired data
Created paired data
Time taken to create datasets: 69.60 s
up_proj_dim: 512
output_dim: 512
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 7876608
Number of parameters in AWE model: 6825984
Number of parameters in other model: 1050624
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 918.9897155761719
Time: 2.19 s
Epoch 0: 10.04% done
Loss: 711.7311975020397
Time: 5.62 s
Epoch 0: 20.08% done
Loss: 454.1937327384949
Time: 7.46 s
Epoch 0: 30.11% done
Loss: 365.71385645866394
Time: 9.93 s
Epoch 0: 40.03% done
Loss: 358.46189064315604
Time: 11.24 s
Epoch 0: 50.06% done
Loss: 338.783061504364
Time: 13.12 s
Epoch 0: 60.10% done
Loss: 329.3944764137268
Time: 15.07 s
Epoch 0: 70.01% done
Loss: 328.0322027407618
Time: 16.95 s
Epoch 0: 80.05% done
Loss: 316.79856872558594
Time: 18.80 s
Epoch 0: 90.09% done
Loss: 322.03082543611526
Time: 20.60 s

Epoch 0 done
Epoch loss: 384.72443866809505

Time taken for epoch: 22.99 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 353.0196765367536

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 325.49610137939453
Time: 0.02 s
Epoch 1: 10.06% done
Loss: 305.0105425677722
Time: 1.82 s
Epoch 1: 20.13% done
Loss: 299.4808282852173
Time: 3.63 s
Epoch 1: 30.06% done
Loss: 297.9377264916142
Time: 4.97 s
Epoch 1: 40.13% done
Loss: 297.99467090765637
Time: 6.97 s
Epoch 1: 50.06% done
Loss: 290.1925074903271
Time: 8.91 s
Epoch 1: 60.13% done
Loss: 294.5617125034332
Time: 10.77 s
Epoch 1: 70.06% done
Loss: 272.4615757374824
Time: 12.57 s
Epoch 1: 80.13% done
Loss: 293.85423469543457
Time: 14.46 s
Epoch 1: 90.06% done
Loss: 295.0227812272084
Time: 16.37 s

Epoch 1 done
Epoch loss: 294.08737736028195

Time taken for epoch: 18.60 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 350.3393380538277

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 278.84057998657227
Time: 0.01 s
Epoch 2: 10.06% done
Loss: 265.14296181594267
Time: 1.86 s
Epoch 2: 20.13% done
Loss: 268.3000508447488
Time: 3.22 s
Epoch 2: 30.06% done
Loss: 282.1352048463459
Time: 5.03 s
Epoch 2: 40.13% done
Loss: 276.13250547647476
Time: 6.86 s
Epoch 2: 50.06% done
Loss: 263.6719442922858
Time: 8.68 s
Epoch 2: 60.13% done
Loss: 274.55097806453705
Time: 10.58 s
Epoch 2: 70.06% done
Loss: 251.41746315775038
Time: 12.47 s
Epoch 2: 80.13% done
Loss: 257.7340854406357
Time: 14.36 s
Epoch 2: 90.06% done
Loss: 264.5898269098016
Time: 16.28 s

Epoch 2 done
Epoch loss: 266.7482580138952

Time taken for epoch: 18.53 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.00 s
Calculating validation loss: 20.65% done
Time: 0.11 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.34 s
Calculating validation loss: 80.43% done
Time: 0.45 s

Validation loss: 348.9354497798975

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 239.6663475036621
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 257.7112888384469
Time: 1.86 s
Epoch 3: 20.13% done
Loss: 240.344233751297
Time: 3.82 s
Epoch 3: 30.06% done
Loss: 250.4314501074296
Time: 5.15 s
Epoch 3: 40.13% done
Loss: 232.88339861234027
Time: 6.96 s
Epoch 3: 50.06% done
Loss: 244.86866069745415
Time: 8.76 s
Epoch 3: 60.13% done
Loss: 243.08126893639565
Time: 10.61 s
Epoch 3: 70.06% done
Loss: 243.34877629823322
Time: 12.50 s
Epoch 3: 80.13% done
Loss: 220.8216546177864
Time: 14.39 s
Epoch 3: 90.06% done
Loss: 238.58006103129327
Time: 16.36 s

Epoch 3 done
Epoch loss: 240.8517669571771

Time taken for epoch: 18.74 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.60 s

Validation loss: 348.4437011981356

Time taken: 0.72 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 321.75025939941406
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 214.72577758982212
Time: 1.36 s
Epoch 4: 20.03% done
Loss: 222.3809300796895
Time: 3.30 s
Epoch 4: 30.10% done
Loss: 239.13119155168533
Time: 5.24 s
Epoch 4: 40.05% done
Loss: 220.37152405026592
Time: 7.12 s
Epoch 4: 50.13% done
Loss: 239.67453902959824
Time: 8.99 s
Epoch 4: 60.08% done
Loss: 214.29474625406385
Time: 10.97 s
Epoch 4: 70.03% done
Loss: 212.3472321787967
Time: 12.88 s
Epoch 4: 80.10% done
Loss: 208.94511818885803
Time: 14.86 s
Epoch 4: 90.05% done
Loss: 217.53896695149095
Time: 16.76 s

Epoch 4 done
Epoch loss: 219.74963641099126

Time taken for epoch: 19.20 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 348.32314163014513

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 213.59399795532227
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 188.83225734475292
Time: 1.35 s
Epoch 5: 20.05% done
Loss: 198.60502744022804
Time: 3.19 s
Epoch 5: 30.01% done
Loss: 190.19988648499114
Time: 5.03 s
Epoch 5: 40.10% done
Loss: 201.27126759290695
Time: 6.84 s
Epoch 5: 50.06% done
Loss: 202.70402202123327
Time: 8.62 s
Epoch 5: 60.03% done
Loss: 201.60334306427194
Time: 10.58 s
Epoch 5: 70.11% done
Loss: 203.0205116868019
Time: 12.59 s
Epoch 5: 80.08% done
Loss: 201.75065873544426
Time: 14.45 s
Epoch 5: 90.04% done
Loss: 184.13647929324378
Time: 16.35 s

Epoch 5 done
Epoch loss: 197.1727546414391

Time taken for epoch: 18.87 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.56 s

Validation loss: 348.69418154592097

Time taken: 0.69 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 183.02282333374023
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 188.58204008657722
Time: 1.86 s
Epoch 6: 20.13% done
Loss: 177.1336032897234
Time: 3.86 s
Epoch 6: 30.06% done
Loss: 172.03278221661532
Time: 5.29 s
Epoch 6: 40.13% done
Loss: 179.0587297976017
Time: 7.21 s
Epoch 6: 50.06% done
Loss: 168.84181288224232
Time: 9.13 s
Epoch 6: 60.13% done
Loss: 173.85855075716972
Time: 11.10 s
Epoch 6: 70.06% done
Loss: 169.36253366591055
Time: 12.94 s
Epoch 6: 80.13% done
Loss: 171.08097022771835
Time: 14.79 s
Epoch 6: 90.06% done
Loss: 189.50198559821408
Time: 16.61 s

Epoch 6 done
Epoch loss: 176.18635398550134

Time taken for epoch: 18.91 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.11 s
Calculating validation loss: 40.22% done
Time: 0.22 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.46 s

Validation loss: 348.9762911243715

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 79.08368110656738
Time: 0.02 s
Epoch 7: 10.04% done
Loss: 154.6731963338731
Time: 1.31 s
Epoch 7: 20.08% done
Loss: 165.7902729511261
Time: 3.17 s
Epoch 7: 30.11% done
Loss: 168.20527735352516
Time: 5.07 s
Epoch 7: 40.03% done
Loss: 173.7399858733018
Time: 6.90 s
Epoch 7: 50.06% done
Loss: 168.10968628525734
Time: 8.90 s
Epoch 7: 60.10% done
Loss: 166.74592956155539
Time: 10.84 s
Epoch 7: 70.01% done
Loss: 156.9796171067636
Time: 12.81 s
Epoch 7: 80.05% done
Loss: 164.6541953906417
Time: 14.65 s
Epoch 7: 90.09% done
Loss: 162.53191263973713
Time: 16.60 s

Epoch 7 done
Epoch loss: 166.03114094876537

Time taken for epoch: 18.96 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.22 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 349.14405318274015

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 115.64986228942871
Time: 0.01 s
Epoch 8: 10.08% done
Loss: 141.64940785758102
Time: 1.90 s
Epoch 8: 20.03% done
Loss: 149.2638654648503
Time: 3.21 s
Epoch 8: 30.10% done
Loss: 151.60522764921188
Time: 4.98 s
Epoch 8: 40.05% done
Loss: 151.43822280666495
Time: 6.93 s
Epoch 8: 50.13% done
Loss: 155.046357691288
Time: 8.91 s
Epoch 8: 60.08% done
Loss: 150.1088238667838
Time: 10.88 s
Epoch 8: 70.03% done
Loss: 146.55476650608242
Time: 12.77 s
Epoch 8: 80.10% done
Loss: 156.32372444868088
Time: 14.69 s
Epoch 8: 90.05% done
Loss: 135.86837671980072
Time: 16.50 s

Epoch 8 done
Epoch loss: 150.47039236820214

Time taken for epoch: 18.97 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 349.35534572255784

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.13% done
Loss: 199.40082550048828
Time: 0.01 s
Epoch 9: 10.06% done
Loss: 138.25596707782665
Time: 1.27 s
Epoch 9: 20.13% done
Loss: 145.81970340013504
Time: 3.13 s
Epoch 9: 30.06% done
Loss: 131.05055965954745
Time: 4.98 s
Epoch 9: 40.13% done
Loss: 115.51788093149662
Time: 7.05 s
Epoch 9: 50.06% done
Loss: 137.4538768394084
Time: 8.88 s
Epoch 9: 60.13% done
Loss: 140.81035199016333
Time: 10.88 s
Epoch 9: 70.06% done
Loss: 135.1734607355504
Time: 12.74 s
Epoch 9: 80.13% done
Loss: 148.1666987836361
Time: 14.69 s
Epoch 9: 90.06% done
Loss: 142.94462063644505
Time: 16.65 s

Epoch 9 done
Epoch loss: 136.22245144794073

Time taken for epoch: 18.95 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.25 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 349.5222562983416

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_512_lr_0.001_tmp_0.07_weight_decay_0.0/2024-08-07_11:10:58_checkpoint_epoch_9.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 348.32314163014513 at epoch 4

