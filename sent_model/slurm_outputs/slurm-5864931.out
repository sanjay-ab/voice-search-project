Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-06_22:19:10
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 5, learning rate: 1e-06
clip norm: 10, temperature: 0.07, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr_division: 100
temperature: 0.07

Created paired data
Created paired data
Time taken to create datasets: 71.29 s
Number of parameters in model: 6825984

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 647.8575134277344
Time: 15.76 s
Epoch 0: 10.04% done
Loss: 751.585137331033
Time: 21.11 s
Epoch 0: 20.08% done
Loss: 710.5418057441711
Time: 22.98 s
Epoch 0: 30.11% done
Loss: 537.958577712377
Time: 24.51 s
Epoch 0: 40.03% done
Loss: 462.42646869224836
Time: 26.65 s
Epoch 0: 50.06% done
Loss: 405.8856519460678
Time: 28.65 s
Epoch 0: 60.10% done
Loss: 346.7896845340729
Time: 30.67 s
Epoch 0: 70.01% done
Loss: 343.0792303930355
Time: 32.62 s
Epoch 0: 80.05% done
Loss: 329.48586869239807
Time: 34.59 s
Epoch 0: 90.09% done
Loss: 324.3676669597626
Time: 36.42 s

Epoch 0 done
Epoch loss: 453.9794243254555

Time taken for epoch: 38.74 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.16 s
Calculating validation loss: 40.22% done
Time: 0.29 s
Calculating validation loss: 60.87% done
Time: 0.47 s
Calculating validation loss: 80.43% done
Time: 0.61 s

Validation loss: 340.06251024163294

Time taken: 0.73 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 311.30773544311523
Time: 0.01 s
Epoch 1: 10.06% done
Loss: 323.1037491786329
Time: 1.55 s
Epoch 1: 20.13% done
Loss: 316.18480783700943
Time: 3.56 s
Epoch 1: 30.06% done
Loss: 320.4214925001442
Time: 5.56 s
Epoch 1: 40.13% done
Loss: 303.8707983493805
Time: 7.51 s
Epoch 1: 50.06% done
Loss: 298.78733140003834
Time: 9.43 s
Epoch 1: 60.13% done
Loss: 302.31772089004517
Time: 11.24 s
Epoch 1: 70.06% done
Loss: 294.3112853810757
Time: 13.25 s
Epoch 1: 80.13% done
Loss: 288.68065774440765
Time: 15.16 s
Epoch 1: 90.06% done
Loss: 268.4605204908154
Time: 17.12 s

Epoch 1 done
Epoch loss: 299.489382841802

Time taken for epoch: 19.42 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.19 s
Calculating validation loss: 40.22% done
Time: 0.32 s
Calculating validation loss: 60.87% done
Time: 0.47 s
Calculating validation loss: 80.43% done
Time: 0.57 s

Validation loss: 338.05943089982736

Time taken: 0.69 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 316.14599227905273
Time: 0.01 s
Epoch 2: 10.06% done
Loss: 265.5789727400124
Time: 1.26 s
Epoch 2: 20.13% done
Loss: 254.76073694229126
Time: 3.09 s
Epoch 2: 30.06% done
Loss: 258.50825029083444
Time: 5.00 s
Epoch 2: 40.13% done
Loss: 249.30793017148972
Time: 7.01 s
Epoch 2: 50.06% done
Loss: 244.28347068496896
Time: 9.01 s
Epoch 2: 60.13% done
Loss: 231.54009008407593
Time: 10.96 s
Epoch 2: 70.06% done
Loss: 243.70607388170458
Time: 12.75 s
Epoch 2: 80.13% done
Loss: 231.28982409834862
Time: 14.80 s
Epoch 2: 90.06% done
Loss: 235.29481151435948
Time: 16.81 s

Epoch 2 done
Epoch loss: 245.07555288238845

Time taken for epoch: 19.12 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 337.07224303397584

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 300.6277847290039
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 213.67562390580963
Time: 2.07 s
Epoch 3: 20.13% done
Loss: 199.38745111227036
Time: 4.04 s
Epoch 3: 30.06% done
Loss: 214.46975653684592
Time: 5.41 s
Epoch 3: 40.13% done
Loss: 200.33208912611008
Time: 7.36 s
Epoch 3: 50.06% done
Loss: 181.52723300306104
Time: 9.30 s
Epoch 3: 60.13% done
Loss: 187.73862201968828
Time: 11.08 s
Epoch 3: 70.06% done
Loss: 193.43643792067903
Time: 13.07 s
Epoch 3: 80.13% done
Loss: 175.91470682621002
Time: 14.93 s
Epoch 3: 90.06% done
Loss: 198.27279640149467
Time: 16.91 s

Epoch 3 done
Epoch loss: 195.26725871707905

Time taken for epoch: 19.32 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 336.59591876942176

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 49.73280429840088
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 169.37274907963186
Time: 1.47 s
Epoch 4: 20.03% done
Loss: 168.79941451398633
Time: 3.54 s
Epoch 4: 30.10% done
Loss: 154.81461584568024
Time: 5.48 s
Epoch 4: 40.05% done
Loss: 161.41895381710196
Time: 7.48 s
Epoch 4: 50.13% done
Loss: 144.32656145095825
Time: 9.53 s
Epoch 4: 60.08% done
Loss: 143.78988371619695
Time: 11.54 s
Epoch 4: 70.03% done
Loss: 140.43424220024784
Time: 13.46 s
Epoch 4: 80.10% done
Loss: 142.3776694238186
Time: 15.32 s
Epoch 4: 90.05% done
Loss: 148.8682115530666
Time: 17.32 s

Epoch 4 done
Epoch loss: 149.95325582078482

Time taken for epoch: 19.64 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.33 s
Calculating validation loss: 80.43% done
Time: 0.45 s

Validation loss: 336.81550138238543

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 37.705078125
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 125.84123828743077
Time: 1.87 s
Epoch 5: 20.05% done
Loss: 129.4256121146528
Time: 3.39 s
Epoch 5: 30.01% done
Loss: 110.23091892652874
Time: 5.37 s
Epoch 5: 40.10% done
Loss: 125.90792992990464
Time: 7.26 s
Epoch 5: 50.06% done
Loss: 110.48891223684141
Time: 9.21 s
Epoch 5: 60.03% done
Loss: 97.17536598821229
Time: 11.19 s
Epoch 5: 70.11% done
Loss: 115.75061710830778
Time: 13.19 s
Epoch 5: 80.08% done
Loss: 107.1195084887718
Time: 15.14 s
Epoch 5: 90.04% done
Loss: 100.13586681854876
Time: 17.07 s

Epoch 5 done
Epoch loss: 113.13818991102808

Time taken for epoch: 19.34 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.22 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 337.0895955700806

Time taken: 0.59 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 89.04115676879883
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 85.66875972325289
Time: 1.93 s
Epoch 6: 20.13% done
Loss: 92.3335584953602
Time: 3.41 s
Epoch 6: 30.06% done
Loss: 84.47102273566813
Time: 5.37 s
Epoch 6: 40.13% done
Loss: 80.19424702972174
Time: 7.42 s
Epoch 6: 50.06% done
Loss: 84.23646283300617
Time: 9.35 s
Epoch 6: 60.13% done
Loss: 83.73777656257153
Time: 11.41 s
Epoch 6: 70.06% done
Loss: 70.804872957966
Time: 13.43 s
Epoch 6: 80.13% done
Loss: 89.37864731997252
Time: 15.30 s
Epoch 6: 90.06% done
Loss: 93.3783498632757
Time: 17.18 s

Epoch 6 done
Epoch loss: 84.6519046226405

Time taken for epoch: 19.51 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.48 s

Validation loss: 337.3408429173455

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 136.36080741882324
Time: 0.01 s
Epoch 7: 10.04% done
Loss: 59.54311579843111
Time: 2.04 s
Epoch 7: 20.08% done
Loss: 69.23802483826876
Time: 3.47 s
Epoch 7: 30.11% done
Loss: 65.84076804667711
Time: 5.49 s
Epoch 7: 40.03% done
Loss: 74.81882879251167
Time: 7.39 s
Epoch 7: 50.06% done
Loss: 68.46008306741714
Time: 9.38 s
Epoch 7: 60.10% done
Loss: 57.99017874523997
Time: 11.30 s
Epoch 7: 70.01% done
Loss: 70.89557149742222
Time: 13.17 s
Epoch 7: 80.05% done
Loss: 62.65789242647588
Time: 15.12 s
Epoch 7: 90.09% done
Loss: 66.9348420985043
Time: 17.06 s

Epoch 7 done
Epoch loss: 65.8453056740367

Time taken for epoch: 19.35 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 337.8939024952874

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 11.579077243804932
Time: 0.02 s
Epoch 8: 10.08% done
Loss: 46.74856988170856
Time: 1.92 s
Epoch 8: 20.03% done
Loss: 47.91143072745468
Time: 3.27 s
Epoch 8: 30.10% done
Loss: 47.96154771372676
Time: 5.16 s
Epoch 8: 40.05% done
Loss: 52.2238933864022
Time: 7.13 s
Epoch 8: 50.13% done
Loss: 48.15571593865752
Time: 9.17 s
Epoch 8: 60.08% done
Loss: 50.497121180914625
Time: 11.10 s
Epoch 8: 70.03% done
Loss: 45.97015513083603
Time: 12.98 s
Epoch 8: 80.10% done
Loss: 43.84686763584614
Time: 14.84 s
Epoch 8: 90.05% done
Loss: 57.72521959075445
Time: 16.92 s

Epoch 8 done
Epoch loss: 49.58295592543998

Time taken for epoch: 19.28 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 338.3055375624394

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_1e-06_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_8.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 336.59591876942176 at epoch 3

