Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-05_19:57:30
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 15, learning rate: 0.0005
clip norm: 10, temperature: 0.15, num pairs per batch: 2
time limit to create dataset: 600
weight decay: 0.01
awe_lr_division: 100
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 49.61 s
up_proj_dim: 512
output_dim: 1024
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 8401920
Number of parameters in AWE model: 6825984
Number of parameters in other model: 1575936
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.05% done
Loss: 683.2019329071045
Time: 19.48 s
Epoch 0: 10.03% done
Loss: 681.9983394459041
Time: 23.64 s
Epoch 0: 20.05% done
Loss: 503.17861313496405
Time: 25.38 s
Epoch 0: 30.03% done
Loss: 389.82961951482173
Time: 27.28 s
Epoch 0: 40.05% done
Loss: 388.3371104846648
Time: 29.13 s
Epoch 0: 50.03% done
Loss: 373.49844316039423
Time: 30.92 s
Epoch 0: 60.05% done
Loss: 388.16802876079504
Time: 32.79 s
Epoch 0: 70.03% done
Loss: 394.1328696831308
Time: 34.57 s
Epoch 0: 80.05% done
Loss: 360.6215101091107
Time: 36.30 s
Epoch 0: 90.03% done
Loss: 369.59278877786915
Time: 38.73 s

Epoch 0 done
Epoch loss: 421.3179024153152

Time taken for epoch: 41.02 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 418.1040104376067

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.05% done
Loss: 289.15090560913086
Time: 0.00 s
Epoch 1: 10.04% done
Loss: 360.952774518066
Time: 1.32 s
Epoch 1: 20.03% done
Loss: 350.8726939257949
Time: 3.42 s
Epoch 1: 30.02% done
Loss: 331.443533135785
Time: 5.35 s
Epoch 1: 40.01% done
Loss: 341.1163219297775
Time: 7.10 s
Epoch 1: 50.05% done
Loss: 321.1066443641581
Time: 8.95 s
Epoch 1: 60.04% done
Loss: 317.05934345872714
Time: 10.72 s
Epoch 1: 70.03% done
Loss: 316.1606230330889
Time: 12.56 s
Epoch 1: 80.02% done
Loss: 319.9485571396471
Time: 14.29 s
Epoch 1: 90.01% done
Loss: 341.41355072609105
Time: 16.08 s

Epoch 1 done
Epoch loss: 333.6286491393953

Time taken for epoch: 18.34 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 417.0601112033249

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.05% done
Loss: 161.11276149749756
Time: 0.04 s
Epoch 2: 10.04% done
Loss: 316.92220004218996
Time: 1.31 s
Epoch 2: 20.02% done
Loss: 308.9623328110184
Time: 3.12 s
Epoch 2: 30.01% done
Loss: 308.178435401483
Time: 4.95 s
Epoch 2: 40.04% done
Loss: 303.77096902185946
Time: 6.67 s
Epoch 2: 50.03% done
Loss: 306.9946051998572
Time: 8.41 s
Epoch 2: 60.01% done
Loss: 277.0083408840377
Time: 10.16 s
Epoch 2: 70.05% done
Loss: 306.0845425717495
Time: 11.95 s
Epoch 2: 80.03% done
Loss: 287.39745420488447
Time: 13.81 s
Epoch 2: 90.02% done
Loss: 329.9614343754571
Time: 15.73 s

Epoch 2 done
Epoch loss: 305.358693181747

Time taken for epoch: 18.06 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.03 s
Calculating validation loss: 20.18% done
Time: 0.24 s
Calculating validation loss: 40.37% done
Time: 0.37 s
Calculating validation loss: 60.09% done
Time: 0.49 s
Calculating validation loss: 80.28% done
Time: 0.62 s

Validation loss: 416.52312311557455

Time taken: 0.75 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.05% done
Loss: 456.28576278686523
Time: 0.00 s
Epoch 3: 10.03% done
Loss: 292.0713541634155
Time: 1.31 s
Epoch 3: 20.01% done
Loss: 256.9979040008603
Time: 3.13 s
Epoch 3: 30.04% done
Loss: 285.8746856226394
Time: 4.92 s
Epoch 3: 40.02% done
Loss: 290.64609263882494
Time: 6.68 s
Epoch 3: 50.05% done
Loss: 294.40320756417424
Time: 8.47 s
Epoch 3: 60.03% done
Loss: 289.5824353050704
Time: 10.27 s
Epoch 3: 70.01% done
Loss: 253.03345717715496
Time: 12.12 s
Epoch 3: 80.04% done
Loss: 279.6901232408519
Time: 13.98 s
Epoch 3: 90.02% done
Loss: 272.2663327712904
Time: 15.89 s

Epoch 3 done
Epoch loss: 276.4079223112804

Time taken for epoch: 18.16 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 416.15787263310284

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.05% done
Loss: 295.03283500671387
Time: 0.01 s
Epoch 4: 10.03% done
Loss: 252.88431579717482
Time: 1.32 s
Epoch 4: 20.05% done
Loss: 273.0966570838612
Time: 3.13 s
Epoch 4: 30.03% done
Loss: 268.7499914186621
Time: 4.90 s
Epoch 4: 40.05% done
Loss: 257.53653114240376
Time: 6.68 s
Epoch 4: 50.03% done
Loss: 282.65128493158505
Time: 8.65 s
Epoch 4: 60.05% done
Loss: 258.3665727633028
Time: 10.51 s
Epoch 4: 70.03% done
Loss: 263.95283109988225
Time: 12.30 s
Epoch 4: 80.05% done
Loss: 271.209024087568
Time: 14.13 s
Epoch 4: 90.03% done
Loss: 272.0835923508863
Time: 16.03 s

Epoch 4 done
Epoch loss: 265.4270590141753

Time taken for epoch: 18.28 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.28 s
Calculating validation loss: 60.09% done
Time: 0.40 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 415.73130220448206

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.05% done
Loss: 337.91160583496094
Time: 0.01 s
Epoch 5: 10.04% done
Loss: 268.4070893271704
Time: 1.36 s
Epoch 5: 20.03% done
Loss: 243.34581048049108
Time: 3.20 s
Epoch 5: 30.02% done
Loss: 254.4068749824708
Time: 5.01 s
Epoch 5: 40.01% done
Loss: 269.61039673123094
Time: 6.86 s
Epoch 5: 50.05% done
Loss: 232.41607243728697
Time: 8.72 s
Epoch 5: 60.04% done
Loss: 238.51448244725665
Time: 10.50 s
Epoch 5: 70.03% done
Loss: 253.69946411387488
Time: 12.30 s
Epoch 5: 80.02% done
Loss: 235.8082992542121
Time: 14.12 s
Epoch 5: 90.01% done
Loss: 257.01621717049017
Time: 15.88 s

Epoch 5 done
Epoch loss: 249.35213593149598

Time taken for epoch: 18.23 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.03 s
Calculating validation loss: 20.18% done
Time: 0.27 s
Calculating validation loss: 40.37% done
Time: 0.40 s
Calculating validation loss: 60.09% done
Time: 0.51 s
Calculating validation loss: 80.28% done
Time: 0.64 s

Validation loss: 415.77116436914565

Time taken: 0.75 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.05% done
Loss: 14.646841585636139
Time: 0.01 s
Epoch 6: 10.03% done
Loss: 217.29257334813929
Time: 1.25 s
Epoch 6: 20.01% done
Loss: 199.00810484106492
Time: 2.97 s
Epoch 6: 30.04% done
Loss: 226.14547711371176
Time: 4.69 s
Epoch 6: 40.02% done
Loss: 230.50602922084354
Time: 6.49 s
Epoch 6: 50.05% done
Loss: 236.43073336076196
Time: 8.32 s
Epoch 6: 60.03% done
Loss: 218.18790532686194
Time: 10.15 s
Epoch 6: 70.01% done
Loss: 219.96368189916166
Time: 11.97 s
Epoch 6: 80.04% done
Loss: 219.89820329612823
Time: 13.83 s
Epoch 6: 90.02% done
Loss: 232.03842072474836
Time: 15.62 s

Epoch 6 done
Epoch loss: 224.16762975063324

Time taken for epoch: 17.82 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.49 s

Validation loss: 415.7681418121408

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.05% done
Loss: 323.5599994659424
Time: 0.01 s
Epoch 7: 10.03% done
Loss: 232.1064112501012
Time: 1.42 s
Epoch 7: 20.01% done
Loss: 226.30459692592572
Time: 3.16 s
Epoch 7: 30.04% done
Loss: 237.39507456296053
Time: 4.92 s
Epoch 7: 40.02% done
Loss: 211.3505727639704
Time: 6.84 s
Epoch 7: 50.05% done
Loss: 221.58691528154978
Time: 8.70 s
Epoch 7: 60.03% done
Loss: 208.68953738474485
Time: 10.45 s
Epoch 7: 70.01% done
Loss: 234.43687811042324
Time: 12.23 s
Epoch 7: 80.04% done
Loss: 231.42501197418375
Time: 14.05 s
Epoch 7: 90.02% done
Loss: 207.65690657150265
Time: 15.90 s

Epoch 7 done
Epoch loss: 223.7931190768347

Time taken for epoch: 18.17 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.02 s
Calculating validation loss: 20.18% done
Time: 0.31 s
Calculating validation loss: 40.37% done
Time: 0.44 s
Calculating validation loss: 60.09% done
Time: 0.56 s
Calculating validation loss: 80.28% done
Time: 0.69 s

Validation loss: 415.1694679478987

Time taken: 0.80 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.05% done
Loss: 134.98611450195312
Time: 0.02 s
Epoch 8: 10.04% done
Loss: 226.9405235395287
Time: 1.55 s
Epoch 8: 20.03% done
Loss: 222.40845105714268
Time: 3.35 s
Epoch 8: 30.02% done
Loss: 202.27151712252186
Time: 5.16 s
Epoch 8: 40.01% done
Loss: 207.58041870413405
Time: 7.03 s
Epoch 8: 50.05% done
Loss: 210.25521715531997
Time: 8.81 s
Epoch 8: 60.04% done
Loss: 203.05364901162307
Time: 10.65 s
Epoch 8: 70.03% done
Loss: 224.77758493265043
Time: 12.50 s
Epoch 8: 80.02% done
Loss: 202.396738579064
Time: 14.30 s
Epoch 8: 90.01% done
Loss: 206.49539731969736
Time: 16.09 s

Epoch 8 done
Epoch loss: 210.84712103528923

Time taken for epoch: 18.34 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 414.6986564365002

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.05% done
Loss: 52.82943248748779
Time: 0.01 s
Epoch 9: 10.03% done
Loss: 202.2154866971753
Time: 1.33 s
Epoch 9: 20.05% done
Loss: 200.705666679084
Time: 3.16 s
Epoch 9: 30.03% done
Loss: 200.54534227220398
Time: 5.37 s
Epoch 9: 40.05% done
Loss: 190.6932703102354
Time: 7.18 s
Epoch 9: 50.03% done
Loss: 204.13101359430436
Time: 9.25 s
Epoch 9: 60.05% done
Loss: 194.48771831490586
Time: 11.03 s
Epoch 9: 70.03% done
Loss: 217.553143647283
Time: 12.78 s
Epoch 9: 80.05% done
Loss: 206.8862190310979
Time: 14.60 s
Epoch 9: 90.03% done
Loss: 215.99578905963537
Time: 16.40 s

Epoch 9 done
Epoch loss: 205.61151746266358

Time taken for epoch: 18.71 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 415.7449329664948

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_9.pt

Regenerated paired data
Epoch 10: 0.05% done
Loss: 107.20531940460205
Time: 0.01 s
Epoch 10: 10.03% done
Loss: 190.86816870052405
Time: 1.35 s
Epoch 10: 20.01% done
Loss: 202.3731426138318
Time: 3.24 s
Epoch 10: 30.04% done
Loss: 199.694610435759
Time: 5.06 s
Epoch 10: 40.02% done
Loss: 208.11433486565195
Time: 6.83 s
Epoch 10: 50.05% done
Loss: 194.91083098472512
Time: 8.60 s
Epoch 10: 60.03% done
Loss: 198.14032202191424
Time: 10.45 s
Epoch 10: 70.01% done
Loss: 198.50409129126504
Time: 12.34 s
Epoch 10: 80.04% done
Loss: 174.85914217617045
Time: 14.22 s
Epoch 10: 90.02% done
Loss: 197.39239597316794
Time: 16.03 s

Epoch 10 done
Epoch loss: 196.18228931550777

Time taken for epoch: 18.12 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.49 s

Validation loss: 416.5155920413656

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_10.pt

Regenerated paired data
Epoch 11: 0.05% done
Loss: 81.82065486907959
Time: 0.01 s
Epoch 11: 10.03% done
Loss: 172.83725457059012
Time: 1.23 s
Epoch 11: 20.05% done
Loss: 175.00558583318588
Time: 3.05 s
Epoch 11: 30.03% done
Loss: 168.8188300383362
Time: 4.78 s
Epoch 11: 40.05% done
Loss: 177.72565908430508
Time: 6.64 s
Epoch 11: 50.03% done
Loss: 199.91445556433513
Time: 8.47 s
Epoch 11: 60.05% done
Loss: 193.8683546686442
Time: 10.30 s
Epoch 11: 70.03% done
Loss: 208.96601052108136
Time: 12.16 s
Epoch 11: 80.05% done
Loss: 183.98389334430047
Time: 13.94 s
Epoch 11: 90.03% done
Loss: 176.4391926201907
Time: 15.78 s

Epoch 11 done
Epoch loss: 186.2453377440159

Time taken for epoch: 18.21 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 416.1394977788313

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_11.pt

Regenerated paired data
Epoch 12: 0.05% done
Loss: 14.175200462341309
Time: 0.00 s
Epoch 12: 10.03% done
Loss: 191.9921941979967
Time: 1.23 s
Epoch 12: 20.05% done
Loss: 184.18664141703192
Time: 3.05 s
Epoch 12: 30.03% done
Loss: 195.72560374220512
Time: 4.87 s
Epoch 12: 40.05% done
Loss: 171.094359527225
Time: 6.66 s
Epoch 12: 50.03% done
Loss: 194.54532529443804
Time: 8.42 s
Epoch 12: 60.05% done
Loss: 160.44971578235004
Time: 10.25 s
Epoch 12: 70.03% done
Loss: 177.37182430320917
Time: 12.17 s
Epoch 12: 80.05% done
Loss: 183.86611769434495
Time: 14.04 s
Epoch 12: 90.03% done
Loss: 173.69632556131393
Time: 15.90 s

Epoch 12 done
Epoch loss: 181.4187680327941

Time taken for epoch: 18.22 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 416.2538498913476

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_12.pt

Regenerated paired data
Epoch 13: 0.05% done
Loss: 343.8051462173462
Time: 0.01 s
Epoch 13: 10.04% done
Loss: 163.9866468600101
Time: 1.37 s
Epoch 13: 20.02% done
Loss: 186.73891250156996
Time: 3.21 s
Epoch 13: 30.01% done
Loss: 178.0088647560339
Time: 5.08 s
Epoch 13: 40.04% done
Loss: 165.04732693297478
Time: 6.82 s
Epoch 13: 50.03% done
Loss: 190.83729794606415
Time: 8.64 s
Epoch 13: 60.01% done
Loss: 168.0457721530187
Time: 10.39 s
Epoch 13: 70.05% done
Loss: 162.8542715150058
Time: 12.14 s
Epoch 13: 80.03% done
Loss: 165.72941539037708
Time: 14.01 s
Epoch 13: 90.02% done
Loss: 178.37840897040536
Time: 15.86 s

Epoch 13 done
Epoch loss: 173.76785816524256

Time taken for epoch: 18.13 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 416.7162400866867

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_13.pt

Regenerated paired data
Epoch 14: 0.05% done
Loss: 299.09887313842773
Time: 0.01 s
Epoch 14: 10.03% done
Loss: 169.29009627325064
Time: 1.27 s
Epoch 14: 20.01% done
Loss: 172.81687054783106
Time: 3.06 s
Epoch 14: 30.04% done
Loss: 168.10427204560693
Time: 4.83 s
Epoch 14: 40.02% done
Loss: 156.75218903807678
Time: 6.70 s
Epoch 14: 50.05% done
Loss: 170.40785134362815
Time: 8.48 s
Epoch 14: 60.03% done
Loss: 162.10780328873432
Time: 10.22 s
Epoch 14: 70.01% done
Loss: 164.37758018987046
Time: 12.04 s
Epoch 14: 80.04% done
Loss: 169.74155981486766
Time: 13.86 s
Epoch 14: 90.02% done
Loss: 182.96716297656823
Time: 15.69 s

Epoch 14 done
Epoch loss: 168.19496648559806

Time taken for epoch: 17.93 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.05 s
Calculating validation loss: 20.18% done
Time: 0.18 s
Calculating validation loss: 40.37% done
Time: 0.31 s
Calculating validation loss: 60.09% done
Time: 0.44 s
Calculating validation loss: 80.28% done
Time: 0.57 s

Validation loss: 416.7022364948868

Time taken: 0.69 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_14.pt

Regenerated paired data
Epoch 15: 0.05% done
Loss: 404.4926643371582
Time: 0.01 s
Epoch 15: 10.03% done
Loss: 154.99925952442368
Time: 1.33 s
Epoch 15: 20.05% done
Loss: 166.106916565317
Time: 3.20 s
Epoch 15: 30.03% done
Loss: 170.2374668603744
Time: 5.12 s
Epoch 15: 40.05% done
Loss: 147.55885118440767
Time: 7.08 s
Epoch 15: 50.03% done
Loss: 146.57848476799148
Time: 8.88 s
Epoch 15: 60.05% done
Loss: 158.7007127664796
Time: 10.84 s
Epoch 15: 70.03% done
Loss: 146.0310649032695
Time: 12.70 s
Epoch 15: 80.05% done
Loss: 175.2679444147489
Time: 14.50 s
Epoch 15: 90.03% done
Loss: 144.89920787099334
Time: 16.41 s

Epoch 15 done
Epoch loss: 158.59710334614857

Time taken for epoch: 18.66 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.02 s
Calculating validation loss: 20.18% done
Time: 0.30 s
Calculating validation loss: 40.37% done
Time: 0.45 s
Calculating validation loss: 60.09% done
Time: 0.58 s
Calculating validation loss: 80.28% done
Time: 0.70 s

Validation loss: 415.78968730541544

Time taken: 0.83 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_15.pt

Regenerated paired data
Epoch 16: 0.05% done
Loss: 12.986300885677338
Time: 0.01 s
Epoch 16: 10.03% done
Loss: 148.19971131480705
Time: 1.31 s
Epoch 16: 20.01% done
Loss: 147.02888744873832
Time: 3.19 s
Epoch 16: 30.04% done
Loss: 169.89802389672428
Time: 5.09 s
Epoch 16: 40.02% done
Loss: 165.50786904426235
Time: 6.94 s
Epoch 16: 50.05% done
Loss: 160.70673228358504
Time: 8.79 s
Epoch 16: 60.03% done
Loss: 163.58221590406063
Time: 10.65 s
Epoch 16: 70.01% done
Loss: 175.41652388042874
Time: 12.47 s
Epoch 16: 80.04% done
Loss: 143.1213065289223
Time: 14.32 s
Epoch 16: 90.02% done
Loss: 159.38595177963225
Time: 16.15 s

Epoch 16 done
Epoch loss: 159.4936201176625

Time taken for epoch: 18.34 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 417.3602108561665

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_16.pt

Regenerated paired data
Epoch 17: 0.05% done
Loss: 83.4313154220581
Time: 0.03 s
Epoch 17: 10.04% done
Loss: 144.73072241502578
Time: 1.36 s
Epoch 17: 20.03% done
Loss: 143.77676496479774
Time: 3.20 s
Epoch 17: 30.02% done
Loss: 157.34231952822418
Time: 5.10 s
Epoch 17: 40.01% done
Loss: 153.0761043277729
Time: 6.87 s
Epoch 17: 50.05% done
Loss: 156.7000272566799
Time: 8.78 s
Epoch 17: 60.04% done
Loss: 169.645242559526
Time: 10.69 s
Epoch 17: 70.03% done
Loss: 165.06374827162787
Time: 12.50 s
Epoch 17: 80.02% done
Loss: 145.778271840001
Time: 14.23 s
Epoch 17: 90.01% done
Loss: 135.66880266774785
Time: 16.20 s

Epoch 17 done
Epoch loss: 154.09907901421843

Time taken for epoch: 18.30 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 418.811182472684

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_17.pt

Regenerated paired data
Epoch 18: 0.05% done
Loss: 64.2509400844574
Time: 0.00 s
Epoch 18: 10.03% done
Loss: 141.8273573270952
Time: 1.21 s
Epoch 18: 20.05% done
Loss: 145.12006876400517
Time: 3.03 s
Epoch 18: 30.03% done
Loss: 148.4643555631993
Time: 4.88 s
Epoch 18: 40.05% done
Loss: 142.73697539939354
Time: 6.71 s
Epoch 18: 50.03% done
Loss: 133.7526494928758
Time: 8.53 s
Epoch 18: 60.05% done
Loss: 157.18604501280652
Time: 10.42 s
Epoch 18: 70.03% done
Loss: 152.1391704029432
Time: 12.18 s
Epoch 18: 80.05% done
Loss: 158.23769769327126
Time: 14.03 s
Epoch 18: 90.03% done
Loss: 149.1994614468304
Time: 15.86 s

Epoch 18 done
Epoch loss: 149.315301346914

Time taken for epoch: 18.14 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.04 s
Calculating validation loss: 20.18% done
Time: 0.22 s
Calculating validation loss: 40.37% done
Time: 0.36 s
Calculating validation loss: 60.09% done
Time: 0.48 s
Calculating validation loss: 80.28% done
Time: 0.61 s

Validation loss: 417.38572339399144

Time taken: 0.74 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_18.pt

Regenerated paired data
Epoch 19: 0.05% done
Loss: 54.07918691635132
Time: 0.01 s
Epoch 19: 10.03% done
Loss: 131.5162246124913
Time: 1.31 s
Epoch 19: 20.01% done
Loss: 140.12025823684954
Time: 3.22 s
Epoch 19: 30.04% done
Loss: 157.8740450511476
Time: 5.10 s
Epoch 19: 40.02% done
Loss: 153.99364752488003
Time: 6.89 s
Epoch 19: 50.05% done
Loss: 144.75862342471154
Time: 8.66 s
Epoch 19: 60.03% done
Loss: 154.28348883911215
Time: 10.74 s
Epoch 19: 70.01% done
Loss: 132.9372903388558
Time: 12.46 s
Epoch 19: 80.04% done
Loss: 171.2578540490051
Time: 14.32 s
Epoch 19: 90.02% done
Loss: 141.72272653121388
Time: 16.31 s

Epoch 19 done
Epoch loss: 149.93294041393506

Time taken for epoch: 18.52 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.04 s
Calculating validation loss: 20.18% done
Time: 0.16 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 417.7662674440156

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_19.pt

Regenerated paired data
Epoch 20: 0.05% done
Loss: 36.44293546676636
Time: 0.01 s
Epoch 20: 10.04% done
Loss: 155.40836673568595
Time: 1.22 s
Epoch 20: 20.03% done
Loss: 132.88977003345886
Time: 2.95 s
Epoch 20: 30.02% done
Loss: 122.9418877827361
Time: 4.77 s
Epoch 20: 40.01% done
Loss: 147.136235813789
Time: 6.53 s
Epoch 20: 50.05% done
Loss: 145.47957974256732
Time: 8.33 s
Epoch 20: 60.04% done
Loss: 165.35319895718735
Time: 10.13 s
Epoch 20: 70.03% done
Loss: 142.369402647771
Time: 11.94 s
Epoch 20: 80.02% done
Loss: 156.66873222781402
Time: 13.69 s
Epoch 20: 90.01% done
Loss: 170.69061381549508
Time: 15.50 s

Epoch 20 done
Epoch loss: 148.50696954262037

Time taken for epoch: 17.69 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.03 s
Calculating validation loss: 20.18% done
Time: 0.15 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 416.4755670302505

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_20.pt

Regenerated paired data
Epoch 21: 0.05% done
Loss: 77.51945853233337
Time: 0.01 s
Epoch 21: 10.03% done
Loss: 117.08520481908562
Time: 1.28 s
Epoch 21: 20.05% done
Loss: 141.1445681249077
Time: 3.06 s
Epoch 21: 30.03% done
Loss: 132.9697859896855
Time: 4.84 s
Epoch 21: 40.05% done
Loss: 125.23914914082118
Time: 6.67 s
Epoch 21: 50.03% done
Loss: 133.67787576139426
Time: 8.48 s
Epoch 21: 60.05% done
Loss: 152.56735200680157
Time: 10.28 s
Epoch 21: 70.03% done
Loss: 138.73448055237532
Time: 12.05 s
Epoch 21: 80.05% done
Loss: 138.8299775763822
Time: 13.83 s
Epoch 21: 90.03% done
Loss: 143.68451855173618
Time: 15.69 s

Epoch 21 done
Epoch loss: 135.3236959658338

Time taken for epoch: 18.03 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 416.84300538596756

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_21.pt

Regenerated paired data
Epoch 22: 0.05% done
Loss: 57.16080665588379
Time: 0.03 s
Epoch 22: 10.03% done
Loss: 122.64800586765914
Time: 1.44 s
Epoch 22: 20.01% done
Loss: 119.28814601886904
Time: 3.24 s
Epoch 22: 30.04% done
Loss: 119.30298037106293
Time: 5.07 s
Epoch 22: 40.02% done
Loss: 135.21946064935
Time: 6.89 s
Epoch 22: 50.05% done
Loss: 117.80602431168134
Time: 8.74 s
Epoch 22: 60.03% done
Loss: 129.5107358939607
Time: 10.57 s
Epoch 22: 70.01% done
Loss: 134.56775824812175
Time: 12.34 s
Epoch 22: 80.04% done
Loss: 129.96115319093866
Time: 14.21 s
Epoch 22: 90.02% done
Loss: 140.72178897607807
Time: 16.01 s

Epoch 22 done
Epoch loss: 129.58240176815326

Time taken for epoch: 18.26 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 419.1787906742971

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_22.pt

Regenerated paired data
Epoch 23: 0.05% done
Loss: 224.273943901062
Time: 0.01 s
Epoch 23: 10.05% done
Loss: 123.08717105782243
Time: 1.43 s
Epoch 23: 20.04% done
Loss: 121.24836939749204
Time: 3.26 s
Epoch 23: 30.04% done
Loss: 125.40207293655986
Time: 5.10 s
Epoch 23: 40.03% done
Loss: 133.36510839354662
Time: 6.84 s
Epoch 23: 50.03% done
Loss: 136.88111632657848
Time: 8.77 s
Epoch 23: 60.02% done
Loss: 114.5497218178905
Time: 10.63 s
Epoch 23: 70.02% done
Loss: 117.53470318184958
Time: 12.45 s
Epoch 23: 80.01% done
Loss: 129.49258835816926
Time: 14.39 s
Epoch 23: 90.01% done
Loss: 121.02074783408281
Time: 16.21 s

Epoch 23 done
Epoch loss: 126.54040416875165

Time taken for epoch: 18.46 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.50 s

Validation loss: 422.2694247140797

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_1024_lr_0.0005_weight_decay_0.01/2024-08-05_19:57:30_checkpoint_epoch_23.pt

Regenerated paired data
Validation loss has not improved for 15 epochs. Stopping training.
BEST VALIDATION LOSS: 414.6986564365002 at epoch 8

