Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-05_20:24:12
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 15, learning rate: 0.0005
clip norm: 10, temperature: 0.15, num pairs per batch: 2
time limit to create dataset: 600
weight decay: 0.01
awe_lr_division: 100
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 56.09 s
up_proj_dim: 512
output_dim: 128
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 7482624
Number of parameters in AWE model: 6825984
Number of parameters in other model: 656640
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.05% done
Loss: 425.00762939453125
Time: 23.28 s
Epoch 0: 10.03% done
Loss: 727.1311558858313
Time: 26.92 s
Epoch 0: 20.05% done
Loss: 559.3982063645693
Time: 28.71 s
Epoch 0: 30.03% done
Loss: 461.603248691318
Time: 30.44 s
Epoch 0: 40.05% done
Loss: 439.5280494761826
Time: 32.24 s
Epoch 0: 50.03% done
Loss: 398.87654301193027
Time: 34.64 s
Epoch 0: 60.05% done
Loss: 388.023999827591
Time: 36.45 s
Epoch 0: 70.03% done
Loss: 388.33484149942495
Time: 38.19 s
Epoch 0: 80.05% done
Loss: 393.550687459246
Time: 39.99 s
Epoch 0: 90.03% done
Loss: 397.81702666571647
Time: 41.76 s

Epoch 0 done
Epoch loss: 455.6228224621612

Time taken for epoch: 43.89 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 415.629731187033

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.05% done
Loss: 128.10165882110596
Time: 0.00 s
Epoch 1: 10.04% done
Loss: 378.7111984960961
Time: 1.28 s
Epoch 1: 20.03% done
Loss: 357.92945638449504
Time: 3.02 s
Epoch 1: 30.02% done
Loss: 355.67221268258913
Time: 4.74 s
Epoch 1: 40.01% done
Loss: 376.03780591126645
Time: 6.47 s
Epoch 1: 50.05% done
Loss: 346.6929266650473
Time: 8.25 s
Epoch 1: 60.04% done
Loss: 355.04212856593756
Time: 9.99 s
Epoch 1: 70.03% done
Loss: 338.8903787039747
Time: 11.84 s
Epoch 1: 80.02% done
Loss: 323.16603361055104
Time: 13.76 s
Epoch 1: 90.01% done
Loss: 367.8914098577066
Time: 15.50 s

Epoch 1 done
Epoch loss: 353.6079712490261

Time taken for epoch: 17.68 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 414.7092785310308

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.05% done
Loss: 195.1843023300171
Time: 0.01 s
Epoch 2: 10.04% done
Loss: 327.9207980542472
Time: 1.31 s
Epoch 2: 20.02% done
Loss: 306.06250200006696
Time: 3.11 s
Epoch 2: 30.01% done
Loss: 333.8399588158636
Time: 4.90 s
Epoch 2: 40.04% done
Loss: 348.900985568013
Time: 6.67 s
Epoch 2: 50.03% done
Loss: 305.45765151730694
Time: 8.43 s
Epoch 2: 60.01% done
Loss: 347.01588308871396
Time: 10.16 s
Epoch 2: 70.05% done
Loss: 299.6737968307644
Time: 11.94 s
Epoch 2: 80.03% done
Loss: 324.5665346469843
Time: 13.74 s
Epoch 2: 90.02% done
Loss: 301.4715381418214
Time: 15.51 s

Epoch 2 done
Epoch loss: 320.8885028225661

Time taken for epoch: 17.66 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.40 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 414.3210204369431

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.05% done
Loss: 35.731878876686096
Time: 0.01 s
Epoch 3: 10.03% done
Loss: 286.6580573296306
Time: 1.29 s
Epoch 3: 20.01% done
Loss: 309.9845935750489
Time: 3.06 s
Epoch 3: 30.04% done
Loss: 303.01371754039474
Time: 4.82 s
Epoch 3: 40.02% done
Loss: 292.28947255195993
Time: 6.55 s
Epoch 3: 50.05% done
Loss: 293.33646130659173
Time: 8.33 s
Epoch 3: 60.03% done
Loss: 300.49636611771405
Time: 10.13 s
Epoch 3: 70.01% done
Loss: 300.8194333676136
Time: 11.95 s
Epoch 3: 80.04% done
Loss: 316.3401319565785
Time: 13.75 s
Epoch 3: 90.02% done
Loss: 287.0118253083542
Time: 15.57 s

Epoch 3 done
Epoch loss: 298.6060250839669

Time taken for epoch: 17.85 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.94936218174223

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.05% done
Loss: 538.730525970459
Time: 0.01 s
Epoch 4: 10.03% done
Loss: 321.5720083545705
Time: 1.28 s
Epoch 4: 20.05% done
Loss: 296.45948053978793
Time: 3.05 s
Epoch 4: 30.03% done
Loss: 272.16269116510045
Time: 4.85 s
Epoch 4: 40.05% done
Loss: 278.4170709662701
Time: 6.62 s
Epoch 4: 50.03% done
Loss: 274.06693717295474
Time: 8.40 s
Epoch 4: 60.05% done
Loss: 288.0201243013892
Time: 10.15 s
Epoch 4: 70.03% done
Loss: 272.62803338505705
Time: 12.11 s
Epoch 4: 80.05% done
Loss: 273.988458323269
Time: 13.98 s
Epoch 4: 90.03% done
Loss: 312.1900116328639
Time: 15.75 s

Epoch 4 done
Epoch loss: 286.80933130140266

Time taken for epoch: 17.92 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.4610870562562

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.05% done
Loss: 403.21950912475586
Time: 0.01 s
Epoch 5: 10.04% done
Loss: 257.11547383906867
Time: 1.28 s
Epoch 5: 20.03% done
Loss: 256.3470996351856
Time: 3.01 s
Epoch 5: 30.02% done
Loss: 270.0264844018025
Time: 4.79 s
Epoch 5: 40.01% done
Loss: 265.33764117971214
Time: 6.58 s
Epoch 5: 50.05% done
Loss: 259.28066197838916
Time: 8.38 s
Epoch 5: 60.04% done
Loss: 252.6562176644802
Time: 10.09 s
Epoch 5: 70.03% done
Loss: 291.14189498653315
Time: 11.84 s
Epoch 5: 80.02% done
Loss: 257.79461038646974
Time: 13.65 s
Epoch 5: 90.01% done
Loss: 249.7483090920882
Time: 15.40 s

Epoch 5 done
Epoch loss: 262.985956046625

Time taken for epoch: 17.59 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.1988884112157

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.05% done
Loss: 29.01744246482849
Time: 0.00 s
Epoch 6: 10.03% done
Loss: 232.3594692304279
Time: 1.29 s
Epoch 6: 20.01% done
Loss: 230.13347428517812
Time: 3.07 s
Epoch 6: 30.04% done
Loss: 264.1929119001681
Time: 4.85 s
Epoch 6: 40.02% done
Loss: 256.1342708634758
Time: 6.63 s
Epoch 6: 50.05% done
Loss: 232.78785605526448
Time: 8.38 s
Epoch 6: 60.03% done
Loss: 246.4531227871023
Time: 10.15 s
Epoch 6: 70.01% done
Loss: 246.424857741504
Time: 11.92 s
Epoch 6: 80.04% done
Loss: 253.6380698895035
Time: 13.72 s
Epoch 6: 90.02% done
Loss: 251.16226334557538
Time: 15.45 s

Epoch 6 done
Epoch loss: 245.61658321660119

Time taken for epoch: 17.56 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 412.8325030344342

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.05% done
Loss: 310.72258949279785
Time: 0.00 s
Epoch 7: 10.03% done
Loss: 268.19849166304175
Time: 1.32 s
Epoch 7: 20.01% done
Loss: 239.42008835047181
Time: 3.08 s
Epoch 7: 30.04% done
Loss: 226.83113993487166
Time: 4.82 s
Epoch 7: 40.02% done
Loss: 241.60611471067173
Time: 6.55 s
Epoch 7: 50.05% done
Loss: 241.39267992808593
Time: 8.33 s
Epoch 7: 60.03% done
Loss: 250.18794476383863
Time: 10.11 s
Epoch 7: 70.01% done
Loss: 245.40894306970364
Time: 11.86 s
Epoch 7: 80.04% done
Loss: 255.55627767239983
Time: 13.59 s
Epoch 7: 90.02% done
Loss: 256.8429030590888
Time: 15.38 s

Epoch 7 done
Epoch loss: 246.07637996237065

Time taken for epoch: 17.51 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 412.7482263320083

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.05% done
Loss: 146.3983654975891
Time: 0.01 s
Epoch 8: 10.04% done
Loss: 218.7758443632511
Time: 1.30 s
Epoch 8: 20.03% done
Loss: 229.34737693085665
Time: 3.05 s
Epoch 8: 30.02% done
Loss: 229.60269505515544
Time: 4.87 s
Epoch 8: 40.01% done
Loss: 246.63697504485495
Time: 6.64 s
Epoch 8: 50.05% done
Loss: 228.32943032959
Time: 8.43 s
Epoch 8: 60.04% done
Loss: 230.83447583320768
Time: 10.22 s
Epoch 8: 70.03% done
Loss: 237.0524766544501
Time: 11.95 s
Epoch 8: 80.02% done
Loss: 228.20667119113483
Time: 13.73 s
Epoch 8: 90.01% done
Loss: 236.22860698161102
Time: 15.51 s

Epoch 8 done
Epoch loss: 231.01107917404636

Time taken for epoch: 17.68 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 412.98427811456384

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.05% done
Loss: 94.61179971694946
Time: 0.01 s
Epoch 9: 10.03% done
Loss: 223.29506884432502
Time: 1.30 s
Epoch 9: 20.05% done
Loss: 227.278386709753
Time: 3.11 s
Epoch 9: 30.03% done
Loss: 226.3955760766009
Time: 4.87 s
Epoch 9: 40.05% done
Loss: 228.2393785352683
Time: 6.66 s
Epoch 9: 50.03% done
Loss: 221.4549971198795
Time: 8.45 s
Epoch 9: 60.05% done
Loss: 206.97621476635263
Time: 10.25 s
Epoch 9: 70.03% done
Loss: 220.8324957102763
Time: 12.00 s
Epoch 9: 80.05% done
Loss: 215.84744436207728
Time: 13.78 s
Epoch 9: 90.03% done
Loss: 220.75512417112336
Time: 15.58 s

Epoch 9 done
Epoch loss: 222.43363510685919

Time taken for epoch: 17.78 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.40 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 413.13962969211263

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_9.pt

Regenerated paired data
Epoch 10: 0.05% done
Loss: 91.51614904403687
Time: 0.01 s
Epoch 10: 10.03% done
Loss: 214.12056698416822
Time: 1.38 s
Epoch 10: 20.01% done
Loss: 213.91175420333943
Time: 3.24 s
Epoch 10: 30.04% done
Loss: 229.1752675613716
Time: 5.05 s
Epoch 10: 40.02% done
Loss: 192.24994615322413
Time: 6.77 s
Epoch 10: 50.05% done
Loss: 208.93405962304854
Time: 8.54 s
Epoch 10: 60.03% done
Loss: 199.82364190874077
Time: 10.26 s
Epoch 10: 70.01% done
Loss: 201.51108262138536
Time: 11.99 s
Epoch 10: 80.04% done
Loss: 192.775138733375
Time: 13.73 s
Epoch 10: 90.02% done
Loss: 207.6892853723933
Time: 15.49 s

Epoch 10 done
Epoch loss: 207.8681375333784

Time taken for epoch: 17.71 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.147785685478

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_10.pt

Regenerated paired data
Epoch 11: 0.05% done
Loss: 57.70745277404785
Time: 0.01 s
Epoch 11: 10.03% done
Loss: 186.77982529531224
Time: 1.42 s
Epoch 11: 20.05% done
Loss: 199.13844348647487
Time: 3.17 s
Epoch 11: 30.03% done
Loss: 189.66125124933743
Time: 4.97 s
Epoch 11: 40.05% done
Loss: 215.16835862330754
Time: 6.76 s
Epoch 11: 50.03% done
Loss: 208.0927517660188
Time: 8.58 s
Epoch 11: 60.05% done
Loss: 188.04215719951458
Time: 10.38 s
Epoch 11: 70.03% done
Loss: 213.63037972325327
Time: 12.17 s
Epoch 11: 80.05% done
Loss: 202.53199915116156
Time: 13.96 s
Epoch 11: 90.03% done
Loss: 169.2401401623331
Time: 15.75 s

Epoch 11 done
Epoch loss: 196.58502091895272

Time taken for epoch: 17.92 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 413.3604257478627

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_11.pt

Regenerated paired data
Epoch 12: 0.05% done
Loss: 221.08354568481445
Time: 0.01 s
Epoch 12: 10.03% done
Loss: 209.53935057564516
Time: 1.24 s
Epoch 12: 20.05% done
Loss: 177.0267730906381
Time: 2.98 s
Epoch 12: 30.03% done
Loss: 178.38609556507583
Time: 4.70 s
Epoch 12: 40.05% done
Loss: 207.75382317340555
Time: 6.45 s
Epoch 12: 50.03% done
Loss: 199.895557776244
Time: 8.15 s
Epoch 12: 60.05% done
Loss: 192.85538371689805
Time: 9.91 s
Epoch 12: 70.03% done
Loss: 217.2358749622498
Time: 11.65 s
Epoch 12: 80.05% done
Loss: 179.23288616590463
Time: 13.47 s
Epoch 12: 90.03% done
Loss: 182.93020187717195
Time: 15.19 s

Epoch 12 done
Epoch loss: 193.40326160408313

Time taken for epoch: 17.39 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 413.6179476702979

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_12.pt

Regenerated paired data
Epoch 13: 0.05% done
Loss: 582.048225402832
Time: 0.01 s
Epoch 13: 10.04% done
Loss: 176.84596723340678
Time: 1.25 s
Epoch 13: 20.02% done
Loss: 191.1415291395076
Time: 2.95 s
Epoch 13: 30.01% done
Loss: 197.87918493208107
Time: 4.78 s
Epoch 13: 40.04% done
Loss: 194.70222195863124
Time: 6.58 s
Epoch 13: 50.03% done
Loss: 172.85481046584218
Time: 8.38 s
Epoch 13: 60.01% done
Loss: 181.80438242100104
Time: 10.16 s
Epoch 13: 70.05% done
Loss: 176.25760152823662
Time: 11.92 s
Epoch 13: 80.03% done
Loss: 184.0696661328577
Time: 13.68 s
Epoch 13: 90.02% done
Loss: 179.38829996256214
Time: 15.47 s

Epoch 13 done
Epoch loss: 184.32452841518625

Time taken for epoch: 17.60 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.03 s
Calculating validation loss: 20.18% done
Time: 0.15 s
Calculating validation loss: 40.37% done
Time: 0.28 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.9617601665882

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_13.pt

Regenerated paired data
Epoch 14: 0.05% done
Loss: 47.838231921195984
Time: 0.00 s
Epoch 14: 10.03% done
Loss: 180.04671228353422
Time: 1.22 s
Epoch 14: 20.01% done
Loss: 181.594245540298
Time: 3.02 s
Epoch 14: 30.04% done
Loss: 199.96464620133742
Time: 4.80 s
Epoch 14: 40.02% done
Loss: 176.8326504391176
Time: 6.60 s
Epoch 14: 50.05% done
Loss: 176.76311913859004
Time: 8.40 s
Epoch 14: 60.03% done
Loss: 171.75558071696398
Time: 10.33 s
Epoch 14: 70.01% done
Loss: 181.126217968348
Time: 12.13 s
Epoch 14: 80.04% done
Loss: 179.18403915986045
Time: 13.96 s
Epoch 14: 90.02% done
Loss: 185.94066358801692
Time: 15.65 s

Epoch 14 done
Epoch loss: 179.27947503781218

Time taken for epoch: 17.81 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.48 s

Validation loss: 413.7670962088699

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_14.pt

Regenerated paired data
Epoch 15: 0.05% done
Loss: 186.31349802017212
Time: 0.01 s
Epoch 15: 10.03% done
Loss: 158.48189174018876
Time: 1.21 s
Epoch 15: 20.05% done
Loss: 167.3472084516856
Time: 2.98 s
Epoch 15: 30.03% done
Loss: 148.8060091631581
Time: 4.71 s
Epoch 15: 40.05% done
Loss: 170.65105917091347
Time: 6.45 s
Epoch 15: 50.03% done
Loss: 163.14899764597567
Time: 8.18 s
Epoch 15: 60.05% done
Loss: 173.41313085252017
Time: 9.95 s
Epoch 15: 70.03% done
Loss: 161.7882450384022
Time: 11.69 s
Epoch 15: 80.05% done
Loss: 169.04796218527622
Time: 13.46 s
Epoch 15: 90.03% done
Loss: 162.76520303629263
Time: 15.21 s

Epoch 15 done
Epoch loss: 164.04122150214553

Time taken for epoch: 17.32 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.49 s

Validation loss: 413.5204702342322

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_15.pt

Regenerated paired data
Epoch 16: 0.05% done
Loss: 54.65893745422363
Time: 0.00 s
Epoch 16: 10.03% done
Loss: 155.50630487915544
Time: 1.44 s
Epoch 16: 20.01% done
Loss: 163.10208749891532
Time: 3.14 s
Epoch 16: 30.04% done
Loss: 168.43075882924262
Time: 4.84 s
Epoch 16: 40.02% done
Loss: 167.96115745856153
Time: 6.82 s
Epoch 16: 50.05% done
Loss: 172.27372316334714
Time: 8.58 s
Epoch 16: 60.03% done
Loss: 167.32377824232435
Time: 10.41 s
Epoch 16: 70.01% done
Loss: 175.58675860289972
Time: 12.18 s
Epoch 16: 80.04% done
Loss: 170.935929280205
Time: 13.99 s
Epoch 16: 90.02% done
Loss: 166.44715071665217
Time: 15.71 s

Epoch 16 done
Epoch loss: 168.487069617878

Time taken for epoch: 17.85 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.40 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.8150543247888

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_16.pt

Regenerated paired data
Epoch 17: 0.05% done
Loss: 100.70323944091797
Time: 0.01 s
Epoch 17: 10.04% done
Loss: 161.82955353511403
Time: 1.30 s
Epoch 17: 20.03% done
Loss: 151.39159081818894
Time: 3.08 s
Epoch 17: 30.02% done
Loss: 168.80820239470765
Time: 4.86 s
Epoch 17: 40.01% done
Loss: 144.24496189963938
Time: 6.62 s
Epoch 17: 50.05% done
Loss: 160.8611563529501
Time: 8.31 s
Epoch 17: 60.04% done
Loss: 163.65451261852726
Time: 10.14 s
Epoch 17: 70.03% done
Loss: 148.77926460138025
Time: 11.91 s
Epoch 17: 80.02% done
Loss: 172.48215861806665
Time: 13.70 s
Epoch 17: 90.01% done
Loss: 150.17461231862657
Time: 15.54 s

Epoch 17 done
Epoch loss: 159.8194184861316

Time taken for epoch: 17.68 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.40 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.61734538996984

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_17.pt

Regenerated paired data
Epoch 18: 0.05% done
Loss: 130.82479238510132
Time: 0.01 s
Epoch 18: 10.03% done
Loss: 153.9696037374211
Time: 1.31 s
Epoch 18: 20.05% done
Loss: 155.4551464996895
Time: 3.13 s
Epoch 18: 30.03% done
Loss: 163.703315224348
Time: 4.96 s
Epoch 18: 40.05% done
Loss: 146.69664067370658
Time: 6.76 s
Epoch 18: 50.03% done
Loss: 145.72763538383174
Time: 8.59 s
Epoch 18: 60.05% done
Loss: 153.39675811142777
Time: 10.38 s
Epoch 18: 70.03% done
Loss: 142.33553400357263
Time: 12.14 s
Epoch 18: 80.05% done
Loss: 154.87098768915064
Time: 13.95 s
Epoch 18: 90.03% done
Loss: 171.5740686047333
Time: 15.67 s

Epoch 18 done
Epoch loss: 153.86962774371545

Time taken for epoch: 17.83 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 414.14043498695446

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_18.pt

Regenerated paired data
Epoch 19: 0.05% done
Loss: 405.95502853393555
Time: 0.01 s
Epoch 19: 10.03% done
Loss: 131.81417970588865
Time: 1.33 s
Epoch 19: 20.01% done
Loss: 160.26605352726435
Time: 3.09 s
Epoch 19: 30.04% done
Loss: 156.17741265863032
Time: 4.86 s
Epoch 19: 40.02% done
Loss: 145.60261523689735
Time: 6.62 s
Epoch 19: 50.05% done
Loss: 158.08811354914204
Time: 8.36 s
Epoch 19: 60.03% done
Loss: 145.88048100998304
Time: 10.07 s
Epoch 19: 70.01% done
Loss: 141.3939476935099
Time: 11.86 s
Epoch 19: 80.04% done
Loss: 160.21441069136762
Time: 13.68 s
Epoch 19: 90.02% done
Loss: 152.54378027724798
Time: 15.53 s

Epoch 19 done
Epoch loss: 151.74655434208447

Time taken for epoch: 17.67 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.53 s

Validation loss: 414.356624861376

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_19.pt

Regenerated paired data
Epoch 20: 0.05% done
Loss: 141.32142066955566
Time: 0.01 s
Epoch 20: 10.04% done
Loss: 140.79696621576494
Time: 1.31 s
Epoch 20: 20.03% done
Loss: 145.05557452394353
Time: 3.28 s
Epoch 20: 30.02% done
Loss: 131.33548554221187
Time: 5.02 s
Epoch 20: 40.01% done
Loss: 156.68348930587973
Time: 6.78 s
Epoch 20: 50.05% done
Loss: 147.17668742877456
Time: 8.55 s
Epoch 20: 60.04% done
Loss: 143.71671025838816
Time: 10.64 s
Epoch 20: 70.03% done
Loss: 143.48598472736398
Time: 12.41 s
Epoch 20: 80.02% done
Loss: 152.5814558031282
Time: 14.16 s
Epoch 20: 90.01% done
Loss: 160.63230577291864
Time: 15.90 s

Epoch 20 done
Epoch loss: 144.84066839765018

Time taken for epoch: 17.98 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 413.9628932016705

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_20.pt

Regenerated paired data
Epoch 21: 0.05% done
Loss: 28.256097435951233
Time: 0.00 s
Epoch 21: 10.03% done
Loss: 142.94810335932658
Time: 1.29 s
Epoch 21: 20.05% done
Loss: 124.99381859023967
Time: 3.05 s
Epoch 21: 30.03% done
Loss: 122.98075500687565
Time: 4.81 s
Epoch 21: 40.05% done
Loss: 115.76997306123002
Time: 6.56 s
Epoch 21: 50.03% done
Loss: 155.01101325369544
Time: 8.35 s
Epoch 21: 60.05% done
Loss: 137.81053033896052
Time: 10.14 s
Epoch 21: 70.03% done
Loss: 141.0313554202216
Time: 11.88 s
Epoch 21: 80.05% done
Loss: 136.51638658221194
Time: 13.65 s
Epoch 21: 90.03% done
Loss: 141.2578920598584
Time: 15.44 s

Epoch 21 done
Epoch loss: 135.78037017299525

Time taken for epoch: 17.63 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.52 s

Validation loss: 413.99411608319764

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_21.pt

Regenerated paired data
Epoch 22: 0.05% done
Loss: 94.54365968704224
Time: 0.01 s
Epoch 22: 10.03% done
Loss: 122.22453311951172
Time: 1.29 s
Epoch 22: 20.01% done
Loss: 123.77933546468013
Time: 3.06 s
Epoch 22: 30.04% done
Loss: 146.72675946605716
Time: 4.81 s
Epoch 22: 40.02% done
Loss: 134.3860724498753
Time: 6.56 s
Epoch 22: 50.05% done
Loss: 143.34665443935435
Time: 8.34 s
Epoch 22: 60.03% done
Loss: 137.68406530672854
Time: 10.07 s
Epoch 22: 70.01% done
Loss: 130.82254350448798
Time: 11.85 s
Epoch 22: 80.04% done
Loss: 139.98324053184172
Time: 13.64 s
Epoch 22: 90.02% done
Loss: 129.54604233041255
Time: 15.43 s

Epoch 22 done
Epoch loss: 136.1911919657066

Time taken for epoch: 17.54 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.49 s

Validation loss: 414.46982840879247

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_100_middle_dim_512_output_dim_128_lr_0.0005_weight_decay_0.01/2024-08-05_20:24:12_checkpoint_epoch_22.pt

Regenerated paired data
Validation loss has not improved for 15 epochs. Stopping training.
BEST VALIDATION LOSS: 412.7482263320083 at epoch 7

