Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-06_22:19:10
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 5, learning rate: 0.001
clip norm: 10, temperature: 0.07, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr_division: 100
temperature: 0.07

Created paired data
Created paired data
Time taken to create datasets: 74.86 s
Number of parameters in model: 6825984

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 647.8575134277344
Time: 15.76 s
Epoch 0: 10.04% done
Loss: 751.585137331033
Time: 21.16 s
Epoch 0: 20.08% done
Loss: 710.5418057441711
Time: 23.11 s
Epoch 0: 30.11% done
Loss: 537.9585740566254
Time: 24.56 s
Epoch 0: 40.03% done
Loss: 462.426432476768
Time: 26.71 s
Epoch 0: 50.06% done
Loss: 405.8855741918087
Time: 28.72 s
Epoch 0: 60.10% done
Loss: 346.78954005241394
Time: 30.63 s
Epoch 0: 70.01% done
Loss: 343.0794537218311
Time: 32.60 s
Epoch 0: 80.05% done
Loss: 329.486390709877
Time: 34.60 s
Epoch 0: 90.09% done
Loss: 324.3679313659668
Time: 36.45 s

Epoch 0 done
Epoch loss: 453.9796945381643

Time taken for epoch: 38.68 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.29 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.55 s

Validation loss: 340.06270178850144

Time taken: 0.68 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 311.3254737854004
Time: 0.01 s
Epoch 1: 10.06% done
Loss: 323.1063873556596
Time: 1.56 s
Epoch 1: 20.13% done
Loss: 316.1932392716408
Time: 3.62 s
Epoch 1: 30.06% done
Loss: 320.4200734166656
Time: 5.73 s
Epoch 1: 40.13% done
Loss: 303.88077187538147
Time: 7.65 s
Epoch 1: 50.06% done
Loss: 298.78834470917906
Time: 9.62 s
Epoch 1: 60.13% done
Loss: 302.30088686943054
Time: 11.51 s
Epoch 1: 70.06% done
Loss: 294.2832938327065
Time: 13.54 s
Epoch 1: 80.13% done
Loss: 288.6615813970566
Time: 15.60 s
Epoch 1: 90.06% done
Loss: 268.42364721660374
Time: 17.47 s

Epoch 1 done
Epoch loss: 299.4819790142387

Time taken for epoch: 19.75 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.23 s
Calculating validation loss: 40.22% done
Time: 0.39 s
Calculating validation loss: 60.87% done
Time: 0.53 s
Calculating validation loss: 80.43% done
Time: 0.64 s

Validation loss: 338.06560188099957

Time taken: 0.77 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 316.2721824645996
Time: 0.01 s
Epoch 2: 10.06% done
Loss: 265.64146794347323
Time: 1.33 s
Epoch 2: 20.13% done
Loss: 254.75530886650085
Time: 3.25 s
Epoch 2: 30.06% done
Loss: 258.71806072283397
Time: 5.33 s
Epoch 2: 40.13% done
Loss: 249.4156129360199
Time: 7.26 s
Epoch 2: 50.06% done
Loss: 244.46749832056744
Time: 9.24 s
Epoch 2: 60.13% done
Loss: 231.48667132854462
Time: 11.29 s
Epoch 2: 70.06% done
Loss: 243.58741905115826
Time: 13.14 s
Epoch 2: 80.13% done
Loss: 231.37708446383476
Time: 15.08 s
Epoch 2: 90.06% done
Loss: 235.36014399951017
Time: 17.10 s

Epoch 2 done
Epoch loss: 245.14277193181422

Time taken for epoch: 19.40 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.16 s
Calculating validation loss: 40.22% done
Time: 0.30 s
Calculating validation loss: 60.87% done
Time: 0.43 s
Calculating validation loss: 80.43% done
Time: 0.56 s

Validation loss: 337.0946866878565

Time taken: 0.71 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 299.9984359741211
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 213.2534614997574
Time: 2.06 s
Epoch 3: 20.13% done
Loss: 199.124014377594
Time: 4.06 s
Epoch 3: 30.06% done
Loss: 214.20807186561294
Time: 5.53 s
Epoch 3: 40.13% done
Loss: 200.24635192751884
Time: 7.52 s
Epoch 3: 50.06% done
Loss: 181.19169624545907
Time: 9.42 s
Epoch 3: 60.13% done
Loss: 188.10115241010985
Time: 11.32 s
Epoch 3: 70.06% done
Loss: 193.07953080044518
Time: 13.20 s
Epoch 3: 80.13% done
Loss: 176.0338203907013
Time: 15.05 s
Epoch 3: 90.06% done
Loss: 198.95377014256732
Time: 17.13 s

Epoch 3 done
Epoch loss: 195.22185636816286

Time taken for epoch: 19.48 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.29 s
Calculating validation loss: 40.22% done
Time: 0.39 s
Calculating validation loss: 60.87% done
Time: 0.52 s
Calculating validation loss: 80.43% done
Time: 0.64 s

Validation loss: 336.5599603065546

Time taken: 0.76 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 51.41158580780029
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 170.12949455387985
Time: 1.38 s
Epoch 4: 20.03% done
Loss: 168.76121490816527
Time: 3.24 s
Epoch 4: 30.10% done
Loss: 155.02350062131882
Time: 5.18 s
Epoch 4: 40.05% done
Loss: 161.86014407797705
Time: 7.27 s
Epoch 4: 50.13% done
Loss: 144.49155044555664
Time: 9.25 s
Epoch 4: 60.08% done
Loss: 143.90067209171343
Time: 11.23 s
Epoch 4: 70.03% done
Loss: 140.79228343842905
Time: 13.17 s
Epoch 4: 80.10% done
Loss: 142.56271091103554
Time: 15.00 s
Epoch 4: 90.05% done
Loss: 148.5485778579229
Time: 17.05 s

Epoch 4 done
Epoch loss: 150.08638966068514

Time taken for epoch: 19.40 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.06 s
Calculating validation loss: 20.65% done
Time: 0.26 s
Calculating validation loss: 40.22% done
Time: 0.39 s
Calculating validation loss: 60.87% done
Time: 0.51 s
Calculating validation loss: 80.43% done
Time: 0.63 s

Validation loss: 336.7906251852063

Time taken: 0.77 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 38.464252948760986
Time: 0.02 s
Epoch 5: 10.09% done
Loss: 126.26526459862914
Time: 1.95 s
Epoch 5: 20.05% done
Loss: 130.1707923261425
Time: 3.42 s
Epoch 5: 30.01% done
Loss: 110.03756796257406
Time: 5.36 s
Epoch 5: 40.10% done
Loss: 125.58633319183718
Time: 7.27 s
Epoch 5: 50.06% done
Loss: 110.73940260500848
Time: 9.15 s
Epoch 5: 60.03% done
Loss: 97.29002708121192
Time: 10.98 s
Epoch 5: 70.11% done
Loss: 115.33853398729116
Time: 12.92 s
Epoch 5: 80.08% done
Loss: 107.7413338246728
Time: 14.96 s
Epoch 5: 90.04% done
Loss: 99.90445598016811
Time: 16.98 s

Epoch 5 done
Epoch loss: 113.22443310666584

Time taken for epoch: 19.32 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.03 s
Calculating validation loss: 20.65% done
Time: 0.17 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.56 s

Validation loss: 337.0903436342875

Time taken: 0.70 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 88.28874588012695
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 86.14363994779465
Time: 1.96 s
Epoch 6: 20.13% done
Loss: 92.81672835620702
Time: 3.41 s
Epoch 6: 30.06% done
Loss: 85.00095904627933
Time: 5.34 s
Epoch 6: 40.13% done
Loss: 79.5438852161169
Time: 7.32 s
Epoch 6: 50.06% done
Loss: 84.71818413915513
Time: 9.30 s
Epoch 6: 60.13% done
Loss: 83.84285134077072
Time: 11.34 s
Epoch 6: 70.06% done
Loss: 70.7704705603515
Time: 13.29 s
Epoch 6: 80.13% done
Loss: 90.0742475092411
Time: 15.23 s
Epoch 6: 90.06% done
Loss: 94.63798114770576
Time: 17.14 s

Epoch 6 done
Epoch loss: 84.9991589959787

Time taken for epoch: 19.54 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 337.3964920942334

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 131.85617446899414
Time: 0.01 s
Epoch 7: 10.04% done
Loss: 59.93800716309608
Time: 1.96 s
Epoch 7: 20.08% done
Loss: 69.15138268470764
Time: 3.47 s
Epoch 7: 30.11% done
Loss: 66.4837855771184
Time: 5.54 s
Epoch 7: 40.03% done
Loss: 75.14511478098133
Time: 7.56 s
Epoch 7: 50.06% done
Loss: 69.78337347507477
Time: 9.55 s
Epoch 7: 60.10% done
Loss: 58.288673382252455
Time: 11.49 s
Epoch 7: 70.01% done
Loss: 71.03383524508416
Time: 13.44 s
Epoch 7: 80.05% done
Loss: 62.946275267750025
Time: 15.31 s
Epoch 7: 90.09% done
Loss: 66.18636202439666
Time: 17.31 s

Epoch 7 done
Epoch loss: 66.09793428345635

Time taken for epoch: 19.60 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 338.00226373948914

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 12.744778394699097
Time: 0.02 s
Epoch 8: 10.08% done
Loss: 46.86480326554443
Time: 1.95 s
Epoch 8: 20.03% done
Loss: 48.35905814472633
Time: 3.31 s
Epoch 8: 30.10% done
Loss: 47.60747777670622
Time: 5.19 s
Epoch 8: 40.05% done
Loss: 52.90808114195674
Time: 7.12 s
Epoch 8: 50.13% done
Loss: 47.85582394339144
Time: 9.14 s
Epoch 8: 60.08% done
Loss: 49.659002004544945
Time: 11.11 s
Epoch 8: 70.03% done
Loss: 46.32727542821365
Time: 13.09 s
Epoch 8: 80.10% done
Loss: 43.570701632648706
Time: 14.98 s
Epoch 8: 90.05% done
Loss: 58.07031745397592
Time: 16.98 s

Epoch 8 done
Epoch loss: 49.657885514005805

Time taken for epoch: 19.34 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.53 s

Validation loss: 338.32174225129944

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_lr_0.001_tmp_0.07/2024-08-06_22:19:10_checkpoint_epoch_8.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 336.5599603065546 at epoch 3

