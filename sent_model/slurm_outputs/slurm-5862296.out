Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-05_20:08:53
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 50, patience: 15, learning rate: 0.0005
clip norm: 10, temperature: 0.15, num pairs per batch: 2
time limit to create dataset: 600
weight decay: 0.01
awe_lr_division: 10
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 55.07 s
up_proj_dim: 512
output_dim: 256
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 7613952
Number of parameters in AWE model: 6825984
Number of parameters in other model: 787968
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.05% done
Loss: 542.009687423706
Time: 21.17 s
Epoch 0: 10.03% done
Loss: 634.4280588205414
Time: 24.86 s
Epoch 0: 20.05% done
Loss: 450.58428414502936
Time: 26.55 s
Epoch 0: 30.03% done
Loss: 392.53616275811436
Time: 28.27 s
Epoch 0: 40.05% done
Loss: 361.627181361069
Time: 30.01 s
Epoch 0: 50.03% done
Loss: 367.8103637815726
Time: 31.70 s
Epoch 0: 60.05% done
Loss: 362.2524870100932
Time: 33.53 s
Epoch 0: 70.03% done
Loss: 376.9894185692373
Time: 35.63 s
Epoch 0: 80.05% done
Loss: 332.3009337312612
Time: 37.33 s
Epoch 0: 90.03% done
Loss: 330.1761951560926
Time: 39.00 s

Epoch 0 done
Epoch loss: 394.05365219644693

Time taken for epoch: 41.17 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 410.3766527744608

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.05% done
Loss: 198.72279167175293
Time: 0.01 s
Epoch 1: 10.04% done
Loss: 294.6199353295143
Time: 1.27 s
Epoch 1: 20.03% done
Loss: 283.8519133138235
Time: 3.00 s
Epoch 1: 30.02% done
Loss: 305.27581688054283
Time: 4.76 s
Epoch 1: 40.01% done
Loss: 288.1266876001551
Time: 6.41 s
Epoch 1: 50.05% done
Loss: 270.41561405487994
Time: 8.10 s
Epoch 1: 60.04% done
Loss: 283.3807514096149
Time: 9.77 s
Epoch 1: 70.03% done
Loss: 269.0244432576377
Time: 11.43 s
Epoch 1: 80.02% done
Loss: 259.3186062619542
Time: 13.11 s
Epoch 1: 90.01% done
Loss: 243.2976523564771
Time: 14.85 s

Epoch 1 done
Epoch loss: 273.51097724476585

Time taken for epoch: 16.99 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.47 s

Validation loss: 409.5750923550457

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.05% done
Loss: 120.11944055557251
Time: 0.00 s
Epoch 2: 10.04% done
Loss: 215.42704097098775
Time: 1.26 s
Epoch 2: 20.02% done
Loss: 231.2337448161961
Time: 3.01 s
Epoch 2: 30.01% done
Loss: 200.9880942941615
Time: 4.75 s
Epoch 2: 40.04% done
Loss: 210.65723948817157
Time: 6.41 s
Epoch 2: 50.03% done
Loss: 197.707949488452
Time: 8.13 s
Epoch 2: 60.01% done
Loss: 192.07813723407912
Time: 9.92 s
Epoch 2: 70.05% done
Loss: 206.87332887333542
Time: 11.59 s
Epoch 2: 80.03% done
Loss: 195.27360499657766
Time: 13.28 s
Epoch 2: 90.02% done
Loss: 189.99264399718606
Time: 14.98 s

Epoch 2 done
Epoch loss: 200.9957144806651

Time taken for epoch: 17.09 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.11 s
Calculating validation loss: 40.37% done
Time: 0.23 s
Calculating validation loss: 60.09% done
Time: 0.35 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 409.9626623162436

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.05% done
Loss: 8.205556869506836
Time: 0.00 s
Epoch 3: 10.03% done
Loss: 166.44598740491676
Time: 1.28 s
Epoch 3: 20.01% done
Loss: 155.39885344209546
Time: 2.97 s
Epoch 3: 30.04% done
Loss: 154.30992616143956
Time: 4.71 s
Epoch 3: 40.02% done
Loss: 167.9075031484844
Time: 6.41 s
Epoch 3: 50.05% done
Loss: 173.7030783495
Time: 8.14 s
Epoch 3: 60.03% done
Loss: 179.31734278910992
Time: 9.87 s
Epoch 3: 70.01% done
Loss: 141.1041129267577
Time: 11.54 s
Epoch 3: 80.04% done
Loss: 159.50080073958065
Time: 13.24 s
Epoch 3: 90.02% done
Loss: 148.85032892772796
Time: 15.00 s

Epoch 3 done
Epoch loss: 159.96695319364306

Time taken for epoch: 17.14 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.14 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 410.46459193623394

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.05% done
Loss: 43.09698939323425
Time: 0.01 s
Epoch 4: 10.03% done
Loss: 140.51268462543234
Time: 1.26 s
Epoch 4: 20.05% done
Loss: 139.1275747740014
Time: 2.97 s
Epoch 4: 30.03% done
Loss: 139.5755542032985
Time: 4.69 s
Epoch 4: 40.05% done
Loss: 124.30823100309576
Time: 6.44 s
Epoch 4: 50.03% done
Loss: 136.16393325718664
Time: 8.20 s
Epoch 4: 60.05% done
Loss: 137.57936926605413
Time: 9.89 s
Epoch 4: 70.03% done
Loss: 115.01838732228586
Time: 11.63 s
Epoch 4: 80.05% done
Loss: 136.51675082434286
Time: 13.39 s
Epoch 4: 90.03% done
Loss: 108.69200433689085
Time: 15.14 s

Epoch 4 done
Epoch loss: 131.08166565072327

Time taken for epoch: 17.32 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.23 s
Calculating validation loss: 60.09% done
Time: 0.35 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 410.9911185885788

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.05% done
Loss: 2.490420453250408
Time: 0.00 s
Epoch 5: 10.04% done
Loss: 116.69647107841541
Time: 1.19 s
Epoch 5: 20.03% done
Loss: 94.57734709680832
Time: 2.97 s
Epoch 5: 30.02% done
Loss: 105.4749999454038
Time: 4.72 s
Epoch 5: 40.01% done
Loss: 108.68946089211738
Time: 6.46 s
Epoch 5: 50.05% done
Loss: 108.96114701704688
Time: 8.14 s
Epoch 5: 60.04% done
Loss: 108.74788130614718
Time: 9.85 s
Epoch 5: 70.03% done
Loss: 103.71593155926375
Time: 11.54 s
Epoch 5: 80.02% done
Loss: 102.04966583317427
Time: 13.27 s
Epoch 5: 90.01% done
Loss: 104.38764279454269
Time: 15.06 s

Epoch 5 done
Epoch loss: 105.95756988261951

Time taken for epoch: 17.21 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.26 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.50 s

Validation loss: 411.30424934789676

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.05% done
Loss: 29.082971811294556
Time: 0.00 s
Epoch 6: 10.03% done
Loss: 78.51850509248447
Time: 1.29 s
Epoch 6: 20.01% done
Loss: 101.3342672182868
Time: 3.03 s
Epoch 6: 30.04% done
Loss: 102.15751215132262
Time: 4.76 s
Epoch 6: 40.02% done
Loss: 85.47790551092476
Time: 6.48 s
Epoch 6: 50.05% done
Loss: 77.92437160762036
Time: 8.28 s
Epoch 6: 60.03% done
Loss: 82.1908141843824
Time: 10.14 s
Epoch 6: 70.01% done
Loss: 74.30088176025134
Time: 11.92 s
Epoch 6: 80.04% done
Loss: 84.72953634944993
Time: 13.60 s
Epoch 6: 90.02% done
Loss: 80.15961922338289
Time: 15.25 s

Epoch 6 done
Epoch loss: 85.35967719923768

Time taken for epoch: 17.35 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.37 s
Calculating validation loss: 80.28% done
Time: 0.50 s

Validation loss: 411.25412184164065

Time taken: 0.62 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.05% done
Loss: 505.5121898651123
Time: 0.01 s
Epoch 7: 10.03% done
Loss: 80.61644368748547
Time: 1.30 s
Epoch 7: 20.01% done
Loss: 98.34538940810646
Time: 3.05 s
Epoch 7: 30.04% done
Loss: 76.3338829882106
Time: 4.81 s
Epoch 7: 40.02% done
Loss: 82.04549376622332
Time: 6.52 s
Epoch 7: 50.05% done
Loss: 68.2034148184124
Time: 8.24 s
Epoch 7: 60.03% done
Loss: 73.27696092407962
Time: 10.05 s
Epoch 7: 70.01% done
Loss: 81.12881499991724
Time: 11.77 s
Epoch 7: 80.04% done
Loss: 75.58325770306378
Time: 13.44 s
Epoch 7: 90.02% done
Loss: 64.36272402378646
Time: 15.17 s

Epoch 7 done
Epoch loss: 77.43093364366477

Time taken for epoch: 17.21 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.11 s
Calculating validation loss: 40.37% done
Time: 0.23 s
Calculating validation loss: 60.09% done
Time: 0.33 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 411.9144611402389

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.05% done
Loss: 5.266111716628075
Time: 0.01 s
Epoch 8: 10.04% done
Loss: 63.091798916205086
Time: 1.19 s
Epoch 8: 20.03% done
Loss: 76.55978578594372
Time: 2.90 s
Epoch 8: 30.02% done
Loss: 57.175462247775585
Time: 4.61 s
Epoch 8: 40.01% done
Loss: 66.71706412688387
Time: 6.32 s
Epoch 8: 50.05% done
Loss: 61.673146778081154
Time: 8.03 s
Epoch 8: 60.04% done
Loss: 61.03030200777199
Time: 9.76 s
Epoch 8: 70.03% done
Loss: 72.56303082027407
Time: 11.53 s
Epoch 8: 80.02% done
Loss: 64.34774639942175
Time: 13.27 s
Epoch 8: 90.01% done
Loss: 66.70190526473553
Time: 14.93 s

Epoch 8 done
Epoch loss: 65.30540539202559

Time taken for epoch: 17.02 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.34 s
Calculating validation loss: 80.28% done
Time: 0.45 s

Validation loss: 412.5782717258558

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.05% done
Loss: 124.4513750076294
Time: 0.01 s
Epoch 9: 10.03% done
Loss: 55.237405307619184
Time: 1.24 s
Epoch 9: 20.05% done
Loss: 60.26658602386488
Time: 2.88 s
Epoch 9: 30.03% done
Loss: 65.70842206746227
Time: 4.55 s
Epoch 9: 40.05% done
Loss: 65.27611027148801
Time: 6.21 s
Epoch 9: 50.03% done
Loss: 53.39399510547707
Time: 7.88 s
Epoch 9: 60.05% done
Loss: 57.879477212832654
Time: 9.57 s
Epoch 9: 70.03% done
Loss: 57.83851382962306
Time: 11.35 s
Epoch 9: 80.05% done
Loss: 46.81250904101747
Time: 13.06 s
Epoch 9: 90.03% done
Loss: 55.311044189382834
Time: 14.83 s

Epoch 9 done
Epoch loss: 57.169112452711694

Time taken for epoch: 16.98 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.39 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 413.4550732210142

Time taken: 0.64 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_9.pt

Regenerated paired data
Epoch 10: 0.05% done
Loss: 40.3356671333313
Time: 0.01 s
Epoch 10: 10.03% done
Loss: 53.17460369269335
Time: 1.29 s
Epoch 10: 20.01% done
Loss: 53.83155424726393
Time: 3.07 s
Epoch 10: 30.04% done
Loss: 48.96666571754148
Time: 4.85 s
Epoch 10: 40.02% done
Loss: 44.471962954566784
Time: 6.63 s
Epoch 10: 50.05% done
Loss: 49.773107880455015
Time: 8.33 s
Epoch 10: 60.03% done
Loss: 52.84653241296929
Time: 10.01 s
Epoch 10: 70.01% done
Loss: 42.51833239339781
Time: 11.76 s
Epoch 10: 80.04% done
Loss: 46.69166890221025
Time: 13.46 s
Epoch 10: 90.02% done
Loss: 41.165301123295315
Time: 15.11 s

Epoch 10 done
Epoch loss: 48.89260505397913

Time taken for epoch: 17.22 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.27 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.51 s

Validation loss: 413.2859512206611

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_10.pt

Regenerated paired data
Epoch 11: 0.05% done
Loss: 1.0363838635385036
Time: 0.01 s
Epoch 11: 10.03% done
Loss: 42.780504621932224
Time: 1.28 s
Epoch 11: 20.05% done
Loss: 42.29165699482713
Time: 2.99 s
Epoch 11: 30.03% done
Loss: 42.404099042536785
Time: 4.74 s
Epoch 11: 40.05% done
Loss: 51.703925508696905
Time: 6.45 s
Epoch 11: 50.03% done
Loss: 36.09542489731496
Time: 8.20 s
Epoch 11: 60.05% done
Loss: 43.593660320726286
Time: 9.91 s
Epoch 11: 70.03% done
Loss: 46.594977456749405
Time: 11.67 s
Epoch 11: 80.05% done
Loss: 42.07597584346656
Time: 13.43 s
Epoch 11: 90.03% done
Loss: 40.3661987338582
Time: 15.15 s

Epoch 11 done
Epoch loss: 43.60611303164612

Time taken for epoch: 17.25 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.35 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 413.55363305555574

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_11.pt

Regenerated paired data
Epoch 12: 0.05% done
Loss: 70.39150595664978
Time: 0.00 s
Epoch 12: 10.03% done
Loss: 40.34569991536137
Time: 1.27 s
Epoch 12: 20.05% done
Loss: 38.21641102564245
Time: 2.96 s
Epoch 12: 30.03% done
Loss: 41.38826246448614
Time: 4.62 s
Epoch 12: 40.05% done
Loss: 40.07745683345307
Time: 6.36 s
Epoch 12: 50.03% done
Loss: 40.52977480113064
Time: 8.09 s
Epoch 12: 60.05% done
Loss: 37.49911955033602
Time: 9.76 s
Epoch 12: 70.03% done
Loss: 41.27813606873399
Time: 11.44 s
Epoch 12: 80.05% done
Loss: 37.64301352109292
Time: 13.13 s
Epoch 12: 90.03% done
Loss: 41.57049588169997
Time: 14.87 s

Epoch 12 done
Epoch loss: 39.87589876022617

Time taken for epoch: 16.89 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.12 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.47 s

Validation loss: 413.8421022563899

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_12.pt

Regenerated paired data
Epoch 13: 0.05% done
Loss: 30.03997802734375
Time: 0.00 s
Epoch 13: 10.04% done
Loss: 33.61414067807015
Time: 1.24 s
Epoch 13: 20.02% done
Loss: 31.470293529811457
Time: 3.00 s
Epoch 13: 30.01% done
Loss: 32.678288698897255
Time: 4.76 s
Epoch 13: 40.04% done
Loss: 35.75816862250037
Time: 6.52 s
Epoch 13: 50.03% done
Loss: 37.51168683738062
Time: 8.21 s
Epoch 13: 60.01% done
Loss: 33.042547413303204
Time: 9.93 s
Epoch 13: 70.05% done
Loss: 32.71199118383416
Time: 11.71 s
Epoch 13: 80.03% done
Loss: 35.463125538551296
Time: 13.43 s
Epoch 13: 90.02% done
Loss: 35.72561665687409
Time: 15.06 s

Epoch 13 done
Epoch loss: 34.113979234279846

Time taken for epoch: 17.12 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.11 s
Calculating validation loss: 40.37% done
Time: 0.23 s
Calculating validation loss: 60.09% done
Time: 0.34 s
Calculating validation loss: 80.28% done
Time: 0.45 s

Validation loss: 414.10072002935846

Time taken: 0.57 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_13.pt

Regenerated paired data
Epoch 14: 0.05% done
Loss: 83.5115373134613
Time: 0.01 s
Epoch 14: 10.03% done
Loss: 30.941544349230075
Time: 1.20 s
Epoch 14: 20.01% done
Loss: 29.857677268981462
Time: 2.89 s
Epoch 14: 30.04% done
Loss: 33.93224917162659
Time: 4.66 s
Epoch 14: 40.02% done
Loss: 36.5286400919719
Time: 6.42 s
Epoch 14: 50.05% done
Loss: 38.948526139307916
Time: 8.11 s
Epoch 14: 60.03% done
Loss: 29.51528136236264
Time: 9.76 s
Epoch 14: 70.01% done
Loss: 31.52586565237737
Time: 11.55 s
Epoch 14: 80.04% done
Loss: 37.50544000784375
Time: 13.35 s
Epoch 14: 90.02% done
Loss: 32.560169513130354
Time: 15.08 s

Epoch 14 done
Epoch loss: 33.72574092013125

Time taken for epoch: 17.10 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.11 s
Calculating validation loss: 40.37% done
Time: 0.22 s
Calculating validation loss: 60.09% done
Time: 0.33 s
Calculating validation loss: 80.28% done
Time: 0.45 s

Validation loss: 414.86817598342896

Time taken: 0.56 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_14.pt

Regenerated paired data
Epoch 15: 0.05% done
Loss: 67.80969500541687
Time: 0.00 s
Epoch 15: 10.03% done
Loss: 30.869868078864076
Time: 1.25 s
Epoch 15: 20.05% done
Loss: 27.87411708669063
Time: 3.01 s
Epoch 15: 30.03% done
Loss: 25.707634628310593
Time: 4.78 s
Epoch 15: 40.05% done
Loss: 31.017746049068382
Time: 6.52 s
Epoch 15: 50.03% done
Loss: 26.284630663723526
Time: 8.20 s
Epoch 15: 60.05% done
Loss: 24.02222121728762
Time: 9.96 s
Epoch 15: 70.03% done
Loss: 30.604248975622326
Time: 11.69 s
Epoch 15: 80.05% done
Loss: 35.42797682325978
Time: 13.39 s
Epoch 15: 90.03% done
Loss: 20.316631069631466
Time: 15.10 s

Epoch 15 done
Epoch loss: 28.14850036346621

Time taken for epoch: 17.25 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.00 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.25 s
Calculating validation loss: 60.09% done
Time: 0.38 s
Calculating validation loss: 80.28% done
Time: 0.50 s

Validation loss: 414.86050142060725

Time taken: 0.63 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_15.pt

Regenerated paired data
Epoch 16: 0.05% done
Loss: 2.7121033519506454
Time: 0.00 s
Epoch 16: 10.03% done
Loss: 34.79065824120136
Time: 1.30 s
Epoch 16: 20.01% done
Loss: 23.277397785891985
Time: 3.04 s
Epoch 16: 30.04% done
Loss: 30.076574292338112
Time: 4.76 s
Epoch 16: 40.02% done
Loss: 33.663073646532595
Time: 6.55 s
Epoch 16: 50.05% done
Loss: 32.75849634401814
Time: 8.25 s
Epoch 16: 60.03% done
Loss: 30.536638431379224
Time: 9.97 s
Epoch 16: 70.01% done
Loss: 26.476756762184035
Time: 11.66 s
Epoch 16: 80.04% done
Loss: 38.88489338639543
Time: 13.35 s
Epoch 16: 90.02% done
Loss: 24.549851492353046
Time: 15.06 s

Epoch 16 done
Epoch loss: 30.915668425155495

Time taken for epoch: 17.17 s
Number of gradients clipped: 20

Calculating validation loss: 0.46% done
Time: 0.01 s
Calculating validation loss: 20.18% done
Time: 0.13 s
Calculating validation loss: 40.37% done
Time: 0.24 s
Calculating validation loss: 60.09% done
Time: 0.36 s
Calculating validation loss: 80.28% done
Time: 0.46 s

Validation loss: 415.4120059188353

Time taken: 0.58 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_lr_division_10_middle_dim_512_output_dim_256_lr_0.0005_weight_decay_0.01/2024-08-05_20:08:53_checkpoint_epoch_16.pt

Regenerated paired data
Validation loss has not improved for 15 epochs. Stopping training.
BEST VALIDATION LOSS: 409.5750923550457 at epoch 1

