Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
Training reference file: data/tamil/analysis/ref_of_queries_in_docs_train.txt
Validation reference file: data/tamil/analysis/ref_of_queries_in_docs_valid.txt
START TIME: 2024-08-07_12:43:13
Training model for tamil with inputs from mHuBERT layer 9
Number of epochs: 500, patience: 5, learning rate: 0.0001
clip norm: 10, temperature: 0.15, num pairs per batch: 5
time limit to create dataset: 600
weight decay: 0.0
awe_lr: 1e-05
temperature: 0.15

Created paired data
Created paired data
Time taken to create datasets: 65.44 s
up_proj_dim: 512
output_dim: 1024
middle_dim: 512

Loading model from data/tamil/models/awe/9/lr_1e-4_tmp_0.07_acc_1000_bs_5_3_9/2024-07-20_23:47:58_checkpoint_epoch_0.pt

Number of parameters in model: 8401920
Number of parameters in AWE model: 6825984
Number of parameters in other model: 1575936
/work/y07/shared/cirrus-software/pytorch/1.13.1-gpu/python/3.10.8/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch 0: 0.13% done
Loss: 583.2263946533203
Time: 5.98 s
Epoch 0: 10.04% done
Loss: 493.493864566465
Time: 8.58 s
Epoch 0: 20.08% done
Loss: 431.0437333583832
Time: 10.64 s
Epoch 0: 30.11% done
Loss: 396.08334946632385
Time: 12.49 s
Epoch 0: 40.03% done
Loss: 370.02905938192765
Time: 13.81 s
Epoch 0: 50.06% done
Loss: 351.93477606773376
Time: 15.71 s
Epoch 0: 60.10% done
Loss: 341.419203042984
Time: 17.59 s
Epoch 0: 70.01% done
Loss: 332.91630618179903
Time: 19.46 s
Epoch 0: 80.05% done
Loss: 330.95032501220703
Time: 21.38 s
Epoch 0: 90.09% done
Loss: 340.0758136312167
Time: 23.23 s

Epoch 0 done
Epoch loss: 371.33941829877637

Time taken for epoch: 25.54 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.11 s
Calculating validation loss: 40.22% done
Time: 0.27 s
Calculating validation loss: 60.87% done
Time: 0.52 s
Calculating validation loss: 80.43% done
Time: 0.70 s

Validation loss: 347.51549738040865

Time taken: 0.86 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_0.pt

Regenerated paired data
Epoch 1: 0.13% done
Loss: 299.04565811157227
Time: 0.01 s
Epoch 1: 10.06% done
Loss: 321.4385453960564
Time: 1.28 s
Epoch 1: 20.13% done
Loss: 309.74395060539246
Time: 3.15 s
Epoch 1: 30.06% done
Loss: 298.58436765550056
Time: 5.03 s
Epoch 1: 40.13% done
Loss: 310.3791024684906
Time: 6.86 s
Epoch 1: 50.06% done
Loss: 299.9116962469077
Time: 8.74 s
Epoch 1: 60.13% done
Loss: 299.88094663619995
Time: 10.68 s
Epoch 1: 70.06% done
Loss: 292.08133166349387
Time: 12.54 s
Epoch 1: 80.13% done
Loss: 306.0599316358566
Time: 14.51 s
Epoch 1: 90.06% done
Loss: 303.560822241417
Time: 16.59 s

Epoch 1 done
Epoch loss: 303.66377377560053

Time taken for epoch: 19.00 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.51 s

Validation loss: 346.9329671410547

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_1.pt

Regenerated paired data
Epoch 2: 0.13% done
Loss: 340.5158233642578
Time: 0.01 s
Epoch 2: 10.06% done
Loss: 291.27320362042775
Time: 1.89 s
Epoch 2: 20.13% done
Loss: 281.7872853279114
Time: 3.93 s
Epoch 2: 30.06% done
Loss: 281.3022717343101
Time: 5.23 s
Epoch 2: 40.13% done
Loss: 289.0081880092621
Time: 7.20 s
Epoch 2: 50.06% done
Loss: 281.0175665722618
Time: 9.13 s
Epoch 2: 60.13% done
Loss: 273.1436812877655
Time: 11.31 s
Epoch 2: 70.06% done
Loss: 290.9553445341215
Time: 13.26 s
Epoch 2: 80.13% done
Loss: 275.64877820014954
Time: 15.13 s
Epoch 2: 90.06% done
Loss: 280.18540201307854
Time: 17.10 s

Epoch 2 done
Epoch loss: 280.7037477093423

Time taken for epoch: 19.52 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 346.6269815313643

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_2.pt

Regenerated paired data
Epoch 3: 0.13% done
Loss: 368.5105514526367
Time: 0.01 s
Epoch 3: 10.06% done
Loss: 282.31069190592706
Time: 1.98 s
Epoch 3: 20.13% done
Loss: 272.29998767375946
Time: 3.95 s
Epoch 3: 30.06% done
Loss: 271.7743014082124
Time: 5.33 s
Epoch 3: 40.13% done
Loss: 274.79186856746674
Time: 7.30 s
Epoch 3: 50.06% done
Loss: 274.02419488641283
Time: 9.31 s
Epoch 3: 60.13% done
Loss: 255.01805782318115
Time: 11.18 s
Epoch 3: 70.06% done
Loss: 251.4244860637037
Time: 13.07 s
Epoch 3: 80.13% done
Loss: 268.86748814582825
Time: 14.94 s
Epoch 3: 90.06% done
Loss: 257.1910708061251
Time: 16.92 s

Epoch 3 done
Epoch loss: 266.3877433400984

Time taken for epoch: 19.23 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.36 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.4292355205702

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_3.pt

Regenerated paired data
Epoch 4: 0.13% done
Loss: 291.2180519104004
Time: 0.01 s
Epoch 4: 10.08% done
Loss: 245.29200783258753
Time: 1.97 s
Epoch 4: 20.03% done
Loss: 256.8956932840468
Time: 3.25 s
Epoch 4: 30.10% done
Loss: 258.37504482269287
Time: 5.30 s
Epoch 4: 40.05% done
Loss: 249.71431804608696
Time: 7.21 s
Epoch 4: 50.13% done
Loss: 251.89351242780685
Time: 9.06 s
Epoch 4: 60.08% done
Loss: 255.37939735605747
Time: 10.95 s
Epoch 4: 70.03% done
Loss: 250.84290287162685
Time: 12.90 s
Epoch 4: 80.10% done
Loss: 226.37223291397095
Time: 14.85 s
Epoch 4: 90.05% done
Loss: 259.71081896673275
Time: 16.77 s

Epoch 4 done
Epoch loss: 249.94923606026984

Time taken for epoch: 19.11 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.49 s

Validation loss: 346.25376894854116

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_4.pt

Regenerated paired data
Epoch 5: 0.13% done
Loss: 291.43184661865234
Time: 0.01 s
Epoch 5: 10.09% done
Loss: 242.9352325729177
Time: 1.86 s
Epoch 5: 20.05% done
Loss: 239.27516164658945
Time: 3.20 s
Epoch 5: 30.01% done
Loss: 239.3581297427793
Time: 5.15 s
Epoch 5: 40.10% done
Loss: 243.12537848949432
Time: 7.00 s
Epoch 5: 50.06% done
Loss: 249.24086784008682
Time: 8.81 s
Epoch 5: 60.03% done
Loss: 230.85266741016244
Time: 10.72 s
Epoch 5: 70.11% done
Loss: 232.54255640506744
Time: 12.79 s
Epoch 5: 80.08% done
Loss: 219.7398378879209
Time: 14.73 s
Epoch 5: 90.04% done
Loss: 220.28063611139225
Time: 16.51 s

Epoch 5 done
Epoch loss: 234.05539655945995

Time taken for epoch: 18.86 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.12 s
Calculating validation loss: 40.22% done
Time: 0.23 s
Calculating validation loss: 60.87% done
Time: 0.37 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 346.06456378231877

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_5.pt

Regenerated paired data
Epoch 6: 0.13% done
Loss: 202.3817253112793
Time: 0.01 s
Epoch 6: 10.06% done
Loss: 217.48236306105989
Time: 1.82 s
Epoch 6: 20.13% done
Loss: 220.40710896253586
Time: 3.10 s
Epoch 6: 30.06% done
Loss: 217.52613365650177
Time: 5.03 s
Epoch 6: 40.13% done
Loss: 216.94935739040375
Time: 6.94 s
Epoch 6: 50.06% done
Loss: 220.08063618140883
Time: 8.90 s
Epoch 6: 60.13% done
Loss: 230.3253744840622
Time: 10.89 s
Epoch 6: 70.06% done
Loss: 217.73488998413086
Time: 12.78 s
Epoch 6: 80.13% done
Loss: 216.23010608553886
Time: 14.68 s
Epoch 6: 90.06% done
Loss: 222.07727643507945
Time: 16.65 s

Epoch 6 done
Epoch loss: 220.21413789230323

Time taken for epoch: 19.15 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.35 s
Calculating validation loss: 80.43% done
Time: 0.47 s

Validation loss: 345.95966271732163

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_6.pt

Regenerated paired data
Epoch 7: 0.13% done
Loss: 289.37246322631836
Time: 0.01 s
Epoch 7: 10.04% done
Loss: 211.8895815595796
Time: 2.12 s
Epoch 7: 20.08% done
Loss: 223.16098082065582
Time: 4.07 s
Epoch 7: 30.11% done
Loss: 210.41046226024628
Time: 6.03 s
Epoch 7: 40.03% done
Loss: 219.59630628175373
Time: 7.31 s
Epoch 7: 50.06% done
Loss: 196.80796587467194
Time: 9.26 s
Epoch 7: 60.10% done
Loss: 204.30856096744537
Time: 11.08 s
Epoch 7: 70.01% done
Loss: 206.79052727131904
Time: 13.02 s
Epoch 7: 80.05% done
Loss: 208.0116451581319
Time: 14.93 s
Epoch 7: 90.09% done
Loss: 203.31688222289085
Time: 16.83 s

Epoch 7 done
Epoch loss: 209.71185034959697

Time taken for epoch: 19.16 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.29 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 345.88580183360887

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_7.pt

Regenerated paired data
Epoch 8: 0.13% done
Loss: 219.60487365722656
Time: 0.01 s
Epoch 8: 10.08% done
Loss: 206.77285339258893
Time: 1.95 s
Epoch 8: 20.03% done
Loss: 199.30310533016544
Time: 3.37 s
Epoch 8: 30.10% done
Loss: 197.95209246873856
Time: 5.30 s
Epoch 8: 40.05% done
Loss: 198.312040642847
Time: 7.18 s
Epoch 8: 50.13% done
Loss: 196.87724590301514
Time: 9.21 s
Epoch 8: 60.08% done
Loss: 203.22006998182852
Time: 11.09 s
Epoch 8: 70.03% done
Loss: 194.11245273638374
Time: 13.01 s
Epoch 8: 80.10% done
Loss: 184.28410524129868
Time: 14.98 s
Epoch 8: 90.05% done
Loss: 205.45156267624867
Time: 16.92 s

Epoch 8 done
Epoch loss: 197.4038332715743

Time taken for epoch: 19.20 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.14 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.40 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 345.8234988433727

Time taken: 0.65 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_8.pt

Regenerated paired data
Epoch 9: 0.13% done
Loss: 137.64490127563477
Time: 0.01 s
Epoch 9: 10.06% done
Loss: 190.44698159905928
Time: 1.88 s
Epoch 9: 20.13% done
Loss: 192.19220614433289
Time: 3.79 s
Epoch 9: 30.06% done
Loss: 196.18047883238972
Time: 5.12 s
Epoch 9: 40.13% done
Loss: 184.5899719595909
Time: 7.04 s
Epoch 9: 50.06% done
Loss: 199.80290992350518
Time: 9.04 s
Epoch 9: 60.13% done
Loss: 191.7589300274849
Time: 10.96 s
Epoch 9: 70.06% done
Loss: 184.56955185419397
Time: 12.85 s
Epoch 9: 80.13% done
Loss: 190.2279166976611
Time: 14.86 s
Epoch 9: 90.06% done
Loss: 186.91101626504826
Time: 16.70 s

Epoch 9 done
Epoch loss: 189.28951157363954

Time taken for epoch: 19.02 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.50 s

Validation loss: 345.78418887179834

Time taken: 0.61 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_9.pt

Regenerated paired data
Epoch 10: 0.13% done
Loss: 185.29430389404297
Time: 0.01 s
Epoch 10: 10.06% done
Loss: 174.6865127660051
Time: 1.96 s
Epoch 10: 20.13% done
Loss: 182.564668238163
Time: 3.32 s
Epoch 10: 30.06% done
Loss: 175.0852122487901
Time: 5.20 s
Epoch 10: 40.13% done
Loss: 174.743567943573
Time: 7.08 s
Epoch 10: 50.06% done
Loss: 170.3258692463742
Time: 9.01 s
Epoch 10: 60.13% done
Loss: 185.3970329761505
Time: 10.92 s
Epoch 10: 70.06% done
Loss: 175.44245357754863
Time: 12.80 s
Epoch 10: 80.13% done
Loss: 180.23944306373596
Time: 14.72 s
Epoch 10: 90.06% done
Loss: 166.41848075238966
Time: 16.66 s

Epoch 10 done
Epoch loss: 175.92984641273065

Time taken for epoch: 19.15 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.46 s
Calculating validation loss: 80.43% done
Time: 0.58 s

Validation loss: 345.6750327953394

Time taken: 0.69 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_10.pt

Regenerated paired data
Epoch 11: 0.13% done
Loss: 186.25415802001953
Time: 0.01 s
Epoch 11: 10.06% done
Loss: 175.65368833421152
Time: 1.94 s
Epoch 11: 20.13% done
Loss: 156.03030121326447
Time: 3.99 s
Epoch 11: 30.06% done
Loss: 164.34839013256604
Time: 5.43 s
Epoch 11: 40.13% done
Loss: 177.86426231265068
Time: 7.40 s
Epoch 11: 50.06% done
Loss: 172.3493987699098
Time: 9.28 s
Epoch 11: 60.13% done
Loss: 171.00325460731983
Time: 11.28 s
Epoch 11: 70.06% done
Loss: 157.59472321860397
Time: 13.18 s
Epoch 11: 80.13% done
Loss: 164.91889446228743
Time: 15.03 s
Epoch 11: 90.06% done
Loss: 180.69738496707964
Time: 16.88 s

Epoch 11 done
Epoch loss: 169.4756391430801

Time taken for epoch: 19.28 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.41 s
Calculating validation loss: 80.43% done
Time: 0.54 s

Validation loss: 345.6394936727441

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_11.pt

Regenerated paired data
Epoch 12: 0.13% done
Loss: 127.07517623901367
Time: 0.01 s
Epoch 12: 10.05% done
Loss: 173.11665860912467
Time: 1.98 s
Epoch 12: 20.10% done
Loss: 162.01302456855774
Time: 3.35 s
Epoch 12: 30.03% done
Loss: 167.04016525534135
Time: 5.38 s
Epoch 12: 40.08% done
Loss: 162.7802791595459
Time: 7.28 s
Epoch 12: 50.13% done
Loss: 158.53498247265816
Time: 9.27 s
Epoch 12: 60.05% done
Loss: 158.65123072756995
Time: 11.27 s
Epoch 12: 70.10% done
Loss: 166.53397619724274
Time: 13.22 s
Epoch 12: 80.03% done
Loss: 151.57000016562546
Time: 15.14 s
Epoch 12: 90.08% done
Loss: 157.1456597521901
Time: 17.15 s

Epoch 12 done
Epoch loss: 161.51667051054724

Time taken for epoch: 19.54 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.38 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 345.61017682586896

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_12.pt

Regenerated paired data
Epoch 13: 0.13% done
Loss: 200.14787673950195
Time: 0.01 s
Epoch 13: 10.05% done
Loss: 156.32441906989376
Time: 1.99 s
Epoch 13: 20.10% done
Loss: 155.59470696747303
Time: 3.41 s
Epoch 13: 30.03% done
Loss: 153.84910281700425
Time: 5.36 s
Epoch 13: 40.08% done
Loss: 143.2043028064072
Time: 7.27 s
Epoch 13: 50.13% done
Loss: 147.31585186719894
Time: 9.20 s
Epoch 13: 60.05% done
Loss: 153.76764593245107
Time: 11.15 s
Epoch 13: 70.10% done
Loss: 151.47123213484883
Time: 12.96 s
Epoch 13: 80.03% done
Loss: 146.88624502737312
Time: 14.84 s
Epoch 13: 90.08% done
Loss: 154.64786626895267
Time: 16.75 s

Epoch 13 done
Epoch loss: 150.96901329742803

Time taken for epoch: 19.19 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.29 s
Calculating validation loss: 60.87% done
Time: 0.43 s
Calculating validation loss: 80.43% done
Time: 0.55 s

Validation loss: 345.61786292255783

Time taken: 0.67 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_13.pt

Regenerated paired data
Epoch 14: 0.13% done
Loss: 93.29659461975098
Time: 0.01 s
Epoch 14: 10.09% done
Loss: 155.51082236857354
Time: 1.92 s
Epoch 14: 20.05% done
Loss: 139.87926145143146
Time: 3.25 s
Epoch 14: 30.01% done
Loss: 141.9378782827643
Time: 5.13 s
Epoch 14: 40.10% done
Loss: 144.2539705634117
Time: 7.21 s
Epoch 14: 50.06% done
Loss: 142.85967048210435
Time: 9.23 s
Epoch 14: 60.03% done
Loss: 151.8780268898493
Time: 11.17 s
Epoch 14: 70.11% done
Loss: 140.1855310002963
Time: 13.07 s
Epoch 14: 80.08% done
Loss: 139.05934738207466
Time: 14.93 s
Epoch 14: 90.04% done
Loss: 142.4254263503642
Time: 16.76 s

Epoch 14 done
Epoch loss: 144.3468554995049

Time taken for epoch: 19.18 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.24 s
Calculating validation loss: 60.87% done
Time: 0.34 s
Calculating validation loss: 80.43% done
Time: 0.46 s

Validation loss: 345.62361046887827

Time taken: 0.60 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_14.pt

Regenerated paired data
Epoch 15: 0.13% done
Loss: 123.26046943664551
Time: 0.02 s
Epoch 15: 10.08% done
Loss: 137.23717936986608
Time: 1.96 s
Epoch 15: 20.03% done
Loss: 147.9459611071816
Time: 3.29 s
Epoch 15: 30.10% done
Loss: 145.99779468774796
Time: 5.27 s
Epoch 15: 40.05% done
Loss: 123.04076442235633
Time: 7.22 s
Epoch 15: 50.13% done
Loss: 129.8146681250073
Time: 9.17 s
Epoch 15: 60.08% done
Loss: 146.9502905350697
Time: 11.11 s
Epoch 15: 70.03% done
Loss: 143.4731345237056
Time: 12.86 s
Epoch 15: 80.10% done
Loss: 135.8735929131508
Time: 14.73 s
Epoch 15: 90.05% done
Loss: 141.02550657489633
Time: 16.72 s

Epoch 15 done
Epoch loss: 139.13995733250036

Time taken for epoch: 19.06 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.29 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.53 s

Validation loss: 345.6687879044077

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_15.pt

Regenerated paired data
Epoch 16: 0.13% done
Loss: 34.0105938911438
Time: 0.01 s
Epoch 16: 10.06% done
Loss: 138.2711704773239
Time: 1.93 s
Epoch 16: 20.13% done
Loss: 136.35268622636795
Time: 3.97 s
Epoch 16: 30.06% done
Loss: 129.8928846286822
Time: 5.26 s
Epoch 16: 40.13% done
Loss: 135.37520337104797
Time: 7.28 s
Epoch 16: 50.06% done
Loss: 124.64180053034916
Time: 9.25 s
Epoch 16: 60.13% done
Loss: 128.2069250009954
Time: 11.10 s
Epoch 16: 70.06% done
Loss: 130.1112834713127
Time: 13.01 s
Epoch 16: 80.13% done
Loss: 132.0849082916975
Time: 14.96 s
Epoch 16: 90.06% done
Loss: 130.59087317201156
Time: 16.89 s

Epoch 16 done
Epoch loss: 131.02567269029856

Time taken for epoch: 19.20 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.02 s
Calculating validation loss: 20.65% done
Time: 0.15 s
Calculating validation loss: 40.22% done
Time: 0.28 s
Calculating validation loss: 60.87% done
Time: 0.42 s
Calculating validation loss: 80.43% done
Time: 0.55 s

Validation loss: 345.66518628078956

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_16.pt

Regenerated paired data
Epoch 17: 0.13% done
Loss: 113.2771110534668
Time: 0.01 s
Epoch 17: 10.08% done
Loss: 125.68802405007278
Time: 1.34 s
Epoch 17: 20.03% done
Loss: 116.75881844532641
Time: 3.13 s
Epoch 17: 30.10% done
Loss: 128.1815487742424
Time: 4.99 s
Epoch 17: 40.05% done
Loss: 126.68297761603247
Time: 6.93 s
Epoch 17: 50.13% done
Loss: 130.86952486634254
Time: 8.81 s
Epoch 17: 60.08% done
Loss: 111.32607731638076
Time: 10.72 s
Epoch 17: 70.03% done
Loss: 126.38058300259748
Time: 12.67 s
Epoch 17: 80.10% done
Loss: 125.42908817529678
Time: 14.71 s
Epoch 17: 90.05% done
Loss: 118.23694923256016
Time: 16.68 s

Epoch 17 done
Epoch loss: 123.71802640802014

Time taken for epoch: 19.08 s
Number of gradients clipped: 20

Calculating validation loss: 1.09% done
Time: 0.01 s
Calculating validation loss: 20.65% done
Time: 0.13 s
Calculating validation loss: 40.22% done
Time: 0.26 s
Calculating validation loss: 60.87% done
Time: 0.39 s
Calculating validation loss: 80.43% done
Time: 0.52 s

Validation loss: 345.67421868227535

Time taken: 0.66 s
Saving model to data/tamil/models/sent/9/finetune_awe_grad_1_layer_middle_dim_512_output_dim_1024_lr_0.0001_tmp_0.15_weight_decay_0.0/2024-08-07_12:43:13_checkpoint_epoch_17.pt

Regenerated paired data
Validation loss has not improved for 5 epochs. Stopping training.
BEST VALIDATION LOSS: 345.61017682586896 at epoch 12

